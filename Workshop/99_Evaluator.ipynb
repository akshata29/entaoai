{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of evaluating a document based on the chunking techniques (RecursiveCharacterSplit, FormRecognizer, TikToken) and using different chunkSize and Overlap during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import openai\n",
    "from Utilities.envVars import *\n",
    "\n",
    "# Set OpenAI API key and endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OpenAiVersion\n",
    "openai_api_key = OpenAiKey\n",
    "assert openai_api_key, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = openai_api_key\n",
    "openAiEndPoint = f\"https://{OpenAiService}.openai.azure.com\"\n",
    "assert openAiEndPoint, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in openAiEndPoint.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = openAiEndPoint\n",
    "davincimodel = OpenAiDavinci\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from Utilities.cogSearchVsRetriever import CognitiveSearchVsRetriever\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.llms import Replicate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.base import BaseCallbackManager\n",
    "from langchain.document_loaders import PDFMinerLoader, UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from Utilities.evaluator import createEvaluatorDataSearchIndex, indexEvaluatorDataSections, createEvaluatorDocumentSearchIndex, indexDocs\n",
    "from Utilities.evaluator import createEvaluatorQaSearchIndex, searchEvaluatorQaData, searchEvaluatorDocument, searchEvaluatorDocumentIndexedData\n",
    "from Utilities.evaluator import searchEvaluatorRunIndex, createEvaluatorRunIndex, getEvaluatorResult\n",
    "from Utilities.evaluator import createEvaluatorResultIndex, searchEvaluatorRunIdIndex\n",
    "from IPython.display import display, HTML\n",
    "import uuid\n",
    "import random\n",
    "from langchain.chains import QAGenerationChain\n",
    "import itertools\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import time\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "embeddingModelType = \"azureopenai\"\n",
    "temperature = 0\n",
    "tokenLength = 1000\n",
    "fileName = \"Fabric Get Started.pdf\"\n",
    "#fileName = \"exhibit-13.pdf\"\n",
    "regenerateQa = False\n",
    "reEvaluate = False\n",
    "topK = 3\n",
    "totalQuestions = 5\n",
    "retrieverType = \"SimilaritySearch\"\n",
    "promptStyle = \"Descriptive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant Variables\n",
    "evaluatorDocumentIndex = \"evaluatordocument\"\n",
    "evaluatorDataIndexName = \"evaluatordata\"\n",
    "evaluatorQaDataIndexName = \"evaluatorqadata\"\n",
    "evaluatorRunIndexName = \"evaluatorrun\"\n",
    "evaluatorRunResultIndexName = \"evaluatorrunresult\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (embeddingModelType == 'azureopenai'):\n",
    "        openai.api_type = \"azure\"\n",
    "        openai.api_key = OpenAiKey\n",
    "        openai.api_version = OpenAiVersion\n",
    "        openai.api_base = f\"https://{OpenAiService}.openai.azure.com\"\n",
    "\n",
    "        llm = AzureChatOpenAI(\n",
    "                openai_api_base=openai.api_base,\n",
    "                openai_api_version=OpenAiVersion,\n",
    "                deployment_name=OpenAiChat,\n",
    "                temperature=temperature,\n",
    "                openai_api_key=OpenAiKey,\n",
    "                openai_api_type=\"azure\",\n",
    "                max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(deployment=OpenAiEmbedding, chunk_size=1, openai_api_key=OpenAiKey)\n",
    "        logging.info(\"LLM Setup done\")\n",
    "elif embeddingModelType == \"openai\":\n",
    "        openai.api_type = \"open_ai\"\n",
    "        openai.api_base = \"https://api.openai.com/v1\"\n",
    "        openai.api_version = '2020-11-07' \n",
    "        openai.api_key = OpenAiApiKey\n",
    "        llm = ChatOpenAI(temperature=temperature,\n",
    "        openai_api_key=OpenAiApiKey,\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=OpenAiApiKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search index evaluatordocument already exists\n",
      "Total docs: 1\n",
      "\tIndexed 1 sections, 1 succeeded\n"
     ]
    }
   ],
   "source": [
    "# Check if we already have document inserted into our index\n",
    "documentResponse = searchEvaluatorDocument(SearchService, SearchKey, evaluatorDocumentIndex, fileName)\n",
    "if documentResponse.get_count() > 0:\n",
    "    for doc in documentResponse:\n",
    "        documentId = doc[\"documentId\"]\n",
    "        break\n",
    "else:\n",
    "    documentId = str(uuid.uuid4())\n",
    "    # Create the Evaluator Document Search Index\n",
    "    createEvaluatorDocumentSearchIndex(SearchService, SearchKey, evaluatorDocumentIndex)\n",
    "    # Insert the document metadata\n",
    "    evaluatorDocument = []\n",
    "    evaluatorDocument.append({\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"documentId\": documentId,\n",
    "            \"documentName\": fileName,\n",
    "            \"sourceFile\": fileName,\n",
    "        })\n",
    "    indexDocs(SearchService, SearchKey, evaluatorDocumentIndex, evaluatorDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process our fileName\n",
    "# TODO : Add support for other file types\n",
    "\n",
    "fabricGetStartedPath = \"Data/PDF/\" + fileName\n",
    "#fabricGetStartedPath = \"C:\\\\Users\\\\astalati\\\\Downloads\\\\\" + fileName\n",
    "# Load the PDF with Document Loader available from Langchain\n",
    "loader = PDFMinerLoader(fabricGetStartedPath)\n",
    "rawDocs = loader.load()\n",
    "# Set the source \n",
    "for doc in rawDocs:\n",
    "    doc.metadata['source'] = fabricGetStartedPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search index evaluatordata already exists\n",
      "Processing Split Method: RecursiveCharacterTextSplitter Chunk Size: 1500 Overlap: 0\n",
      "Total docs: 962\n",
      "Indexing sections from 'exhibit-13.pdf' into search index 'evaluatordata'\n",
      "\tIndexed 962 sections, 962 succeeded\n"
     ]
    }
   ],
   "source": [
    "# Process the document and create the chunked Index with different split methods, chunk sizes and overlaps.\n",
    "# Eventually we will add support for different models\n",
    "# Add more Split Methods\n",
    "splitMethods = [\"RecursiveCharacterTextSplitter\"]\n",
    "model = \"GPT3.5\"\n",
    "chunkSizes = ['500', '1000', '1500', '2000']\n",
    "overlaps = ['0', '50', '100', '150']\n",
    "\n",
    "# Create the Evaluator Data Search Index to store our vector Data\n",
    "createEvaluatorDataSearchIndex(SearchService, SearchKey, evaluatorDataIndexName)\n",
    "for splitMethod in splitMethods:\n",
    "    for chunkSize in chunkSizes:\n",
    "        for overlap in overlaps:\n",
    "            # Check if we already have data inserted into our index\n",
    "            dataResponse = searchEvaluatorDocumentIndexedData(SearchService, SearchKey, evaluatorDataIndexName, documentId, \n",
    "                                                 splitMethod, chunkSize, overlap)\n",
    "            if dataResponse.get_count() == 0:\n",
    "                print(\"Processing Split Method: \" + splitMethod + \" Chunk Size: \" + chunkSize + \" Overlap: \" + overlap)\n",
    "                # Split the document into chunks of 500 characters & 0 overlap\n",
    "                splitter = RecursiveCharacterTextSplitter(chunk_size=int(chunkSize), chunk_overlap=int(overlap))\n",
    "                docs = splitter.split_documents(rawDocs)\n",
    "                indexEvaluatorDataSections(OpenAiService, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, \n",
    "                            SearchKey, embeddingModelType, OpenAiEmbedding, fileName, evaluatorDataIndexName, docs, \n",
    "                            splitMethod, chunkSize, overlap, model, embeddingModelType, documentId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEvaluation(data, chunk):\n",
    "    # Generate random starting index in the doc to draw question from\n",
    "    noOfChar = len(data)\n",
    "    startingIndex = random.randint(0, noOfChar-chunk)\n",
    "    subSequence = data[startingIndex:startingIndex+chunk]\n",
    "    # Set up QAGenerationChain chain using GPT 3.5 as default\n",
    "    chain = QAGenerationChain.from_llm(llm)\n",
    "    evalSet = []\n",
    "    # Catch any QA generation errors and re-try until QA pair is generated\n",
    "    awaitingAnswer = True\n",
    "    while awaitingAnswer:\n",
    "        try:\n",
    "            qaPair = chain.run(subSequence)\n",
    "            evalSet.append(qaPair)\n",
    "            awaitingAnswer = False\n",
    "        except JSONDecodeError:\n",
    "            startingIndex = random.randint(0, noOfChar-chunk)\n",
    "            subSequence = data[startingIndex:startingIndex+chunk]\n",
    "    evalPair = list(itertools.chain.from_iterable(evalSet))\n",
    "    return evalPair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have indexed the documents, let's go ahead and create the set of the QA pairs for the document and store that in the index\n",
    "# We will use the same QA Pair for evaluating all the different chunk sizes and overlap\n",
    "# Check first if we have already generated the QA pairs for this document\n",
    "# If we have, then we will just use that\n",
    "# If not, then we will generate the QA pairs and store them in the index\n",
    "r = searchEvaluatorQaData(SearchService, SearchKey, evaluatorQaDataIndexName, documentId)\n",
    "evaluatorQaData = []\n",
    "if r.get_count() == 0 or regenerateQa:\n",
    "    generateTotalQuestions = 15\n",
    "    generatedQAPairs = []\n",
    "    for i in range(generateTotalQuestions):\n",
    "        # Generate one question\n",
    "        evalPair = generateEvaluation(rawDocs[0].page_content, 3000)\n",
    "        if len(evalPair) == 0:\n",
    "            # Error in eval generation\n",
    "            continue\n",
    "        else:\n",
    "            # This returns a list, so we unpack to dict\n",
    "            evalPair = evalPair[0]\n",
    "            generatedQAPairs.append(evalPair)\n",
    "    # Create the Evaluator Document Search Index\n",
    "    createEvaluatorQaSearchIndex(SearchService, SearchKey, evaluatorQaDataIndexName)\n",
    "    # Insert the document metadata\n",
    "    if regenerateQa:\n",
    "        i=0\n",
    "        for qa in r:\n",
    "            evaluatorQaData.append({\n",
    "                \"id\": qa['id'],\n",
    "                \"documentId\": qa['documentId'],\n",
    "                \"questionId\": qa['questionId'],\n",
    "                \"question\": generatedQAPairs[i]['question'],\n",
    "                \"answer\": generatedQAPairs[i]['answer'],\n",
    "            })\n",
    "            i+=1\n",
    "    else:\n",
    "        for qa in generatedQAPairs:\n",
    "            evaluatorQaData.append({\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"documentId\": documentId,\n",
    "                \"questionId\": str(uuid.uuid4()),\n",
    "                \"question\": qa['question'],\n",
    "                \"answer\": qa['answer'],\n",
    "            })\n",
    "    indexDocs(SearchService, SearchKey, evaluatorQaDataIndexName, evaluatorQaData)\n",
    "else:\n",
    "    for qa in r:\n",
    "            evaluatorQaData.append({\n",
    "                \"id\": qa['id'],\n",
    "                \"documentId\": qa['documentId'],\n",
    "                \"questionId\": qa['questionId'],\n",
    "                \"question\": qa['question'],\n",
    "                \"answer\": qa['answer'],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QaChainPrompt = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a teacher grading a quiz. \n",
    "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either Correct or Incorrect.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: Correct or Incorrect here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. If the student answers that there is no specific information provided in the context, then the answer is Incorrect. Begin! \n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\"\"\"\n",
    "\n",
    "promptStyleFast = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a teacher grading a quiz. \n",
    "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either Correct or Incorrect.\n",
    "You are also asked to identify potential sources of bias in the question and in the true answer.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: Correct or Incorrect here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. If the student answers that there is no specific information provided in the context, then the answer is Incorrect. Begin! \n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\n",
    "\n",
    "Your response should be as follows:\n",
    "\n",
    "GRADE: (Correct or Incorrect)\n",
    "(line break)\n",
    "JUSTIFICATION: (Without mentioning the student/teacher framing of this prompt, explain why the STUDENT ANSWER is Correct or Incorrect, identify potential sources of bias in the QUESTION, and identify potential sources of bias in the TRUE ANSWER. Use one or two sentences maximum. Keep the answer as concise as possible.)\n",
    "\"\"\"\n",
    "\n",
    "promptStyleBias = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are assessing a submitted student answer to a question relative to the true answer based on the provided criteria: \n",
    "    \n",
    "    ***\n",
    "    QUESTION: {query}\n",
    "    ***\n",
    "    STUDENT ANSWER: {result}\n",
    "    ***\n",
    "    TRUE ANSWER: {answer}\n",
    "    ***\n",
    "    Criteria: \n",
    "      relevance:  Is the submission referring to a real quote from the text?\"\n",
    "      conciseness:  Is the answer concise and to the point?\"\n",
    "      correct: Is the answer correct?\"\n",
    "    ***\n",
    "    Does the submission meet the criterion? First, write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print \"Correct\" or \"Incorrect\" (without quotes or punctuation) on its own line corresponding to the correct answer.\n",
    "    Reasoning:\n",
    "\"\"\"\n",
    "\n",
    "promptStyleGrading = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a teacher grading a quiz. \n",
    "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either Correct or Incorrect.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: Correct or Incorrect here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. If the student answers that there is no specific information provided in the context, then the answer is Incorrect. Begin! \n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\n",
    "\n",
    "Your response should be as follows:\n",
    "\n",
    "GRADE: (Correct or Incorrect)\n",
    "(line break)\n",
    "JUSTIFICATION: (Without mentioning the student/teacher framing of this prompt, explain why the STUDENT ANSWER is Correct or Incorrect. Use one or two sentences maximum. Keep the answer as concise as possible.)\n",
    "\"\"\"\n",
    "\n",
    "promptStyleDefault = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" \n",
    "    Given the question: \\n\n",
    "    {query}\n",
    "    Here are some documents retrieved in response to the question: \\n\n",
    "    {result}\n",
    "    And here is the answer to the question: \\n \n",
    "    {answer}\n",
    "    Criteria: \n",
    "      relevance: Are the retrieved documents relevant to the question and do they support the answer?\"\n",
    "    Do the retrieved documents meet the criterion? Print \"Correct\" (without quotes or punctuation) if the retrieved context are relevant or \"Incorrect\" if not (without quotes or punctuation) on its own line. \"\"\"\n",
    "\n",
    "gradeDocsPromptFast = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" \n",
    "    Given the question: \\n\n",
    "    {query}\n",
    "    Here are some documents retrieved in response to the question: \\n\n",
    "    {result}\n",
    "    And here is the answer to the question: \\n \n",
    "    {answer}\n",
    "    Criteria: \n",
    "      relevance: Are the retrieved documents relevant to the question and do they support the answer?\"\n",
    "\n",
    "    Your response should be as follows:\n",
    "\n",
    "    GRADE: (Correct or Incorrect, depending if the retrieved documents meet the criterion)\n",
    "    (line break)\n",
    "    JUSTIFICATION: (Write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Use one or two sentences maximum. Keep the answer as concise as possible.)\n",
    "    \"\"\"\n",
    "\n",
    "gradeDocsPromptDefault = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradeModelAnswer(predictedDataSet, predictions, promptStyle):\n",
    "    if promptStyle == \"Fast\":\n",
    "        prompt = promptStyleFast\n",
    "    elif promptStyle == \"Descriptive w/ bias check\":\n",
    "        prompt = promptStyleBias\n",
    "    elif promptStyle == \"OpenAI grading prompt\":\n",
    "        prompt = promptStyleGrading\n",
    "    else:\n",
    "        prompt = promptStyleDefault\n",
    "\n",
    "    # Note: GPT-4 grader is advised by OAI \n",
    "    evalChain = QAEvalChain.from_llm(llm=llm,\n",
    "                                      prompt=prompt)\n",
    "    gradedOutputs = evalChain.evaluate(predictedDataSet,\n",
    "                                         predictions,\n",
    "                                         question_key=\"question\",\n",
    "                                         prediction_key=\"result\")\n",
    "    return gradedOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradeModelRetrieval(getDataSet, predictions, gradeDocsPrompt):\n",
    "    if gradeDocsPrompt == \"Fast\":\n",
    "        prompt = gradeDocsPromptFast\n",
    "    else:\n",
    "        prompt = gradeDocsPromptDefault\n",
    "\n",
    "    # Note: GPT-4 grader is advised by OAI\n",
    "    evalChain = QAEvalChain.from_llm(llm=llm,prompt=prompt)\n",
    "    gradedOutputs = evalChain.evaluate(getDataSet,\n",
    "                                         predictions,\n",
    "                                         question_key=\"question\",\n",
    "                                         prediction_key=\"result\")\n",
    "    return gradedOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runEvaluator(totalQuestions, chain, retriever, promptStyle, testDataSet):\n",
    "    d = pd.DataFrame(columns=['question', 'answer', 'predictedAnswer', 'answerScore', 'retrievalScore', 'latency'])\n",
    "    for i in range(totalQuestions):\n",
    "        predictions = []\n",
    "        retrievedDocs = []\n",
    "        gtDataSet = []\n",
    "        latency = []\n",
    "        currentDataSet = testDataSet[i]\n",
    "    \n",
    "        try:\n",
    "            startTime = time.time()\n",
    "            predictions.append(chain({\"query\": currentDataSet[\"question\"]}, return_only_outputs=True))\n",
    "            gtDataSet.append(currentDataSet)\n",
    "            endTime = time.time()\n",
    "            elapsedTime = endTime - startTime\n",
    "            latency.append(elapsedTime)\n",
    "        except:\n",
    "            predictions.append({'result': 'Error in prediction'})\n",
    "            print(\"Error in prediction\")\n",
    "\n",
    "        # Extract text from retrieved docs\n",
    "        retrievedDocText = \"\"\n",
    "        docs = retriever.get_relevant_documents(currentDataSet[\"question\"])\n",
    "        for i, doc in enumerate(docs):\n",
    "            retrievedDocText += \"Doc %s: \" % str(i+1) + \\\n",
    "                doc.page_content + \" \"\n",
    "\n",
    "        # Log\n",
    "        retrieved = {\"question\": currentDataSet[\"question\"],\n",
    "                    \"answer\": currentDataSet[\"answer\"], \"result\": retrievedDocText}\n",
    "        retrievedDocs.append(retrieved)\n",
    "\n",
    "        # Grade\n",
    "        gradedAnswer = gradeModelAnswer(gtDataSet, predictions, promptStyle)\n",
    "        gradedRetrieval = gradeModelRetrieval(gtDataSet, retrievedDocs, promptStyle)\n",
    "\n",
    "        # Assemble output\n",
    "        # Summary statistics\n",
    "        dfOutput = {'question': evaluatorQaData[i]['question'], 'answer': evaluatorQaData[i]['answer'],\n",
    "                    'predictedAnswer': predictions[0]['result'], 'answerScore': [{'score': 1 if \"Incorrect\" not in text else 0,\n",
    "                                'justification': text} for text in [g['text'] for g in gradedAnswer]], \n",
    "                                'retrievalScore': [{'score': 1 if \"Incorrect\" not in text else 0,\n",
    "                                'justification': text} for text in [g['text'] for g in gradedRetrieval]],\n",
    "                    'latency': latency}\n",
    "\n",
    "        # Add to dataframe\n",
    "        d = pd.concat([d, pd.DataFrame(dfOutput)], axis=0)\n",
    "\n",
    "        # Convert dataframe to dict\n",
    "    d_dict = d.to_dict('records')\n",
    "    return d_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search index evaluatorrunresult already exists\n"
     ]
    }
   ],
   "source": [
    "# Select retriever\n",
    "#chunkSizes = ['1500']\n",
    "#overlaps = ['150']\n",
    "createEvaluatorResultIndex(SearchService, SearchKey, evaluatorRunResultIndexName)\n",
    "# Check if we already have runId for this document\n",
    "r = searchEvaluatorRunIdIndex(SearchService, SearchKey, evaluatorRunResultIndexName, documentId)\n",
    "if r.get_count() == 0:\n",
    "    runId = str(uuid.uuid4())\n",
    "else:\n",
    "    for run in r:\n",
    "        runId = run['runId']\n",
    "        break\n",
    "for splitMethod in splitMethods:\n",
    "    for chunkSize in chunkSizes:\n",
    "        for overlap in overlaps:\n",
    "            # Verify if we have created the Run ID\n",
    "            r = searchEvaluatorRunIndex(SearchService, SearchKey, evaluatorRunResultIndexName, documentId, retrieverType, \n",
    "                                    promptStyle, splitMethod, chunkSize, overlap)\n",
    "            if r.get_count() == 0 or reEvaluate:\n",
    "                # Create the Run ID\n",
    "                print(\"Processing: \", documentId, retrieverType, promptStyle, splitMethod, chunkSize, overlap)\n",
    "                runIdData = []\n",
    "                subRunId = str(uuid.uuid4())\n",
    "               \n",
    "                retriever = CognitiveSearchVsRetriever(contentKey=\"contentVector\",\n",
    "                            serviceName=SearchService,\n",
    "                            apiKey=SearchKey,\n",
    "                            indexName=evaluatorDataIndexName,\n",
    "                            topK=topK,\n",
    "                            splitMethod = splitMethod,\n",
    "                            model = model,\n",
    "                            chunkSize = chunkSize,\n",
    "                            overlap = overlap,\n",
    "                            openAiService = OpenAiService,\n",
    "                            openAiKey = OpenAiKey,\n",
    "                            openAiVersion = OpenAiVersion,\n",
    "                            openAiApiKey = OpenAiApiKey,\n",
    "                            documentId = documentId,\n",
    "                            openAiEmbedding=OpenAiEmbedding,\n",
    "                            returnFields=[\"id\", \"content\", \"sourceFile\", \"splitMethod\", \"chunkSize\", \"overlap\", \"model\", \"modelType\", \"documentId\"]\n",
    "                            )\n",
    "                vectorStoreChain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, \n",
    "                                                chain_type_kwargs={\"prompt\": QaChainPrompt})\n",
    "                runEvaluations = runEvaluator(totalQuestions, vectorStoreChain, retriever, promptStyle, evaluatorQaData)\n",
    "                runEvaluationData = []\n",
    "                for runEvaluation in runEvaluations:\n",
    "                        runEvaluationData.append({\n",
    "                            \"id\": str(uuid.uuid4()),\n",
    "                            \"runId\": runId,\n",
    "                            \"subRunId\": subRunId,\n",
    "                            \"documentId\": documentId,\n",
    "                            \"retrieverType\": retrieverType,\n",
    "                            \"promptStyle\": promptStyle,\n",
    "                            \"splitMethod\": splitMethod,\n",
    "                            \"chunkSize\": chunkSize,\n",
    "                            \"overlap\": overlap,\n",
    "                            \"question\": runEvaluation['question'],\n",
    "                            \"answer\": runEvaluation['answer'],\n",
    "                            \"predictedAnswer\": runEvaluation['predictedAnswer'],\n",
    "                            \"answerScore\": json.dumps(runEvaluation['answerScore']),\n",
    "                            \"retrievalScore\": json.dumps(runEvaluation['retrievalScore']),\n",
    "                            \"latency\": str(runEvaluation['latency']),\n",
    "                        })\n",
    "                indexDocs(SearchService, SearchKey, evaluatorRunResultIndexName, runEvaluationData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Predicted Answer</th>\n",
       "      <th>Retriever Type</th>\n",
       "      <th>Prompt Style</th>\n",
       "      <th>Split Method</th>\n",
       "      <th>Chunk Size</th>\n",
       "      <th>Overlap</th>\n",
       "      <th>Answer Score</th>\n",
       "      <th>Retrieval Score</th>\n",
       "      <th>Latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the primary use case for the Pipeline ...</td>\n",
       "      <td>The primary use case for the Pipeline copy act...</td>\n",
       "      <td>The primary use case for the Pipeline copy act...</td>\n",
       "      <td>SimilaritySearch</td>\n",
       "      <td>Descriptive</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>3.0535309314727783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of a workspace in Microsof...</td>\n",
       "      <td>In workspaces, you create collections of items...</td>\n",
       "      <td>Fabric is a non-Power BI experience that requi...</td>\n",
       "      <td>SimilaritySearch</td>\n",
       "      <td>Descriptive</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>2.496325969696045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the primary use case for the Pipeline ...</td>\n",
       "      <td>The primary use case for the Pipeline copy act...</td>\n",
       "      <td>The purpose of a workspace in Microsoft Fabric...</td>\n",
       "      <td>SimilaritySearch</td>\n",
       "      <td>Descriptive</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>{'score': 0, 'justification': 'GRADE: Incorrec...</td>\n",
       "      <td>{'score': 0, 'justification': 'GRADE: Incorrec...</td>\n",
       "      <td>3.873573064804077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the key difference between a per-user ...</td>\n",
       "      <td>The key difference is that a Fabric capacity i...</td>\n",
       "      <td>Fabric is a preview product by Microsoft that ...</td>\n",
       "      <td>SimilaritySearch</td>\n",
       "      <td>Descriptive</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>{'score': 0, 'justification': 'GRADE: Incorrec...</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>2.108778715133667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the purpose of a workspace in Microsof...</td>\n",
       "      <td>In workspaces, you create collections of items...</td>\n",
       "      <td>The 'Create a workspace' pane is used to creat...</td>\n",
       "      <td>SimilaritySearch</td>\n",
       "      <td>Descriptive</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>1.9552397727966309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>What is the purpose of a workspace in Microsof...</td>\n",
       "      <td>In workspaces, you create collections of items...</td>\n",
       "      <td>The key difference is that the Fabric (Preview...</td>\n",
       "      <td>SimilaritySearch</td>\n",
       "      <td>Descriptive</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>1000</td>\n",
       "      <td>150</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>2.2532236576080322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>What is the purpose of a workspace in Microsof...</td>\n",
       "      <td>In workspaces, you create collections of items...</td>\n",
       "      <td>A workspace in Microsoft Fabric is a place whe...</td>\n",
       "      <td>SimilaritySearch</td>\n",
       "      <td>Descriptive</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>1000</td>\n",
       "      <td>150</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>2.2411551475524902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>What is the purpose of a workspace in Microsof...</td>\n",
       "      <td>In workspaces, you create collections of items...</td>\n",
       "      <td>Fabric is a Microsoft platform for data manage...</td>\n",
       "      <td>SimilaritySearch</td>\n",
       "      <td>Descriptive</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>1000</td>\n",
       "      <td>150</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>2.4773612022399902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>What is the purpose of a workspace in Microsof...</td>\n",
       "      <td>In workspaces, you create collections of items...</td>\n",
       "      <td>A workspace in Microsoft Fabric is a place to ...</td>\n",
       "      <td>SimilaritySearch</td>\n",
       "      <td>Descriptive</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>2.587143659591675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>What is the purpose of a workspace in Microsof...</td>\n",
       "      <td>In workspaces, you create collections of items...</td>\n",
       "      <td>The 'Create a workspace' pane is used to creat...</td>\n",
       "      <td>SimilaritySearch</td>\n",
       "      <td>Descriptive</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>{'score': 1, 'justification': 'GRADE: Correct\n",
       "...</td>\n",
       "      <td>2.6436407566070557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Question  \\\n",
       "0   What is the primary use case for the Pipeline ...   \n",
       "1   What is the purpose of a workspace in Microsof...   \n",
       "2   What is the primary use case for the Pipeline ...   \n",
       "3   What is the key difference between a per-user ...   \n",
       "4   What is the purpose of a workspace in Microsof...   \n",
       "..                                                ...   \n",
       "75  What is the purpose of a workspace in Microsof...   \n",
       "76  What is the purpose of a workspace in Microsof...   \n",
       "77  What is the purpose of a workspace in Microsof...   \n",
       "78  What is the purpose of a workspace in Microsof...   \n",
       "79  What is the purpose of a workspace in Microsof...   \n",
       "\n",
       "                                               Answer  \\\n",
       "0   The primary use case for the Pipeline copy act...   \n",
       "1   In workspaces, you create collections of items...   \n",
       "2   The primary use case for the Pipeline copy act...   \n",
       "3   The key difference is that a Fabric capacity i...   \n",
       "4   In workspaces, you create collections of items...   \n",
       "..                                                ...   \n",
       "75  In workspaces, you create collections of items...   \n",
       "76  In workspaces, you create collections of items...   \n",
       "77  In workspaces, you create collections of items...   \n",
       "78  In workspaces, you create collections of items...   \n",
       "79  In workspaces, you create collections of items...   \n",
       "\n",
       "                                     Predicted Answer    Retriever Type  \\\n",
       "0   The primary use case for the Pipeline copy act...  SimilaritySearch   \n",
       "1   Fabric is a non-Power BI experience that requi...  SimilaritySearch   \n",
       "2   The purpose of a workspace in Microsoft Fabric...  SimilaritySearch   \n",
       "3   Fabric is a preview product by Microsoft that ...  SimilaritySearch   \n",
       "4   The 'Create a workspace' pane is used to creat...  SimilaritySearch   \n",
       "..                                                ...               ...   \n",
       "75  The key difference is that the Fabric (Preview...  SimilaritySearch   \n",
       "76  A workspace in Microsoft Fabric is a place whe...  SimilaritySearch   \n",
       "77  Fabric is a Microsoft platform for data manage...  SimilaritySearch   \n",
       "78  A workspace in Microsoft Fabric is a place to ...  SimilaritySearch   \n",
       "79  The 'Create a workspace' pane is used to creat...  SimilaritySearch   \n",
       "\n",
       "   Prompt Style                    Split Method Chunk Size Overlap  \\\n",
       "0   Descriptive  RecursiveCharacterTextSplitter       1000       0   \n",
       "1   Descriptive  RecursiveCharacterTextSplitter       1000       0   \n",
       "2   Descriptive  RecursiveCharacterTextSplitter       1500       0   \n",
       "3   Descriptive  RecursiveCharacterTextSplitter       1500       0   \n",
       "4   Descriptive  RecursiveCharacterTextSplitter       1500     100   \n",
       "..          ...                             ...        ...     ...   \n",
       "75  Descriptive  RecursiveCharacterTextSplitter       1000     150   \n",
       "76  Descriptive  RecursiveCharacterTextSplitter       1000     150   \n",
       "77  Descriptive  RecursiveCharacterTextSplitter       1000     150   \n",
       "78  Descriptive  RecursiveCharacterTextSplitter       1500     100   \n",
       "79  Descriptive  RecursiveCharacterTextSplitter       2000       0   \n",
       "\n",
       "                                         Answer Score  \\\n",
       "0   {'score': 1, 'justification': 'GRADE: Correct\n",
       "...   \n",
       "1   {'score': 1, 'justification': 'GRADE: Correct\n",
       "...   \n",
       "2   {'score': 0, 'justification': 'GRADE: Incorrec...   \n",
       "3   {'score': 0, 'justification': 'GRADE: Incorrec...   \n",
       "4   {'score': 1, 'justification': 'GRADE: Correct\n",
       "...   \n",
       "..                                                ...   \n",
       "75  {'score': 1, 'justification': 'GRADE: Correct\n",
       "...   \n",
       "76  {'score': 1, 'justification': 'GRADE: Correct\n",
       "...   \n",
       "77  {'score': 1, 'justification': 'GRADE: Correct\n",
       "...   \n",
       "78  {'score': 1, 'justification': 'GRADE: Correct\n",
       "...   \n",
       "79  {'score': 1, 'justification': 'GRADE: Correct\n",
       "...   \n",
       "\n",
       "                                      Retrieval Score             Latency  \n",
       "0   {'score': 1, 'justification': 'GRADE: Correct\n",
       "...  3.0535309314727783  \n",
       "1   {'score': 1, 'justification': 'GRADE: Correct\n",
       "...   2.496325969696045  \n",
       "2   {'score': 0, 'justification': 'GRADE: Incorrec...   3.873573064804077  \n",
       "3   {'score': 1, 'justification': 'GRADE: Correct\n",
       "...   2.108778715133667  \n",
       "4   {'score': 1, 'justification': 'GRADE: Correct\n",
       "...  1.9552397727966309  \n",
       "..                                                ...                 ...  \n",
       "75  {'score': 1, 'justification': 'GRADE: Correct\n",
       "...  2.2532236576080322  \n",
       "76  {'score': 1, 'justification': 'GRADE: Correct\n",
       "...  2.2411551475524902  \n",
       "77  {'score': 1, 'justification': 'GRADE: Correct\n",
       "...  2.4773612022399902  \n",
       "78  {'score': 1, 'justification': 'GRADE: Correct\n",
       "...   2.587143659591675  \n",
       "79  {'score': 1, 'justification': 'GRADE: Correct\n",
       "...  2.6436407566070557  \n",
       "\n",
       "[80 rows x 11 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalOutput = []\n",
    "r = getEvaluatorResult(SearchService, SearchKey, evaluatorRunResultIndexName, documentId)\n",
    "for run in r:\n",
    "    finalOutput.append({\n",
    "        \"Question\": run['question'],\n",
    "        \"Answer\": run['answer'],\n",
    "        \"Predicted Answer\": run['predictedAnswer'],\n",
    "        \"Retriever Type\": run['retrieverType'],\n",
    "        \"Prompt Style\": run['promptStyle'],\n",
    "        \"Split Method\": run['splitMethod'],\n",
    "        \"Chunk Size\": run['chunkSize'],\n",
    "        \"Overlap\": run['overlap'],\n",
    "        \"Answer Score\": json.loads(run['answerScore']),\n",
    "        \"Retrieval Score\": json.loads(run['retrievalScore']),\n",
    "        \"Latency\": run['latency'],\n",
    "    })\n",
    "df = pd.DataFrame(finalOutput)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What is Microsoft Fabric\"\n",
    "# #answer = retriever.get_relevant_documents(question)\n",
    "# answer = vectorStoreChain({\"query\": question}, return_only_outputs=True)\n",
    "# answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
