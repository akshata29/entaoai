{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "[LangChain](https://python.langchain.com/en/latest/index.html) is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model, but will also be:\n",
    "- Data-aware: connect a language model to other sources of data\n",
    "- Agentic: allow a language model to interact with its environment\n",
    "\n",
    "The LangChain framework is designed around these principles.\n",
    "\n",
    "We will use Langchain framework for rest of the workshop."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question Answering over the docs/index\n",
    "Question answering in this context refers to question answering over your document data.  For question answering over many documents, you almost always want to create an index over the data. This can be used to smartly access the most relevant documents for a given question, allowing you to avoid having to pass all the documents to the LLM (saving you time and money)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import openai\n",
    "from Utilities.envVars import *\n",
    "\n",
    "# Set Search Service endpoint, index name, and API key from environment variables\n",
    "indexName = SearchIndex\n",
    "\n",
    "# Set OpenAI API key and endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OpenAiVersion\n",
    "openai_api_key = OpenAiKey\n",
    "assert openai_api_key, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = openai_api_key\n",
    "openAiEndPoint = f\"https://{OpenAiService}.openai.azure.com\"\n",
    "assert openAiEndPoint, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in openAiEndPoint.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = openAiEndPoint\n",
    "davincimodel = OpenAiDavinci\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate answer for a question from the document we already indexed in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms.openai import AzureOpenAI, OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from Utilities.cogSearch import performCogSearch\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "embeddingModelType = \"openai\"\n",
    "temperature = 0.3\n",
    "tokenLength = 1000\n",
    "\n",
    "if (embeddingModelType == 'azureopenai'):\n",
    "    openai.api_type = \"azure\"\n",
    "    openai.api_key = OpenAiKey\n",
    "    openai.api_version = OpenAiVersion\n",
    "    openai.api_base = f\"https://{OpenAiService}.openai.azure.com\"\n",
    "\n",
    "    llm = AzureOpenAI(deployment_name=OpenAiDavinci,\n",
    "            temperature=temperature,\n",
    "            openai_api_key=OpenAiKey,\n",
    "            max_tokens=tokenLength,\n",
    "            batch_size=10, \n",
    "            max_retries=12)\n",
    "\n",
    "    logging.info(\"LLM Setup done\")\n",
    "    embeddings = OpenAIEmbeddings(model=OpenAiEmbedding, chunk_size=1, openai_api_key=OpenAiKey)\n",
    "elif embeddingModelType == \"openai\":\n",
    "    openai.api_type = \"open_ai\"\n",
    "    openai.api_base = \"https://api.openai.com/v1\"\n",
    "    openai.api_version = '2020-11-07' \n",
    "    openai.api_key = OpenAiApiKey\n",
    "    llm = OpenAI(temperature=temperature,\n",
    "            openai_api_key=OpenAiApiKey,\n",
    "            max_tokens=tokenLength)\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=OpenAiApiKey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place.\n",
      "SOURCES: Fabric Get Started.pdf\n"
     ]
    }
   ],
   "source": [
    "# We already created our index and loaded the data, so we can skip that part. Let's try to ask a question:\n",
    "# Question answering involves fetching multiple documents, and then asking a question of them. \n",
    "# The LLM response will contain the answer to your question, based on the content of the documents.\n",
    "# The simplest way of using Langchain and LLM is to use load_qa_chain and run it with a query and a list of documents.\n",
    "\n",
    "chainType = \"stuff\"\n",
    "topK = 3\n",
    "query = \"What is Microsoft Fabric\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiService, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType)\n",
    "#qaChain.run(input_documents=docs, question=query)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about we ask a question for which the answer is not in the document we have indexed in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The CEO of Microsoft is Satya Nadella.\n",
      "SOURCES: https://www.microsoft.com/en-us/microsoft-365/blog/2014/02/04/satya-nadella-named-ceo-of-microsoft/\n"
     ]
    }
   ],
   "source": [
    "#query = \"Tell me a Joke\"\n",
    "chainType = \"stuff\"\n",
    "topK = 3\n",
    "query = \"Who is the CEO of Microsoft\"\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiService, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType)\n",
    "#qaChain.run(input_documents=docs, question=query)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we don't want to have LLM answer the question outside of the document we have indexed in Vector Store. We can use the custom prompt to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "chainType = \"stuff\"\n",
    "topK = 3\n",
    "query = \"Who is the CEO of Microsoft\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiService, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "\n",
    "template = \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n",
    "            If the answer is not contained within the text below, say \\\"I don't know\\\".\n",
    "\n",
    "            QUESTION: {question}\n",
    "            =========\n",
    "            {summaries}\n",
    "            =========\n",
    "            \"\"\"\n",
    "#qaPrompt = load_prompt('lc://prompts/qa_with_sources/stuff/basic.json')\n",
    "qaPrompt = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType, prompt=qaPrompt)\n",
    "#qaChain.run(input_documents=docs, question=query)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain type\n",
    "This category of chains are used for interacting with indexes. The purpose these chains is to combine your own data (stored in the indexes) with LLMs. The best example of this is question answering over your own documents.\n",
    "\n",
    "A big part of this is understanding how to pass multiple documents to the language model. There are a few different methods, or chains, for doing so. LangChain supports four of the more common ones - and we are actively looking to include more, so if you have any ideas please reach out! Note that there is not one best method - the decision of which one to use is often very context specific. In order from simplest to most complex\n",
    "\n",
    "##### Stuff\n",
    "Stuffing is the simplest method, whereby you simply stuff all the related data into the prompt as context to pass to the language model. This is implemented in LangChain as the StuffDocumentsChain.\n",
    "\n",
    "- Pros: Only makes a single call to the LLM. When generating text, the LLM has access to all the data at once.\n",
    "- Cons: Most LLMs have a context length, and for large documents (or many documents) this will not work as it will result in a prompt larger than the context length.\n",
    "\n",
    "##### Map-Reduce\n",
    "This method involves running an initial prompt on each chunk of data (for summarization tasks, this could be a summary of that chunk; for question-answering tasks, it could be an answer based solely on that chunk). Then a different prompt is run to combine all the initial outputs. This is implemented in the LangChain as the MapReduceDocumentsChain.\n",
    "\n",
    "- Pros: Can scale to larger documents (and more documents) than StuffDocumentsChain. The calls to the LLM on individual documents are independent and can therefore be parallelized.\n",
    "- Cons: Requires many more calls to the LLM than StuffDocumentsChain. Loses some information during the final combined call.\n",
    "\n",
    "##### Refine\n",
    "This method involves running an initial prompt on the first chunk of data, generating some output. For the remaining documents, that output is passed in, along with the next document, asking the LLM to refine the output based on the new document.\n",
    "\n",
    "- Pros: Can pull in more relevant context, and may be less lossy than MapReduceDocumentsChain.\n",
    "- Cons: Requires many more calls to the LLM than StuffDocumentsChain. The calls are also NOT independent, meaning they cannot be paralleled like MapReduceDocumentsChain. There is also some potential dependencies on the ordering of the documents.\n",
    "\n",
    "##### Map-Rerank\n",
    "This method involves running an initial prompt on each chunk of data, that not only tries to complete a task but also gives a score for how certain it is in its answer. The responses are then ranked according to this score, and the highest score is returned.\n",
    "\n",
    "- Pros: Similar pros as MapReduceDocumentsChain. Requires fewer calls, compared to MapReduceDocumentsChain.\n",
    "- Cons: Cannot combine information between documents. This means it is most useful when you expect there to be a single simple answer in a single document."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's test the same question with Map Reduce Chaintype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. Fabric brings together new and existing components from Power BI, Azure Synapse, and Azure Data Explorer into a single integrated environment. These components are then presented in various customized user experiences, such as Data Engineering, Data Factory, Data Science, Data Warehouse, Real-Time Analytics, and Power BI. It integrates with Azure Machine Learning to provide built-in experiment tracking.\n",
      "SOURCES: Fabric Get Started.pdf\n"
     ]
    }
   ],
   "source": [
    "topK = 3\n",
    "query = \"What is Microsoft Fabric\"\n",
    "chainType = \"map_reduce\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiService, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "\n",
    "qaTemplate = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question.\n",
    "            Return any relevant text.\n",
    "            {context}\n",
    "            Question: {question}\n",
    "            Relevant text, if any :\"\"\"\n",
    "\n",
    "qaPrompt = PromptTemplate(\n",
    "    template=qaTemplate, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "combinePromptTemplate = \"\"\"Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\").\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "\"\"\"\n",
    "combinePrompt = PromptTemplate(\n",
    "    template=combinePromptTemplate, input_variables=[\"summaries\", \"question\"]\n",
    ")\n",
    "\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType, question_prompt=qaPrompt, \n",
    "                                     combine_prompt=combinePrompt, \n",
    "                                     return_intermediate_steps=True)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query})\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b>  Microsoft Fabric offers the comprehensive set of analytics experiences designed to work together seamlessly. Each experience is tailored to a specific persona and a specific task. Fabric includes industry-leading experiences in the following categories for an end-to-end analytical need. Data Engineering - Data Engineering experience provides a world class Spark platform with great authoring experiences, enabling data engineers to perform large scale data transformation and democratize data through the lakehouse. Microsoft Fabric Spark's integration with Data Factory enables notebooks and spark jobs to be scheduled and orchestrated. Data Factory - Azure Data Factory combines the simplicity of Power Query with the scale and power of Azure Data Factory. You can use more than 200 native connectors to connect to data sources on-premises and in the cloud. Data Science - Data Science experience enables you to build, deploy, and operationalize machine learning models seamlessly within your Fabric experience. It integrates with Azure Machine Learning to provide built-in experiment tracking."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b>  Microsoft Fabric brings together new and existing components from Power BI, Azure Synapse, and Azure Data Explorer into a single integrated environment. These components are then presented in various customized user experiences. Fabric brings together experiences such as Data Engineering, Data Factory, Data Science, Data Warehouse, Real-Time Analytics, and Power BI onto a shared SaaS foundation."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b>  Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. With Fabric, you don't need to piece together different services from multiple vendors. Instead, you can enjoy a highly integrated, end-to-end, and easy-to-use product that is designed to simplify your analytics needs. The platform is built on a foundation of Software as a Service (SaaS), which takes simplicity and integration to a whole new level."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the chaintype of MapReduce and Refine, we can also get insight into intermediate steps of the pipeline.\n",
    "# This way you can inspect the results from map_reduce chain type, each top similar chunk summary\n",
    "intermediateSteps = answer['intermediate_steps']\n",
    "for step in intermediateSteps:\n",
    "        display(HTML(\"<b>Chunk Summary:</b> \" + step))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This time with Refine Chain Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Microsoft Fabric is a comprehensive set of analytics experiences designed to work together seamlessly. It includes industry-leading experiences in the categories of data engineering, data factory, and data science. Currently, Microsoft Fabric is in PREVIEW, and brings together new and existing components from Power BI, Azure Synapse, and Azure Data Explorer into a single integrated environment. These components are then presented in various customized user experiences, and are presented on a shared SaaS foundation. This integration provides an extensive range of deeply integrated analytics in the industry, shared experiences across experiences that are familiar and easy to learn, developers can easily access and reuse all assets, a unified data lake that allows you to retain the data where it is while using your preferred analytics tools, and centralized administration and governance across all experiences.\n",
      "\n",
      "Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. With Fabric, you don't need to piece together different services from multiple vendors. Instead, you can enjoy a highly integrated, end-to-end, and easy-to-use product that is designed to simplify your analytics needs. The platform is built on a foundation of Software as a Service (SaaS), which takes simplicity and integration to a whole new level.\n",
      "\n",
      "Source: Fabric Get Started.pdf\n"
     ]
    }
   ],
   "source": [
    "topK = 3\n",
    "query = \"What is Microsoft Fabric\"\n",
    "chainType = \"refine\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiService, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "    \n",
    "refineTemplate = (\n",
    "                    \"The original question is as follows: {question}\\n\"\n",
    "                    \"We have provided an existing answer, including sources: {existing_answer}\\n\"\n",
    "                    \"We have the opportunity to refine the existing answer\"\n",
    "                    \"(only if needed) with some more context below.\\n\"\n",
    "                    \"------------\\n\"\n",
    "                    \"{context_str}\\n\"\n",
    "                    \"------------\\n\"\n",
    "                    \"Given the new context, refine the original answer to better \"\n",
    "                    \"If you do update it, please update the sources as well. \"\n",
    "                    \"If the context isn't useful, return the original answer.\"\n",
    "                )\n",
    "refinePrompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"existing_answer\", \"context_str\"],\n",
    "    template=refineTemplate,\n",
    ")\n",
    "\n",
    "qaTemplate = (\n",
    "    \"Answer the question as truthfully as possible using the provided text below, and if the answer is not contained within the text below, say \\\"I don't know\\\"\\n\"\n",
    "    \"Context information is below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the question: {question}\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    ")\n",
    "qaPrompt = PromptTemplate(\n",
    "    input_variables=[\"context_str\", \"question\"], template=qaTemplate\n",
    ")\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType, question_prompt=qaPrompt, refine_prompt=refinePrompt,\n",
    "                                     return_intermediate_steps=True)\n",
    "\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "modifiedAnswer = answer['output_text']\n",
    "print(modifiedAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is a comprehensive set of analytics experiences designed to work together seamlessly. It includes industry-leading experiences in the categories of data engineering, data factory, and data science."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> \n",
       "\n",
       "Microsoft Fabric is a comprehensive set of analytics experiences designed to work together seamlessly. It includes industry-leading experiences in the categories of data engineering, data factory, and data science. Currently, Microsoft Fabric is in PREVIEW, and brings together new and existing components from Power BI, Azure Synapse, and Azure Data Explorer into a single integrated environment. These components are then presented in various customized user experiences, and are presented on a shared SaaS foundation. This integration provides an extensive range of deeply integrated analytics in the industry, shared experiences across experiences that are familiar and easy to learn, developers can easily access and reuse all assets, a unified data lake that allows you to retain the data where it is while using your preferred analytics tools, and centralized administration and governance across all experiences.\n",
       "\n",
       "Source: Fabric Get Started.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> \n",
       "\n",
       "Microsoft Fabric is a comprehensive set of analytics experiences designed to work together seamlessly. It includes industry-leading experiences in the categories of data engineering, data factory, and data science. Currently, Microsoft Fabric is in PREVIEW, and brings together new and existing components from Power BI, Azure Synapse, and Azure Data Explorer into a single integrated environment. These components are then presented in various customized user experiences, and are presented on a shared SaaS foundation. This integration provides an extensive range of deeply integrated analytics in the industry, shared experiences across experiences that are familiar and easy to learn, developers can easily access and reuse all assets, a unified data lake that allows you to retain the data where it is while using your preferred analytics tools, and centralized administration and governance across all experiences.\n",
       "\n",
       "Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. With Fabric, you don't need to piece together different services from multiple vendors. Instead, you can enjoy a highly integrated, end-to-end, and easy-to-use product that is designed to simplify your analytics needs. The platform is built on a foundation of Software as a Service (SaaS), which takes simplicity and integration to a whole new level.\n",
       "\n",
       "Source: Fabric Get Started.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the chaintype of MapReduce and Refine, we can also get insight into intermediate steps of the pipeline.\n",
    "# This way you can inspect the results from map_reduce chain type, each top similar chunk summary\n",
    "intermediateSteps = answer['intermediate_steps']\n",
    "for step in intermediateSteps:\n",
    "        display(HTML(\"<b>Chunk Summary:</b> \" + step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
