{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "[LangChain](https://python.langchain.com/en/latest/index.html) is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model, but will also be:\n",
    "- Data-aware: connect a language model to other sources of data\n",
    "- Agentic: allow a language model to interact with its environment\n",
    "\n",
    "The LangChain framework is designed around these principles.\n",
    "\n",
    "We will use Langchain framework for rest of the workshop."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question Answering over the docs/index\n",
    "Question answering in this context refers to question answering over your document data.  For question answering over many documents, you almost always want to create an index over the data. This can be used to smartly access the most relevant documents for a given question, allowing you to avoid having to pass all the documents to the LLM (saving you time and money)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import openai\n",
    "from Utilities.envVars import *\n",
    "\n",
    "# Set Search Service endpoint, index name, and API key from environment variables\n",
    "indexName = SearchIndex\n",
    "\n",
    "# Set OpenAI API key and endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OpenAiVersion\n",
    "openai_api_key = OpenAiKey\n",
    "assert openai_api_key, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = openai_api_key\n",
    "openAiEndPoint = f\"https://{OpenAiService}.openai.azure.com\"\n",
    "assert openAiEndPoint, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in openAiEndPoint.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = openAiEndPoint\n",
    "davincimodel = OpenAiDavinci\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate answer for a question from the document we already indexed in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from Utilities.cogSearch import performCogSearch\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "embeddingModelType = \"openai\"\n",
    "temperature = 0.3\n",
    "tokenLength = 1000\n",
    "\n",
    "if (embeddingModelType == 'azureopenai'):\n",
    "        openai.api_type = \"azure\"\n",
    "        openai.api_key = OpenAiKey\n",
    "        openai.api_version = OpenAiVersion\n",
    "        openai.api_base = f\"https://{OpenAiService}.openai.azure.com\"\n",
    "\n",
    "        llm = AzureChatOpenAI(\n",
    "                openai_api_base=openai.api_base,\n",
    "                openai_api_version=OpenAiVersion,\n",
    "                deployment_name=OpenAiChat,\n",
    "                temperature=temperature,\n",
    "                openai_api_key=OpenAiKey,\n",
    "                openai_api_type=\"azure\",\n",
    "                max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(model=OpenAiEmbedding, chunk_size=1, openai_api_key=OpenAiKey)\n",
    "        logging.info(\"LLM Setup done\")\n",
    "elif embeddingModelType == \"openai\":\n",
    "        openai.api_type = \"open_ai\"\n",
    "        openai.api_base = \"https://api.openai.com/v1\"\n",
    "        openai.api_version = '2020-11-07' \n",
    "        openai.api_key = OpenAiApiKey\n",
    "        llm = ChatOpenAI(temperature=temperature,\n",
    "        openai_api_key=OpenAiApiKey,\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=OpenAiApiKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is a unified platform that provides a comprehensive suite of services for enterprises, including data engineering, data integration, data science, Real-Time Analytics, and business intelligence. It is currently in PREVIEW and is a Software as a Service (SaaS) foundation. Fabric allows creators to concentrate on producing their best work, freeing them from the need to integrate, manage, or understand the underlying infrastructure that supports the experience. Fabric is built on a foundation of Software as a Service (SaaS), which takes simplicity and integration to a whole new level. \n",
      "SOURCES: Fabric Get Started.pdf\n"
     ]
    }
   ],
   "source": [
    "# We already created our index and loaded the data, so we can skip that part. Let's try to ask a question:\n",
    "# Question answering involves fetching multiple documents, and then asking a question of them. \n",
    "# The LLM response will contain the answer to your question, based on the content of the documents.\n",
    "# The simplest way of using Langchain and LLM is to use load_qa_chain and run it with a query and a list of documents.\n",
    "\n",
    "chainType = \"stuff\"\n",
    "topK = 3\n",
    "query = \"What is Microsoft Fabric\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiService, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType)\n",
    "#qaChain.run(input_documents=docs, question=query)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about we ask a question for which the answer is not in the document we have indexed in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satya Nadella is the CEO of Microsoft.\n",
      "SOURCES: None provided.\n"
     ]
    }
   ],
   "source": [
    "#query = \"Tell me a Joke\"\n",
    "chainType = \"stuff\"\n",
    "topK = 3\n",
    "query = \"Who is the CEO of Microsoft\"\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiService, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType)\n",
    "#qaChain.run(input_documents=docs, question=query)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we don't want to have LLM answer the question outside of the document we have indexed in Vector Store. We can use the custom prompt to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4b3bd5eaeb1d01fe96d8356c415e68b5 in your message.).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The extracted parts of the document do not provide information about the CEO of Microsoft.\n"
     ]
    }
   ],
   "source": [
    "chainType = \"stuff\"\n",
    "topK = 3\n",
    "query = \"Who is the CEO of Microsoft\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiService, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "\n",
    "template = \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n",
    "            If the answer is not contained within the text below, say \\\"I don't know\\\".\n",
    "\n",
    "            QUESTION: {question}\n",
    "            =========\n",
    "            {summaries}\n",
    "            =========\n",
    "            \"\"\"\n",
    "#qaPrompt = load_prompt('lc://prompts/qa_with_sources/stuff/basic.json')\n",
    "qaPrompt = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType, prompt=qaPrompt)\n",
    "#qaChain.run(input_documents=docs, question=query)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain type\n",
    "This category of chains are used for interacting with indexes. The purpose these chains is to combine your own data (stored in the indexes) with LLMs. The best example of this is question answering over your own documents.\n",
    "\n",
    "A big part of this is understanding how to pass multiple documents to the language model. There are a few different methods, or chains, for doing so. LangChain supports four of the more common ones - and we are actively looking to include more, so if you have any ideas please reach out! Note that there is not one best method - the decision of which one to use is often very context specific. In order from simplest to most complex\n",
    "\n",
    "##### Stuff\n",
    "Stuffing is the simplest method, whereby you simply stuff all the related data into the prompt as context to pass to the language model. This is implemented in LangChain as the StuffDocumentsChain.\n",
    "\n",
    "- Pros: Only makes a single call to the LLM. When generating text, the LLM has access to all the data at once.\n",
    "- Cons: Most LLMs have a context length, and for large documents (or many documents) this will not work as it will result in a prompt larger than the context length.\n",
    "\n",
    "##### Map-Reduce\n",
    "This method involves running an initial prompt on each chunk of data (for summarization tasks, this could be a summary of that chunk; for question-answering tasks, it could be an answer based solely on that chunk). Then a different prompt is run to combine all the initial outputs. This is implemented in the LangChain as the MapReduceDocumentsChain.\n",
    "\n",
    "- Pros: Can scale to larger documents (and more documents) than StuffDocumentsChain. The calls to the LLM on individual documents are independent and can therefore be parallelized.\n",
    "- Cons: Requires many more calls to the LLM than StuffDocumentsChain. Loses some information during the final combined call.\n",
    "\n",
    "##### Refine\n",
    "This method involves running an initial prompt on the first chunk of data, generating some output. For the remaining documents, that output is passed in, along with the next document, asking the LLM to refine the output based on the new document.\n",
    "\n",
    "- Pros: Can pull in more relevant context, and may be less lossy than MapReduceDocumentsChain.\n",
    "- Cons: Requires many more calls to the LLM than StuffDocumentsChain. The calls are also NOT independent, meaning they cannot be paralleled like MapReduceDocumentsChain. There is also some potential dependencies on the ordering of the documents.\n",
    "\n",
    "##### Map-Rerank\n",
    "This method involves running an initial prompt on each chunk of data, that not only tries to complete a task but also gives a score for how certain it is in its answer. The responses are then ranked according to this score, and the highest score is returned.\n",
    "\n",
    "- Pros: Similar pros as MapReduceDocumentsChain. Requires fewer calls, compared to MapReduceDocumentsChain.\n",
    "- Cons: Cannot combine information between documents. This means it is most useful when you expect there to be a single simple answer in a single document."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's test the same question with Map Reduce Chaintype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER: Microsoft Fabric is a comprehensive set of analytics experiences designed to work together seamlessly. It is a platform that brings together new and existing components from Power BI, Azure Synapse, and Azure Data Explorer into a single integrated environment. Fabric allows creators to concentrate on producing their best work, freeing them from the need to integrate, manage, or understand the underlying infrastructure that supports the experience. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. SOURCES: Fabric Get Started.pdf\n"
     ]
    }
   ],
   "source": [
    "topK = 3\n",
    "query = \"What is Microsoft Fabric\"\n",
    "chainType = \"map_reduce\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiService, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "\n",
    "qaTemplate = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question.\n",
    "            Return any relevant text.\n",
    "            {context}\n",
    "            Question: {question}\n",
    "            Relevant text, if any :\"\"\"\n",
    "\n",
    "qaPrompt = PromptTemplate(\n",
    "    template=qaTemplate, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "combinePromptTemplate = \"\"\"Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\").\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "\"\"\"\n",
    "combinePrompt = PromptTemplate(\n",
    "    template=combinePromptTemplate, input_variables=[\"summaries\", \"question\"]\n",
    ")\n",
    "\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType, question_prompt=qaPrompt, \n",
    "                                     combine_prompt=combinePrompt, \n",
    "                                     return_intermediate_steps=True)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query})\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is a comprehensive set of analytics experiences designed to work together seamlessly. It includes industry-leading experiences in data engineering, data factory, and data science for an end-to-end analytical need. Fabric allows creators to concentrate on producing their best work, freeing them from the need to integrate, manage, or understand the underlying infrastructure that supports the experience."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is a platform that brings together new and existing components from Power BI, Azure Synapse, and Azure Data Explorer into a single integrated environment. It provides various customized user experiences and integrates experiences such as Data Engineering, Data Factory, Data Science, Data Warehouse, Real-Time Analytics, and Power BI onto a shared SaaS foundation."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> \"What is Microsoft Fabric?\n",
       "\n",
       "Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place.\""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the chaintype of MapReduce and Refine, we can also get insight into intermediate steps of the pipeline.\n",
    "# This way you can inspect the results from map_reduce chain type, each top similar chunk summary\n",
    "intermediateSteps = answer['intermediate_steps']\n",
    "for step in intermediateSteps:\n",
    "        display(HTML(\"<b>Chunk Summary:</b> \" + step))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This time with Refine Chain Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The existing answer is already comprehensive and accurate. The additional context provided in the PDF does not require any updates to the original answer or sources.\n"
     ]
    }
   ],
   "source": [
    "topK = 3\n",
    "query = \"What is Microsoft Fabric\"\n",
    "chainType = \"refine\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiService, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "    \n",
    "refineTemplate = (\n",
    "                    \"The original question is as follows: {question}\\n\"\n",
    "                    \"We have provided an existing answer, including sources: {existing_answer}\\n\"\n",
    "                    \"We have the opportunity to refine the existing answer\"\n",
    "                    \"(only if needed) with some more context below.\\n\"\n",
    "                    \"------------\\n\"\n",
    "                    \"{context_str}\\n\"\n",
    "                    \"------------\\n\"\n",
    "                    \"Given the new context, refine the original answer to better \"\n",
    "                    \"If you do update it, please update the sources as well. \"\n",
    "                    \"If the context isn't useful, return the original answer.\"\n",
    "                )\n",
    "refinePrompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"existing_answer\", \"context_str\"],\n",
    "    template=refineTemplate,\n",
    ")\n",
    "\n",
    "qaTemplate = (\n",
    "    \"Answer the question as truthfully as possible using the provided text below, and if the answer is not contained within the text below, say \\\"I don't know\\\"\\n\"\n",
    "    \"Context information is below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the question: {question}\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    ")\n",
    "qaPrompt = PromptTemplate(\n",
    "    input_variables=[\"context_str\", \"question\"], template=qaTemplate\n",
    ")\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType, question_prompt=qaPrompt, refine_prompt=refinePrompt,\n",
    "                                     return_intermediate_steps=True)\n",
    "\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "modifiedAnswer = answer['output_text']\n",
    "print(modifiedAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is a platform that offers a comprehensive set of analytics experiences designed to work together seamlessly, tailored to specific personas and tasks, and freeing creators from the need to integrate, manage, or understand the underlying infrastructure that supports the experience. It includes industry-leading experiences in data engineering, data factory, and data science."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Refined Answer: Microsoft Fabric is a SaaS platform that integrates various components from Power BI, Azure Synapse, and Azure Data Explorer into a single environment, offering a comprehensive set of analytics experiences. It includes industry-leading experiences in data engineering, data factory, data science, data warehouse, real-time analytics, and Power BI. Fabric provides a shared SaaS foundation that allows for centralized administration and governance across all experiences. With Fabric, IT teams can configure core enterprise capabilities centrally, and permissions are automatically applied across all underlying services. The platform also offers a unified data lake that retains data where it is while allowing users to use their preferred analytics tools. Microsoft Fabric is currently in preview and may be subject to substantial modifications before its release. \n",
       "\n",
       "Source: Fabric Get Started.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The existing answer is already comprehensive and accurate. The additional context provided in the PDF does not require any updates to the original answer or sources."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the chaintype of MapReduce and Refine, we can also get insight into intermediate steps of the pipeline.\n",
    "# This way you can inspect the results from map_reduce chain type, each top similar chunk summary\n",
    "intermediateSteps = answer['intermediate_steps']\n",
    "for step in intermediateSteps:\n",
    "        display(HTML(\"<b>Chunk Summary:</b> \" + step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
