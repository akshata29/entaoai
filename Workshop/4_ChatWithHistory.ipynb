{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbots with History\n",
    "ChatGPT took the world by storm by exposing a powerful language model with a new interface - chat. There are several components that go into building a chatbot.\n",
    "\n",
    "- The model - you can construct a chatbot from a normal language model or a Chat Model. The important thing to remember is that even if you are using a Chat Model, the API itself is stateless, meaning it won't remember previous interactions - you have to pass them in.\n",
    "- PromptTemplate - this will guide how your chatbot acts. Are they sassy? Helpful? These can be used to give your chatbot some character.\n",
    "- Memory - as mentioned above, the models themselves are stateless. Memory brings some concept of state to the table, allowing it remember previous interactions\n",
    "\n",
    "Chatbots are often very powerful and more differentiated when combined with other sources of data. The same techniques that underpin \"Question Answering Over Docs\" can also be used here to give your chatbot access to that data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import openai\n",
    "from Utilities.envVars import *\n",
    "\n",
    "# Set Search Service endpoint, index name, and API key from environment variables\n",
    "indexName = SearchIndex\n",
    "\n",
    "# Set OpenAI API key and endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OpenAiVersion\n",
    "openai_api_key = OpenAiKey\n",
    "assert openai_api_key, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = openai_api_key\n",
    "openAiEndPoint = f\"https://{OpenAiService}.openai.azure.com\"\n",
    "assert openAiEndPoint, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in openAiEndPoint.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = openAiEndPoint\n",
    "davincimodel = OpenAiDavinci\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's chat on same index document we created in previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from Utilities.cogSearch import performCogSearch\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, HTML\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from Utilities.cogSearchRetriever import CognitiveSearchRetriever\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "embeddingModelType = \"azureopenai\"\n",
    "temperature = 0.3\n",
    "tokenLength = 1000\n",
    "\n",
    "if (embeddingModelType == 'azureopenai'):\n",
    "        baseUrl = f\"https://{OpenAiService}.openai.azure.com\"\n",
    "        openai.api_type = \"azure\"\n",
    "        openai.api_key = OpenAiKey\n",
    "        openai.api_version = OpenAiVersion\n",
    "        openai.api_base = f\"https://{OpenAiService}.openai.azure.com\"\n",
    "\n",
    "        llm = AzureChatOpenAI(\n",
    "                    openai_api_base=baseUrl,\n",
    "                    openai_api_version=OpenAiVersion,\n",
    "                    deployment_name=OpenAiChat,\n",
    "                    temperature=temperature,\n",
    "                    openai_api_key=OpenAiKey,\n",
    "                    openai_api_type=\"azure\",\n",
    "                    max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(model=OpenAiEmbedding, chunk_size=1, openai_api_key=OpenAiKey)\n",
    "        \n",
    "elif embeddingModelType == \"openai\":\n",
    "        openai.api_type = \"open_ai\"\n",
    "        openai.api_base = \"https://api.openai.com/v1\"\n",
    "        openai.api_version = '2020-11-07' \n",
    "        openai.api_key = OpenAiApiKey\n",
    "        llm = ChatOpenAI(temperature=temperature,\n",
    "                openai_api_key=OpenAiApiKey,\n",
    "                max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=OpenAiApiKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now create a memory object, which is neccessary to track the inputs/outputs and hold a conversation.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is a platform that offers a comprehensive set of analytics experiences designed to work together seamlessly. It includes industry-leading experiences in categories such as Data Engineering, Data Factory, Data Science, and OneLake. Microsoft Fabric allows creators to concentrate on producing their best work, freeing them from the need to integrate, manage, or understand the underlying infrastructure that supports the experience.\n"
     ]
    }
   ],
   "source": [
    "topK = 3\n",
    "\n",
    "# We can now create a retriever object, which is neccessary to retrieve documents from the index.\n",
    "retriever = CognitiveSearchRetriever(content_key=\"content\",\n",
    "                                                  service_name=SearchService,\n",
    "                                                  api_key=SearchKey,\n",
    "                                                  index_name=indexName,\n",
    "                                                  topK=topK)\n",
    "\n",
    "## Use the ConversationalRetrievalChain to combine the LLM, retriever, and memory into a single object.\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory)\n",
    "query = \"What is Microsoft Fabric\"\n",
    "answer = qa({\"question\": query})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Microsoft Fabric supports real-time analytics use-cases through its Synapse Real-Time Analytics functionality. The KQL database and KQL Queryset components allow users to query and manipulate real-time data from their Data Explorer database.\n"
     ]
    }
   ],
   "source": [
    "query = \"Does it support real-time analytics use-cases?\"\n",
    "answer = qa({\"question\": query})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is a set of analytics experiences designed to work together seamlessly. It includes industry-leading experiences in data engineering, data factory, data science, and OneLake. Fabric allows creators to concentrate on producing their best work, freeing them from the need to integrate, manage, or understand the underlying infrastructure that supports the experience.\n"
     ]
    }
   ],
   "source": [
    "# In the above example, we used a Memory object to track chat history. We can also just pass it in explicitly. \n",
    "# In order to do this, we need to initialize a chain without any memory object.\n",
    "chat_history = []\n",
    "\n",
    "qaNoMemory = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
    "query = \"What is Microsoft Fabric\"\n",
    "answer = qaNoMemory({\"question\": query, \"chat_history\": chat_history})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Microsoft Fabric supports real-time analytics use-cases through its Synapse Real-Time Analytics feature. The KQL database and KQL Queryset components allow users to query and manipulate real-time data from their Data Explorer database.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, answer[\"answer\"])]\n",
    "query = \"Does it support real-time analytics use-cases?\"\n",
    "answer = qaNoMemory({\"question\": query, \"chat_history\": chat_history})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also try using other Chaintype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainType = \"map_reduce\"\n",
    "questionGenerator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "docChain = load_qa_chain(llm, chain_type=chainType)\n",
    "\n",
    "mapRChain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    question_generator=questionGenerator,\n",
    "    combine_docs_chain=docChain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is a platform that offers a comprehensive set of analytics experiences designed to work together seamlessly. It includes industry-leading experiences in data engineering, data factory, and data science for an end-to-end analytical need.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"What is Microsoft Fabric\"\n",
    "answer = mapRChain({\"question\": query, \"chat_history\": chat_history})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no direct mention of Microsoft Fabric supporting real-time analytics use-cases in the given text. However, Synapse Data Warehouse, which is mentioned in the text, supports real-time analytics through Apache Spark.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, answer[\"answer\"])]\n",
    "query = \"Does it support real-time analytics use-cases?\"\n",
    "answer = mapRChain({\"question\": query, \"chat_history\": chat_history})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Does it support real-time analytics use-cases?', 'chat_history': [('What is Microsoft Fabric', 'Microsoft Fabric is a platform that offers a comprehensive set of analytics experiences designed to work together seamlessly. It includes industry-leading experiences in data engineering, data factory, and data science for an end-to-end analytical need.')], 'answer': 'There is no direct mention of Microsoft Fabric supporting real-time analytics use-cases in the given text. However, Synapse Data Warehouse, which is mentioned in the text, supports real-time analytics through Apache Spark.'}\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
