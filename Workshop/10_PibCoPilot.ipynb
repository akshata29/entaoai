{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIB CoPilot\n",
    "PIBs are also used to create a pitchbook by assessing a company's strategy, competitive positioning, review of financial statements, industry dynamics, and trends within the industry. \n",
    "1. Company Overview and Executive Bio - A brief description of the company and its key executives with biographies.\n",
    "2. Conference calls: The same day a company issues its quarterly press release, it will also hold a conference call. On the call, analysts often learn details about management guidance. These conference calls are transcribed by several service providers and can be accessed by subscribers of large financial data providers.\n",
    "3. Press Release: Can be found in the investor relations section of most companies' websites and contains the financial statements which are used in forms 10-K and 10-Q. \n",
    "4. News: News articles that may affect a company's stock price or growth prospect would be something that analysts look into, particularly within a 6-12 month time horizon.\n",
    "5. SEC filings: These regulatory documents require a company to file Form 10-K and Form 10-Q with the SEC on an ongoing basis. Form 10-K is a financial overview and commentary for the last year, usually found on the company's website. Form 10-Q is similar to form 10-K, but it is a report for the last quarter instead of the previous year.\n",
    "6. Equity research reports: Look into key forecasts for metrics like Revenue, EBITDA, and EPS for the company or competing firms to form a consensus estimate. \n",
    "7. Investor Presentations: Companies provide historical information as an important foundation from which forecasts are made to guide key forecasting drivers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0 -  Pre-requsite and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import openai\n",
    "from Utilities.envVars import *\n",
    "import uuid\n",
    "# Set Search Service endpoint, index name, and API key from environment variables\n",
    "indexName = SearchIndex\n",
    "\n",
    "# Set OpenAI API key and endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OpenAiVersion\n",
    "openai_api_key = OpenAiKey\n",
    "assert openai_api_key, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = openai_api_key\n",
    "openAiEndPoint = f\"https://{OpenAiService}.openai.azure.com\"\n",
    "assert openAiEndPoint, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in openAiEndPoint.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = openAiEndPoint\n",
    "davincimodel = OpenAiDavinci\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "embeddingModelType = \"azureopenai\"\n",
    "temperature = 0\n",
    "tokenLength = 1000\n",
    "symbol = 'WF'\n",
    "apikey = FmpKey\n",
    "os.environ['BING_SUBSCRIPTION_KEY'] = BingKey\n",
    "os.environ['BING_SEARCH_URL'] = BingUrl\n",
    "pibIndexName = 'pibdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms.openai import AzureOpenAI, OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, HTML\n",
    "from langchain.utilities import BingSearchAPIWrapper\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "from langchain.prompts import PromptTemplate\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import timedelta\n",
    "from Utilities.pibCopilot import indexDocs, createPressReleaseIndex, createStockNewsIndex, mergeDocs, createPibIndex, findPibData, findEarningCalls, deletePibData, performEarningCallCogSearch\n",
    "from Utilities.pibCopilot import indexEarningCallSections, createEarningCallVectorIndex, createEarningCallIndex, performCogSearch, createSecFilingIndex, findSecFiling\n",
    "from Utilities.pibCopilot import findLatestSecFilings, createSecFilingsVectorIndex, indexSecFilingsSections\n",
    "import typing\n",
    "from Utilities.fmp import *\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "\n",
    "# Flexibility to change the call to OpenAI or Azure OpenAI\n",
    "\n",
    "if (embeddingModelType == 'azureopenai'):\n",
    "    openai.api_type = \"azure\"\n",
    "    openai.api_key = OpenAiKey\n",
    "    openai.api_version = OpenAiVersion\n",
    "    openai.api_base = OpenAiBase\n",
    "\n",
    "    llm = AzureOpenAI(deployment_name=OpenAiDavinci,\n",
    "            temperature=temperature,\n",
    "            openai_api_key=OpenAiKey,\n",
    "            max_tokens=tokenLength,\n",
    "            batch_size=10, \n",
    "            max_retries=12)\n",
    "    \n",
    "    llmChat = AzureChatOpenAI(\n",
    "                openai_api_base=openai.api_base,\n",
    "                openai_api_version=OpenAiVersion,\n",
    "                deployment_name=OpenAiChat16k,\n",
    "                temperature=temperature,\n",
    "                openai_api_key=OpenAiKey,\n",
    "                openai_api_type=\"azure\",\n",
    "                max_tokens=tokenLength)\n",
    "    \n",
    "    logging.info(\"LLM Setup done\")\n",
    "    embeddings = OpenAIEmbeddings(deployment=OpenAiEmbedding, chunk_size=1, openai_api_key=OpenAiKey)\n",
    "elif embeddingModelType == \"openai\":\n",
    "    openai.api_type = \"open_ai\"\n",
    "    openai.api_base = \"https://api.openai.com/v1\"\n",
    "    openai.api_version = '2020-11-07' \n",
    "    openai.api_key = OpenAiApiKey\n",
    "    llm = OpenAI(temperature=temperature,\n",
    "            openai_api_key=OpenAiApiKey,\n",
    "            max_tokens=tokenLength)\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=OpenAiApiKey)\n",
    "\n",
    "    llmChat = ChatOpenAI(temperature=temperature,\n",
    "        openai_api_key=OpenAiApiKey,\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        max_tokens=tokenLength)\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=OpenAiApiKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "central = timezone('US/Central')\n",
    "today = datetime.now(central)\n",
    "currentYear = today.year\n",
    "historicalDate = today - relativedelta(years=3)\n",
    "historicalYear = historicalDate.year\n",
    "historicalDate = historicalDate.strftime(\"%Y-%m-%d\")\n",
    "totalYears = currentYear - historicalYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find CIK based on Symbol\n",
    "cik = str(int(searchCik(apikey=apikey, ticker=symbol)[0][\"companyCik\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#symbol: str = \"AAPL\"\n",
    "#cik = \"320193\"\n",
    "#symbols: typing.List[str] = [\"AAPL\", \"CSCO\", \"QQQQ\"]\n",
    "#exchange: str = \"NYSE\"\n",
    "#exchanges: typing.List[str] = [\"NYSE\", \"NASDAQ\"]\n",
    "#query: str = \"AA\"\n",
    "#limit: int = 3\n",
    "#period: str = \"quarter\"\n",
    "#download: bool = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Paid Data - Company Profile and Key Executives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search index pibdata already exists\n",
      "[{'id': 'c09a8377-bd69-409e-81b0-e4c7a2da6262', 'symbol': 'WF', 'cik': '1264136', 'step': '1', 'description': 'Biography of Key Executives', 'insertedDate': '2023-07-19', 'pibData': '[{\\'name\\': \\'Mr. Byoung-Kwon  Woo\\', \\'title\\': \\'Deputy Pres & Compliance Officer\\', \\'biography\\': \"Immunotherapy has revolutionized cancer treatment by boosting the body\\'s immune system. 4-1BB is an inducible T cell surface receptor that belongs to the nerve growth factor receptor superfamily. Byoung-Kwon Woo is the Sr. MD & Compliance Officer at Woori Inc. Byoung S. Kwon founded Eutilex Co., Ltd. Byoung Kwon Lee is a Professor at National Cancer Center. Byoung-Suk Kweon is an associate professor at the University of Maryland. Byoung Kwon Choi has conducted research on married women\\'s career orientation. PDMS has been used in triboelectric nanogenerators. Byoung Kwon is the President of an organization. Kwon Young-Woo was a pioneering figure in Korean abstract art. Byoung Kwon Kang is a Nurse Practitioner in Tampa, FL. There are professionals named Byoung Kim on LinkedIn.\"}, {\\'name\\': \\'Mr. Tae-Joong  Ha\\', \\'title\\': \\'Executive Vice President of Corporation Banking Bus. Group\\', \\'biography\\': \\'The summary is about various executives and leaders in the banking industry, including Tae-Joong Ha, who is the Executive VP of the Corporate Banking Business Group at Woori Inc. It also mentions other executives at Wells Fargo and Capital One. The summary also discusses the role of a Vice President in a corporate structure and provides information on corporate banking recruitment.\\'}, {\\'name\\': \\'Mr. Sung-Wook  Lee\\', \\'title\\': \\'Deputy Pres of Fin. Planning Unit\\', \\'biography\\': \\'The summary includes information about various individuals named Sung Wook Lee, Sung Wook Chung, Sung Wook Moon, Sung Wook Park, and Min Sung Wook. It mentions their occupations, achievements, and personal details. Additionally, it briefly mentions the 2020 MBC Drama Awards winners and nominees.\\'}, {\\'name\\': \\'Mr. Jong-In  Lee\\', \\'title\\': \\'Executive Vice President of Risk Management Group\\', \\'biography\\': \\'The summary provides information about various executives and their roles in different organizations. It mentions the Executive Committee, which is the management body of a group, and includes the managers of group divisions and heads of main regions. It also mentions specific executives in companies like Boeing, ASML, Citi, LG Electronics, Samsung Electronics, and Daewoong Pharmaceutical. Additionally, it includes information about their positions, personal life, and education.\\'}, {\\'name\\': \\'Mr. Dong-Yeun  Lee\\', \\'title\\': \\'Executive Vice President of IT Group\\', \\'biography\\': \"The summary is about Mr. Dong-Yeun Lee, who is the Executive Vice President of the IT Group at Woori Inc. The text also mentions Wallmine, a financial terminal, and provides contact information for managing products and account information. It briefly mentions other individuals and their positions within different companies. The summary also includes information about Dong-Yeun Lee\\'s biography and mentions his involvement in various markets and tools portfolios.\"}, {\\'name\\': \\'Mr. Chai-Pong  Cheong\\', \\'title\\': \\'Executive Vice President of Bus. Promotion Unit/Retail Banking Bus. Group\\', \\'biography\\': \\'The given text contains a list of individuals holding executive positions in various companies. It includes their names, titles, and brief descriptions of their roles.\\'}, {\\'name\\': \\'Mr. Tae-Seung  Son\\', \\'title\\': \\'Pres, Chief Executive Officer & Director\\', \\'biography\\': \\'Tae-Seung Son is the Chairman, President, and Chief Executive Officer at Woori Financial Group, Inc. He was previously employed as the President and Director at Woori Bank. Son is 61 years old and has been the Chairman and Chief Executive Officer of Woori Inc since 2018. He has a background in finance and has held various executive positions in the industry. Son is also involved in other organizations such as MOORIM PAPER Co., Ltd. and DRB Holding Co., Ltd.\\'}, {\\'name\\': \\'Ms. Jong Sook  Jeong\\', \\'title\\': \\'Deputy Executive Vice President of Wealth Management Group\\', \\'biography\\': \\'The summary includes information about various individuals holding positions in different organizations. It mentions the former President and CEO of KB Kookmin Bank, as well as the former Deputy President and Chief Financial Officer of KB Financial Group Inc. It also highlights the profiles of board members of Wells Fargo, including the Chair and Vice Chair. Additionally, it mentions the President and CEO of Manulife Asia, the President and Director of KH Vatec Co. Ltd, and the Senior Executive Vice President and CEO of Wealth & Investment Management at Wells Fargo. The summary also includes information about Northern Trust Corporation and its services. Lastly, it mentions the involvement of individuals in various leadership roles and their achievements.\\'}, {\\'name\\': \\'Mr. Jong Deuk  Kim\\', \\'title\\': \\'Deputy Executive Vice President of Financial Market Bus. Group\\', \\'biography\\': \"The summary is about various individuals named Kim Jong-deuk and Kim Jong-un. It mentions their positions, promotions, and career history. It also briefly discusses North Korea\\'s nuclear capabilities and the propaganda machine in the country.\"}, {\\'name\\': \\'Mr. Su Hyeong  Cho\\', \\'title\\': \\'Deputy Executive Vice President of Consumer & Brand Group\\', \\'biography\\': \\'The summary is about various individuals named Su-Hyeong Cho and their positions, career history, and education. It also mentions a book by E.J. Koh and a conversation with Su Cho about her debut collection. Additionally, it includes information about a South Korean mass murderer named Seung-Hui Cho and various other unrelated individuals.\\'}, {\\'name\\': \\'Mr. Hong Sik  Choi\\', \\'title\\': \\'Deputy Executive Vice President of Institutional Banking Bus. Group\\', \\'biography\\': \\'Hong-Sik Choi is an Executive Director at Microsoft Korea Inc. and a Board Member at Dynamic Design Co Ltd. He has a successful career history and is involved in various organizations and companies.\\'}, {\\'name\\': \\'Mr. Hwa Jeon  Jae\\', \\'title\\': \\'Deputy MD & Compliance Officer\\', \\'biography\\': \\'Jae-Hwa Jeon is the Deputy Managing Director at Woori Financial Group Inc. She has a career history in finance and holds various memberships. Jeon is also the Compliance Director at Woori Financial Group.\\'}, {\\'name\\': \\'Mr. Hwa Jae  Park\\', \\'title\\': \\'Pres of Bus. Support Group\\', \\'biography\\': \\'A professional paraphrasing tool has been developed for writers, teachers, students, webmasters, and marketers. It rephrases sentences and eliminates plagiarism. It offers high-quality paraphrasing that is coherent and grammatically correct. The tool is useful for creating short bios and making positive impressions for professional development. It also provides information about various individuals, companies, and services, such as Jae-Woo Park, Wells Fargo, and Hwa-Jae Park. Additionally, the tool offers support for small businesses and access to online banking services.\\'}, {\\'name\\': \\'Mr. Tae-Seung  Son\\', \\'title\\': \\'Pres & Director\\', \\'biography\\': \\'Mr. Tae-Seung Son is the Chairman, President, and CEO of Woori Financial Group, Inc. He was previously employed as the President and Director of Woori Bank. Mr. Son is 61 years old and has been in his current position since 2018. He is also on the board of MOORIM PAPER Co., Ltd. and has a background in finance and auditing. In addition to his professional career, Mr. Son is also known for his involvement in the entertainment industry as a South Korean actor.\\'}, {\\'name\\': \\'Mr. Jong-Yong  Yim\\', \\'title\\': \\'Chief Executive Officer & Executive Chairman\\', \\'biography\\': \\'Jong-Yong Yim is the Chairman and CEO of Woori Financial Group Inc. He has previously held positions as Chairman of NongHyup Financial Group Inc. and as a director at CJ Logistics. Yim was recently nominated as the new Chairman of Woori Financial Group. He has extensive experience in the financial industry and has been recognized for his leadership and innovation.\\'}, {\\'name\\': \\'Mr. Kyu-Mok  Hwang\\', \\'title\\': \\'Deputy Pres of Brand Unit\\', \\'biography\\': \\'The summary is about various individuals named Kyu-Mok Hwang and their positions and affiliations in different organizations. It also mentions other individuals with similar names and their roles.\\'}, {\\'name\\': \\'Mr. Sung-Wook  Lee\\', \\'title\\': \\'Senior Executive Vice President of Fin. Section\\', \\'biography\\': \"The summary mentions various individuals named Sung-Wook Lee and provides information about their positions, compensation, career history, and education. It also includes references to companies such as Bubang Co Ltd, DY POWER Corp, POSCO Inc, Woori Financial Group Inc, and TK Chemical Corp. Additionally, it mentions Sung-Wook Lee\\'s involvement in the finance industry and his roles as President of SW LOGISTICS INC and Deputy President at Woori Financial Group Inc. The summary also briefly mentions Sung-Wook Lee\\'s role as a South Korean actor and provides information about Sang-Wook Lee\\'s position as Vice President at Hanwha Solutions Corp.\"}, {\\'name\\': \\'Cheol-soo  Kim\\', \\'title\\': \\'Chief of Department\\', \\'biography\\': \\'The summary includes information about Ahn Cheol-Soo, the founder of AhnLab, South Korea\\\\\\'s largest Internet security firm. It also mentions Cheol-Soo Kim, a physician, educator, politician, and computer entrepreneur. Additionally, it briefly discusses Kim Soo-chul, a South Korean singer and composer. The summary also includes information about various research works and publications by individuals named Kim Cheol-Soo and Kim Soo-chul. It mentions the actor Cheol-Soo Kim and his role in the film \"My Sassy Girl.\" Lastly, it mentions the drama \"Our Beloved Summer\" and the actress Kim Sung Cheol.\\'}, {\\'name\\': \\'Mi-Young  Shin\\', \\'title\\': \\'Head of Assistant\\', \\'biography\\': \\'The summary provides information about multiple individuals named Shin Mi Young and Young Shin, including their education, career, and personal details. It also mentions their involvement in various fields such as acting, academia, and medicine.\\'}, {\\'name\\': \\'Mr. Jong-Yong  Yim\\', \\'title\\': \\'Pres, Chief Executive Officer & Executive Chairman\\', \\'biography\\': \\'The summary provides information about Jong-Yong Yim, who currently holds the position of Chairman and Chief Executive Officer at Woori Financial Group Inc. He has previously served as Chairman for NongHyup Financial Group Inc. The summary also mentions other executives in different industries and provides brief information about them.\\'}]'}, {'id': '79600cc5-235d-4805-9d59-183ba0c8f366', 'symbol': 'WF', 'cik': '1264136', 'step': '1', 'description': 'Company Profile', 'insertedDate': '2023-07-19', 'pibData': \"[{'symbol': 'WF', 'mktCap': 6736963360, 'companyName': 'Woori Financial Group Inc.', 'currency': 'USD', 'cik': '0001264136', 'isin': 'US9810641087', 'exchange': 'New York Stock Exchange', 'industry': 'Banksâ€”Regional', 'sector': 'Financial Services', 'address': '51, Sogong-ro', 'city': 'Seoul', 'state': '', 'zip': '04632', 'website': 'https://www.woorifg.com', 'description': 'Woori Financial Group Inc., together with its subsidiaries, operates as a commercial bank that provides a range of financial services to individual, business, and institutional customers in Korea. It operates through Banking, Credit Card, Capital, Investment Banking, and Others segments. The company offers savings, demand, and installment deposits; time deposits and certificates of deposit; and working capital, facilities, general purpose household, mortgage, and home equity loans. It also offers debit and credit cards, cash services, card loans, and related services; lease financing; and securities operation, sale of financial instruments, project financing, and other related activities. In addition, the company provides credit purchase, cash advance, credit card loans, foreign exchange services and dealing, import and export-related services, offshore lending, syndicated loans and foreign currency securities investment, investment trust products, bancassurance, and private equity funds. Further, it is involved in the real estate, system software development and maintenance, financing, credit information, securities investment and trading, derivatives trading, asset securitization, investment and international banking, money transfer, and other service. The company also offers automated telephone, Internet, and mobile banking services. In addition, it offers trust management, trustee and custodian service, and repurchase instrument. As of December 31, 2021, it served customers through a network of 768 branches and 4,296 ATMs. The company was founded in 1899 and is headquartered in Seoul, South Korea.'}]\"}]\n"
     ]
    }
   ],
   "source": [
    "# Get the information about the company and list of all executives.\n",
    "# Check if we have already created record for Profile\n",
    "createPibIndex(SearchService, SearchKey, pibIndexName)\n",
    "step = \"1\"\n",
    "s1Data = []\n",
    "r = findPibData(SearchService, SearchKey, pibIndexName, cik, step, returnFields=['id', 'symbol', 'cik', 'step', 'description', 'insertedDate',\n",
    "                                                                   'pibData'])\n",
    "if r.get_count() == 0:\n",
    "    step1Profile = []\n",
    "    profile = companyProfile(apikey=apikey, symbol=symbol)\n",
    "    df = pd.DataFrame.from_dict(pd.json_normalize(profile))\n",
    "    sData = {\n",
    "            'id' : str(uuid.uuid4()),\n",
    "            'symbol': symbol,\n",
    "            'cik': cik,\n",
    "            'step': step,\n",
    "            'description': 'Company Profile',\n",
    "            'insertedDate': today.strftime(\"%Y-%m-%d\"),\n",
    "            'pibData' : str(df[['symbol', 'mktCap', 'companyName', 'currency', 'cik', 'isin', 'exchange', 'industry', 'sector', 'address', 'city', 'state', 'zip', 'website', 'description']].to_dict('records'))\n",
    "    }\n",
    "    step1Profile.append(sData)\n",
    "    s1Data.append(sData)\n",
    "    # Insert data into pibIndex\n",
    "    mergeDocs(SearchService, SearchKey, pibIndexName, step1Profile)\n",
    "\n",
    "    # Get the list of all executives and generate biography for each of them\n",
    "    executives = keyExecutives(apikey=apikey, symbol=symbol)\n",
    "    df = pd.DataFrame.from_dict(pd.json_normalize(executives),orient='columns')\n",
    "    df = df.drop_duplicates(subset='name', keep=\"first\")\n",
    "\n",
    "    step1Biography = []\n",
    "    tools = []\n",
    "    topK = 1\n",
    "    step1Executives = []\n",
    "    #### With the company profile and key executives, we can ask Bing Search to get the biography of the all Key executives and \n",
    "    # ask OpenAI to summarize it - Public Data\n",
    "    for executive in executives:\n",
    "        name = executive['name']\n",
    "        title = executive['title']\n",
    "        query = f\"Give me brief biography of {name} who is {title} at {symbol}. Biography should be restricted to {symbol} and summarize it as 2 paragraphs.\"\n",
    "        qaPromptTemplate = \"\"\"\n",
    "            Rephrase the following question asked by user to perform intelligent internet search\n",
    "            {query}\n",
    "            \"\"\"\n",
    "        optimizedPrompt = qaPromptTemplate.format(query=query)\n",
    "        completion = openai.Completion.create(\n",
    "                    engine=OpenAiDavinci,\n",
    "                    prompt=optimizedPrompt,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=100,\n",
    "                    n=1)\n",
    "        q = completion.choices[0].text\n",
    "        bingSearch = BingSearchAPIWrapper(k=25)\n",
    "        results = bingSearch.run(query=q)\n",
    "        chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "        docs = [Document(page_content=results)]\n",
    "        summary = chain.run(docs)\n",
    "        step1Executives.append({\n",
    "            \"name\": name,\n",
    "            \"title\": title,\n",
    "            \"biography\": summary\n",
    "        })\n",
    "\n",
    "    sData = {\n",
    "            'id' : str(uuid.uuid4()),\n",
    "            'symbol': symbol,\n",
    "            'cik': cik,\n",
    "            'step': step,\n",
    "            'description': 'Biography of Key Executives',\n",
    "            'insertedDate': today.strftime(\"%Y-%m-%d\"),\n",
    "            'pibData' : str(step1Executives)\n",
    "    }\n",
    "    step1Biography.append(sData)\n",
    "    s1Data.append(sData)\n",
    "    mergeDocs(SearchService, SearchKey, pibIndexName, step1Biography)\n",
    "else:\n",
    "    for s in r:\n",
    "        s1Data.append(\n",
    "            {\n",
    "                'id' : s['id'],\n",
    "                'symbol': s['symbol'],\n",
    "                'cik': s['cik'],\n",
    "                'step': s['step'],\n",
    "                'description': s['description'],\n",
    "                'insertedDate': s['insertedDate'],\n",
    "                'pibData' : s['pibData']\n",
    "            })\n",
    "\n",
    "print(s1Data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Paid Data -  Get the Earnings Call Transcript for each quarter for last 3 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search index earningcalls already exists\n",
      "Found 1 records for WF for 1 2023\n"
     ]
    }
   ],
   "source": [
    "# Call the paid data (FMP) API\n",
    "# Get the earning call transcripts for the last 3 years and merge documents into the index.\n",
    "i = 0\n",
    "earningsData = []\n",
    "step = \"2\"\n",
    "earningIndexName = 'earningcalls'\n",
    "# Create the index if it does not exist\n",
    "createEarningCallIndex(SearchService, SearchKey, earningIndexName)\n",
    "# Get the list of all earning calls available\n",
    "earningCallDates = earningCallsAvailableDates(apikey=apikey, symbol=symbol)\n",
    "quarter = earningCallDates[0][0]\n",
    "year = earningCallDates[0][1]\n",
    "r = findEarningCalls(SearchService, SearchKey, earningIndexName, symbol, str(quarter), str(year), returnFields=['id', 'symbol', \n",
    "            'quarter', 'year', 'callDate', 'content'])\n",
    "if r.get_count() == 0:\n",
    "    insertEarningCall = []\n",
    "    earningTranscript = earningCallTranscript(apikey=apikey, symbol=symbol, year=str(year), quarter=quarter)\n",
    "    for transcript in earningTranscript:\n",
    "        symbol = transcript['symbol']\n",
    "        quarter = transcript['quarter']\n",
    "        year = transcript['year']\n",
    "        callDate = transcript['date']\n",
    "        content = transcript['content']\n",
    "        todayYmd = today.strftime(\"%Y-%m-%d\")\n",
    "        id = f\"{symbol}-{year}-{quarter}\"\n",
    "        earningRecord = {\n",
    "            \"id\": id,\n",
    "            \"symbol\": symbol,\n",
    "            \"quarter\": str(quarter),\n",
    "            \"year\": str(year),\n",
    "            \"callDate\": callDate,\n",
    "            \"content\": content,\n",
    "            #\"inserteddate\": datetime.now(central).strftime(\"%Y-%m-%d\"),\n",
    "        }\n",
    "        earningsData.append(earningRecord)\n",
    "        insertEarningCall.append(earningRecord)\n",
    "        mergeDocs(SearchService, SearchKey, earningIndexName, insertEarningCall)\n",
    "else:\n",
    "    print(f\"Found {r.get_count()} records for {symbol} for {quarter} {year}\")\n",
    "    for s in r:\n",
    "        earningsData.append(\n",
    "            {\n",
    "                'id' : s['id'],\n",
    "                'symbol': s['symbol'],\n",
    "                'quarter': s['quarter'],\n",
    "                'year': s['year'],\n",
    "                'callDate': s['callDate'],\n",
    "                'content': s['content']\n",
    "            })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the transcripts as per Split Method, Chunk Size and Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last earning call transcripts was on : 2023-04-24 10:46:09\n",
      "Number of documents chunks generated from Call transcript :  2\n"
     ]
    }
   ],
   "source": [
    "# Let's just use the latest earnings call transcript to create the documents that we want to use it for generative AI tasks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=8000, chunk_overlap=1000)\n",
    "\n",
    "print(\"Last earning call transcripts was on :\", earningsData[-1]['callDate'])\n",
    "rawDocs = splitter.create_documents([earningsData[-1]['content']])\n",
    "docs = splitter.split_documents(rawDocs)\n",
    "print(\"Number of documents chunks generated from Call transcript : \", len(docs))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the vector store embedding data for chunked sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search index latestearningcalls already exists\n",
      "Total docs: 2\n",
      "Found 2 sections for WF 2023 Q1\n",
      "Already indexed 2 sections for WF 2023 Q1\n"
     ]
    }
   ],
   "source": [
    "# Store the last index of the earning call transcript in vector Index\n",
    "earningVectorIndexName = 'latestearningcalls'\n",
    "createEarningCallVectorIndex(SearchService, SearchKey, earningVectorIndexName)\n",
    "\n",
    "indexEarningCallSections(OpenAiService, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey,\n",
    "                         embeddingModelType, OpenAiEmbedding, earningVectorIndexName, docs,\n",
    "                         earningsData[-1]['callDate'], earningsData[-1]['symbol'], earningsData[-1]['year'],\n",
    "                         earningsData[-1]['quarter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAnswer(chainType, topK, symbol, quarter, year, question, indexName, llm):\n",
    "    r = performEarningCallCogSearch(OpenAiService, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, \n",
    "        OpenAiEmbedding, symbol, str(quarter), str(year), question, indexName, topK, returnFields=['id', 'symbol', 'quarter', 'year', 'callDate', 'content'])\n",
    "    \n",
    "    if r == None:\n",
    "        docs = [Document(page_content=\"No results found\")]\n",
    "    else :\n",
    "        docs = [\n",
    "            Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": ''})\n",
    "            for doc in r\n",
    "            ]\n",
    "    \n",
    "    if chainType == \"map_reduce\":\n",
    "        # Prompt for MapReduce\n",
    "        qaTemplate = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question.\n",
    "                Return any relevant text.\n",
    "                {context}\n",
    "                Question: {question}\n",
    "                Relevant text, if any :\"\"\"\n",
    "\n",
    "        qaPrompt = PromptTemplate(\n",
    "            template=qaTemplate, input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "\n",
    "        combinePromptTemplate = \"\"\"Given the following extracted parts of a long document and a question, create a final answer.\n",
    "        If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "        If the answer is not contained within the text below, say \\\"I don't know\\\".\n",
    "\n",
    "        QUESTION: {question}\n",
    "        =========\n",
    "        {summaries}\n",
    "        =========\n",
    "        \"\"\"\n",
    "        combinePrompt = PromptTemplate(\n",
    "            template=combinePromptTemplate, input_variables=[\"summaries\", \"question\"]\n",
    "        )\n",
    "\n",
    "        qaChain = load_qa_with_sources_chain(llm, chain_type=chainType, question_prompt=qaPrompt, \n",
    "                                            combine_prompt=combinePrompt, \n",
    "                                            return_intermediate_steps=True)\n",
    "        answer = qaChain({\"input_documents\": docs, \"question\": question})\n",
    "        outputAnswer = answer['output_text']\n",
    "\n",
    "    elif chainType == \"stuff\":\n",
    "    # Prompt for ChainType = Stuff\n",
    "        template = \"\"\"\n",
    "                Given the following extracted parts of a long document and a question, create a final answer. \n",
    "                If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n",
    "                If the answer is not contained within the text below, say \\\"I don't know\\\".\n",
    "\n",
    "                QUESTION: {question}\n",
    "                =========\n",
    "                {summaries}\n",
    "                =========\n",
    "                \"\"\"\n",
    "        qaPrompt = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
    "        qaChain = load_qa_with_sources_chain(llm, chain_type=chainType, prompt=qaPrompt)\n",
    "        answer = qaChain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)\n",
    "        outputAnswer = answer['output_text']\n",
    "    elif chainType == \"default\":\n",
    "        # Default Prompt\n",
    "        qaChain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "        answer = qaChain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)\n",
    "        outputAnswer = answer['output_text']\n",
    "\n",
    "    return outputAnswer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top questions to ask during earning call - Let's see if we can find the answers to these questions in the transcripts\n",
    "- What are some of the current and looming threats to the business?\n",
    "- What is the debt level or debt ratio of the company right now?\n",
    "- How do you feel about the upcoming product launches or new products?\n",
    "- How are you managing or investing in your human capital?\n",
    "- How do you track the trends in your industry?\n",
    "- Are there major slowdowns in the production of goods?\n",
    "- How will you maintain or surpass this performance in the next few quarters?\n",
    "- What will your market look like in five years as a result of using your product or service?\n",
    "- How are you going to address the risks that will affect the long-term growth of the company?\n",
    "- How is the performance this quarter going to affect the long-term goals of the company?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another specific question to ask\n",
    "- Revenue: Provide key information about revenue for the quarter\n",
    "- Profitability: Provide key information about profits and losses (P&L) for the quarter\n",
    "- Industry Trends: Provide key information about industry trends for the quarter\n",
    "- Trend: Provide key information about business trends discussed on the call\n",
    "- Risk: Provide key information about risk discussed on the call\n",
    "- AI: Provide key information about AI discussed on the call\n",
    "- M&A: Provide any information about mergers and acquisitions (M&A) discussed on the call.\n",
    "- Guidance: Provide key information about guidance discussed on the call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we have the lastest transcripts in the document format, let's summarize the information with following specific summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing data\n",
      "[{'id': '7be9304b-dfac-441c-8cb1-c06f5de88b2d', 'symbol': 'WF', 'cik': '1264136', 'step': '2', 'description': 'Earning Call Q&A', 'insertedDate': '2023-07-19', 'pibData': '[{\\'question\\': \\'Provide key information about revenue for the quarter\\', \\'answer\\': \"The key information about revenue for the quarter is as follows: \\\\n- For the first quarter of 2023, Woori Financial Group\\'s net operating revenue was KRW 2,551 billion, representing a 7.6% increase year-on-year.\"}, {\\'question\\': \\'Provide key information about profits and losses (P&L) for the quarter\\', \\'answer\\': \"Based on the provided information, the key information about profits and losses (P&L) for the quarter is as follows:\\\\n\\\\n- Net income for the first quarter of 2023 was KRW 911 billion, representing an 8.6% increase year-over-year.\\\\n- Group net operating revenue increased by 7.6% year-on-year to KRW 2,551 billion.\\\\n- The group\\'s SG&A expense increased by 6.1% year-on-year to KRW 1,037 billion, but the cost-to-income ratio decreased by 0.8% to 40.4%.\\\\n- The group\\'s credit cost for the first quarter of 2023 was KRW 261 billion, with a credit cost ratio of 0.31%.\\\\n- The group\\'s CET1 ratio is expected to reach 12.1%, up 0.5% from the previous year-end.\\\\n\\\\nTherefore, the key information about profits and losses (P&L) for the quarter includes the net income, net operating revenue, SG&A expense, credit cost, and CET1 ratio.\"}, {\\'question\\': \\'Provide key information about business trends discussed on the call\\', \\'answer\\': \\'The key information about business trends discussed on the call includes:\\\\n\\\\n1. Reshaping corporate culture: Woori Financial Group plans to dramatically improve its group fundamentals by implementing bold innovation in areas such as internal control and corporate governance. The goal is to become the most trusted financial group in the market.\\\\n\\\\n2. Strengthening nonbank business: The group aims to expand its nonbank portfolio, including securities and insurance, to create a more balanced profit structure. They will identify opportunities amid the crisis and accelerate the competition of their nonbank portfolio.\\\\n\\\\n3. Enhancing risk management: Woori Financial Group will continue to enhance its risk management framework to identify a wide variety of risks in advance and address them appropriately. They will strengthen their capital base to prepare for a market downturn.\\\\n\\\\n4. Identifying new growth drivers in nonfinancial areas: The group will identify new growth drivers in nonfinancial areas to enhance future competitiveness. They will leverage the development of digital technology and the changing regulatory environment to explore new opportunities for their business.\\\\n\\\\n5. Supporting Korean people and businesses: Woori Financial Group aims to provide real assistance to companies in difficult situations and vulnerable populations. They will not only provide credit but also offer a wide range of other assistance, including business consulting.\\\\n\\\\nOverall, the discussed trends focus on improving corporate culture, expanding nonbank business, enhancing risk management, exploring new growth drivers, and providing support to the Korean people and businesses.\\'}, {\\'question\\': \\'Provide key information about risk discussed on the call\\', \\'answer\\': \\'The key information about risk discussed on the call includes:\\\\n\\\\n1. Woori Financial Group plans to enhance its risk management framework to identify and address a wide variety of risks in advance.\\\\n2. There has been concern about asset quality in the non-bank financial sector due to recent rate hikes.\\\\n3. Woori Financial Group will exert all efforts in risk management and strengthen its capital base to prepare for a market downturn.\\\\n4. The group is focused on improving its profit structure and increasing its loss absorption capabilities to prepare for possible market downturns.\\\\n5. Woori Financial Group is actively monitoring and responding to increased delinquency rates, particularly in the non-banking sector.\\\\n6. The group has improved its capital adequacy indicators and plans to further improve capital adequacy through solid financial performance.\\\\n\\\\nOverall, risk management and addressing potential risks in the financial sector were key topics discussed on the call.\\'}, {\\'question\\': \\'1. Financial Results\\', \\'answer\\': \" Woori Financial Group reported a net income of KRW 911 billion for the first quarter of 2023, showing an 8.6% increase compared to the previous year. The group\\'s net operating revenue also increased by 7.6% to KRW 2,551 billion. Despite the rise in market rates, the group maintained stable profitability and improved cost efficiency. Expenses, including SG&A, increased by 6.1%, but the cost-to-income ratio decreased. The group\\'s credit cost was KRW 261 billion, with a credit cost ratio of 0.31%. The group plans to enhance shareholder value through a share buyback and cancellation plan worth KRW 100 billion. The group\\'s CET1 ratio is expected to exceed 12% for the first time, reaching 12.1%.\"}, {\\'question\\': \\'2. Business Highlights\\', \\'answer\\': \" Woori Financial Group highlighted several focus areas, including reshaping corporate culture, strengthening nonbank business, enhancing risk management, identifying new growth drivers, and supporting Korean people and businesses. The group\\'s performance remained solid, with corporate loans continuing to grow while retail loans decreased. Noninterest income increased, driven by fees and commissions. The group also improved its cost-to-income ratio despite the increase in SG&A expenses. Credit costs were effectively controlled, contributing to the group\\'s stable financial performance. The group aims to build a foundation for growth and innovation.\"}, {\\'question\\': \\'3. Future Outlook\\', \\'answer\\': \\' Woori Financial Group plans to focus on risk management and asset quality management in the future. They aim to improve capital adequacy and pursue shareholder return policies. The group intends to enhance its loss absorption capabilities and further improve its CET1 ratio. They also aim to identify new growth drivers and strengthen their nonbank business. The group is committed to supporting Korean people and businesses and plans to continue providing financial services that meet their needs. Overall, the group is optimistic about its future prospects and aims to build a strong foundation for sustainable growth.\\'}, {\\'question\\': \\'4. Business Risks\\', \\'answer\\': \" While Woori Financial Group has shown solid performance, there are certain risks that the group acknowledges. The financial markets present ongoing challenges that could impact the group\\'s operations and profitability. Market rates have been rising, which may affect the group\\'s net interest margin. Additionally, there is a risk associated with asset quality and credit costs. The group recognizes the importance of effective risk management to mitigate these risks and maintain stable financial performance.\"}, {\\'question\\': \\'5. Management Positive Sentiment\\', \\'answer\\': \" Woori Financial Group\\'s management is confident about several aspects of the business. They are pleased with the group\\'s solid financial performance, including the increase in net income and net operating revenue. The group\\'s focus on reshaping corporate culture, strengthening nonbank business, and enhancing risk management is expected to contribute to long-term success. The improvement in the cost-to-income ratio and effective control of credit costs are positive indicators. The group\\'s plans to enhance shareholder value through a share buyback and cancellation plan and the expected improvement in the CET1 ratio also instill confidence in management.\"}, {\\'question\\': \\'6. Management Negative Sentiment\\', \\'answer\\': \" While Woori Financial Group is optimistic about its future, there are areas of concern for management. The decrease in the group\\'s net interest margin due to lower loan yields and increased funding costs is a challenge that needs to be addressed. The decline in retail loans is also a point of concern. However, management is focused on identifying new growth drivers and strengthening the nonbank business to mitigate these concerns. They are committed to improving capital adequacy and risk management to ensure the group\\'s long-term success.\"}]'}]\n"
     ]
    }
   ],
   "source": [
    "r = findPibData(SearchService, SearchKey, pibIndexName, cik, step, returnFields=['id', 'symbol', 'cik', 'step', 'description', 'insertedDate',\n",
    "                                                                   'pibData'])\n",
    "\n",
    "earningCallQa = []\n",
    "s2Data = []\n",
    "if r.get_count() == 0:\n",
    "    commonQuestions = [\n",
    "        \"What are some of the current and looming threats to the business?\",\n",
    "        \"What is the debt level or debt ratio of the company right now?\",\n",
    "        \"How do you feel about the upcoming product launches or new products?\",\n",
    "        \"How are you managing or investing in your human capital?\",\n",
    "        \"How do you track the trends in your industry?\",\n",
    "        \"Are there major slowdowns in the production of goods?\",\n",
    "        \"How will you maintain or surpass this performance in the next few quarters?\",\n",
    "        \"What will your market look like in five years as a result of using your product or service?\",\n",
    "        \"How are you going to address the risks that will affect the long-term growth of the company?\",\n",
    "        \"How is the performance this quarter going to affect the long-term goals of the company?\"\n",
    "    ]\n",
    "\n",
    "    for question in commonQuestions:\n",
    "        answer = findAnswer('stuff', 3, symbol, quarter, year, question, earningVectorIndexName, llmChat)\n",
    "        if \"I don't know\" not in answer:\n",
    "            earningCallQa.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "    commonQuestions = [\n",
    "            \"Provide key information about revenue for the quarter\",\n",
    "            \"Provide key information about profits and losses (P&L) for the quarter\",\n",
    "            \"Provide key information about industry trends for the quarter\",\n",
    "            \"Provide key information about business trends discussed on the call\",\n",
    "            \"Provide key information about risk discussed on the call\",\n",
    "            \"Provide key information about AI discussed on the call\",\n",
    "            \"Provide any information about mergers and acquisitions (M&A) discussed on the call.\",\n",
    "            \"Provide key information about guidance discussed on the call\"\n",
    "        ]\n",
    "\n",
    "    for question in commonQuestions:\n",
    "        answer = findAnswer('stuff', 3, symbol, quarter, year, question, earningVectorIndexName, llmChat)\n",
    "        if \"I don't know\" not in answer:\n",
    "            earningCallQa.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "    promptTemplate = \"\"\"You are an AI assistant tasked with summarizing financial information from earning call transcript. \n",
    "            Your summary should accurately capture the key information in the document while avoiding the omission of any domain-specific words. \n",
    "            Please generate a concise and comprehensive summary between 5-7 paragraphs on each of the following numbered topics.  Your response should include the topic as part of the summary.\n",
    "            1. Financial Results: Please provide a summary of the financial results.\n",
    "            2. Business Highlights: Please provide a summary of the business highlights.\n",
    "            3. Future Outlook: Please provide a summary of the future outlook.\n",
    "            4. Business Risks: Please provide a summary of the business risks.\n",
    "            5. Management Positive Sentiment: Please provide a summary of the what management is confident about.\n",
    "            6. Management Negative Sentiment: Please provide a summary of the what management is concerned about.\n",
    "            Please remember to use clear language and maintain the integrity of the original information without missing any important details:\n",
    "            {text}\n",
    "            \"\"\"\n",
    "    customPrompt = PromptTemplate(template=promptTemplate, input_variables=[\"text\"])\n",
    "    chainType = \"map_reduce\"\n",
    "    summaryChain = load_summarize_chain(llmChat, chain_type=chainType, return_intermediate_steps=False, \n",
    "                                combine_prompt=customPrompt)\n",
    "    summaryOutput = summaryChain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "\n",
    "    output = summaryOutput['output_text']\n",
    "    formattedOutput = output.splitlines()\n",
    "    while(\"\" in formattedOutput):\n",
    "        formattedOutput.remove(\"\")\n",
    "    for summary in formattedOutput:\n",
    "        splitSummary = summary.split(\":\")\n",
    "        try:\n",
    "            question = splitSummary[0]\n",
    "            answer = splitSummary[1]\n",
    "            earningCallQa.append({\"question\": question, \"answer\": answer})\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    s2Data.append({\n",
    "                'id' : str(uuid.uuid4()),\n",
    "                'symbol': symbol,\n",
    "                'cik': cik,\n",
    "                'step': step,\n",
    "                'description': 'Earning Call Q&A',\n",
    "                'insertedDate': today.strftime(\"%Y-%m-%d\"),\n",
    "                'pibData' : str(earningCallQa)\n",
    "        })\n",
    "    mergeDocs(SearchService, SearchKey, pibIndexName, s2Data)\n",
    "else:\n",
    "    print('Found existing data')\n",
    "    for s in r:\n",
    "        s2Data.append(\n",
    "            {\n",
    "                'id' : s['id'],\n",
    "                'symbol': s['symbol'],\n",
    "                'cik': s['cik'],\n",
    "                'step': s['step'],\n",
    "                'description': s['description'],\n",
    "                'insertedDate': s['insertedDate'],\n",
    "                'pibData' : s['pibData']\n",
    "            })\n",
    "        \n",
    "print(s2Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = findPibData(SearchService, SearchKey, pibIndexName, cik, step, returnFields=['id', 'symbol', 'cik', 'step', 'description', 'insertedDate',\n",
    "#                                                                    'pibData'])\n",
    "# s2Data = []\n",
    "# if r.get_count() == 0:\n",
    "\n",
    "#     earningCallQa = []\n",
    "#     commonQuestions = [\n",
    "#         \"What are some of the current and looming threats to the business?\",\n",
    "#         \"What is the debt level or debt ratio of the company right now?\",\n",
    "#         \"How do you feel about the upcoming product launches or new products?\",\n",
    "#         \"How are you managing or investing in your human capital?\",\n",
    "#         \"How do you track the trends in your industry?\",\n",
    "#         \"Are there major slowdowns in the production of goods?\",\n",
    "#         \"How will you maintain or surpass this performance in the next few quarters?\",\n",
    "#         \"What will your market look like in five years as a result of using your product or service?\",\n",
    "#         \"How are you going to address the risks that will affect the long-term growth of the company?\",\n",
    "#         \"How is the performance this quarter going to affect the long-term goals of the company?\"\n",
    "#     ]\n",
    "\n",
    "#     for question in commonQuestions:\n",
    "#         answer = findAnswer('map_reduce', 3, question, earningVectorIndexName)\n",
    "#         if \"I don't know\" not in answer:\n",
    "#             earningCallQa.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "#     commonQuestions = [\n",
    "#         \"Provide key information about revenue for the quarter\",\n",
    "#         \"Provide key information about profits and losses (P&L) for the quarter\",\n",
    "#         \"Provide key information about industry trends for the quarter\",\n",
    "#         \"Provide key information about business trends discussed on the call\",\n",
    "#         \"Provide key information about risk discussed on the call\",\n",
    "#         \"Provide key information about AI discussed on the call\",\n",
    "#         \"Provide any information about mergers and acquisitions (M&A) discussed on the call.\",\n",
    "#         \"Provide key information about guidance discussed on the call\"\n",
    "#     ]\n",
    "\n",
    "#     for question in commonQuestions:\n",
    "#         answer = findAnswer('map_reduce', 3, question, earningVectorIndexName)\n",
    "#         if \"I don't know\" not in answer:\n",
    "#             earningCallQa.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "#     # With the data indexed, let's summarize the information\n",
    "#     # While we are using the standard prompt by langchain, you can modify the prompt to suit your needs\n",
    "#     # 1. Financial Results Summary: Please provide a summary of the financial results.\n",
    "#     # 2. Business Highlights: Please provide a summary of the business highlights.\n",
    "#     # 3. Future Outlook: Please provide a summary of the future outlook.\n",
    "#     # 4. Business Risks: Please provide a summary of the business risks.\n",
    "#     # 5. Management Positive Sentiment: Please provide a summary of the what management is confident about.\n",
    "#     # 6. Management Negative Sentiment: Please provide a summary of the what management is concerned about.\n",
    "#     # 7. Future Growth Strategies : Please generate a concise and comprehensive strategies summary that includes the information in  bulleted format.\n",
    "#     # 8. Risk Increase: Please provide a summary of the risks that have increased.\n",
    "#     # 9. Risk Decrease: Please provide a summary of the risks that have decreased.\n",
    "#     # 10. Opportunity Increase: Please provide a summary of the opportunities that have increased.\n",
    "#     # 11. Opportunity Decrease: Please provide a summary of the opportunities that have decreased.\n",
    "#     commonSummary = [\n",
    "#         \"Financial Results\",\n",
    "#         \"Business Highlights\",\n",
    "#         \"Future Outlook\",\n",
    "#         \"Business Risks\",\n",
    "#         \"Management Positive Sentiment\",\n",
    "#         \"Management Negative Sentiment\",\n",
    "#         \"Future Growth Strategies\"\n",
    "#     ]\n",
    "\n",
    "#     promptTemplate = \"\"\"You are an AI assistant tasked with summarizing financial information from earning call transcript. \n",
    "#             Your summary should accurately capture the key information in the document while avoiding the omission of any domain-specific words. \n",
    "#             Please generate a concise and comprehensive summary on the following topics. \n",
    "#             {summarize}\n",
    "#             Please remember to use clear language and maintain the integrity of the original information without missing any important details:\n",
    "#             {text}\n",
    "#             \"\"\"\n",
    "#     for summary in commonSummary:\n",
    "#         customPrompt = PromptTemplate(template=promptTemplate.replace('{summarize}', summary), input_variables=[\"text\"])\n",
    "#         chainType = \"map_reduce\"\n",
    "#         summaryChain = load_summarize_chain(llmChat, chain_type=chainType, return_intermediate_steps=False, \n",
    "#                                     map_prompt=customPrompt, combine_prompt=customPrompt)\n",
    "#         summaryOutput = summaryChain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "#         outputAnswer = summaryOutput['output_text'].replace('Summary:', '')\n",
    "#         if \"I don't know\" not in answer and len(outputAnswer) > 0:\n",
    "#             earningCallQa.append({\"question\": summary, \"answer\": outputAnswer})\n",
    "\n",
    "#     s2Data.append({\n",
    "#                 'id' : str(uuid.uuid4()),\n",
    "#                 'symbol': symbol,\n",
    "#                 'cik': cik,\n",
    "#                 'step': step,\n",
    "#                 'description': 'Earning Call Q&A',\n",
    "#                 'insertedDate': today.strftime(\"%Y-%m-%d\"),\n",
    "#                 'pibData' : str(earningCallQa)\n",
    "#         })\n",
    "#     mergeDocs(SearchService, SearchKey, pibIndexName, s2Data)\n",
    "# else:\n",
    "#     print('Found existing data')\n",
    "#     for s in r:\n",
    "#         s2Data.append(\n",
    "#             {\n",
    "#                 'id' : s['id'],\n",
    "#                 'symbol': s['symbol'],\n",
    "#                 'cik': s['cik'],\n",
    "#                 'step': s['step'],\n",
    "#                 'description': s['description'],\n",
    "#                 'insertedDate': s['insertedDate'],\n",
    "#                 'pibData' : s['pibData']\n",
    "#             })\n",
    "        \n",
    "# print(s2Data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case if we wanted to see summary of summary, run code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For the chaintype of MapReduce and Refine, we can also get insight into intermediate steps of the pipeline.\n",
    "# # This way you can inspect the results from map_reduce chain type, each top similar chunk summary\n",
    "# intermediateSteps = summary['intermediate_steps']\n",
    "# for step in intermediateSteps:\n",
    "#         display(HTML(\"<b>Chunk Summary:</b> \" + step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Paid Data - Press Releases - Get the Press Releases for last year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizePressReleases(llm, docs):\n",
    "    promptTemplate = \"\"\"You are an AI assistant tasked with summarizing company's press releases and performing sentiments on those. \n",
    "                Your summary should accurately capture the key information in the press-releases while avoiding the omission of any domain-specific words. \n",
    "                Please generate a concise and comprehensive summary and sentiment with score with range of 0 to 10. \n",
    "                Your response should be in JSON object with following keys.  All JSON properties are required.\n",
    "                summary: \n",
    "                sentiment:\n",
    "                sentiment score: \n",
    "                {text}\n",
    "                \"\"\"\n",
    "    customPrompt = PromptTemplate(template=promptTemplate, input_variables=[\"text\"])\n",
    "    chainType = \"stuff\"\n",
    "    summaryChain = load_summarize_chain(llm, chain_type=chainType, prompt=customPrompt)\n",
    "    summary = summaryChain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "    outputAnswer = summary['output_text']\n",
    "    return outputAnswer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing data\n",
      "[{'id': 'd23575de-7792-4066-8de3-31c26175efe4', 'symbol': 'WF', 'cik': '1264136', 'step': '3', 'description': 'Press Releases', 'insertedDate': '2023-07-19', 'pibData': '[{\\'releaseDate\\': \\'2023-04-21 11:00:00\\', \\'title\\': \\'WOORI FINANCIAL GROUP INC. FILES ITS ANNUAL REPORT ON FORM 20-F\\', \\'summary\\': \"Woori Financial Group Inc. has filed its annual report on Form 20-F for the year ended December 31, 2022 with the U.S. Securities and Exchange Commission. The report can be downloaded from the company\\'s website and the SEC\\'s website.\", \\'sentiment\\': \\'Neutral\\', \\'sentimentScore\\': 5.0}, {\\'releaseDate\\': \\'2022-05-16 10:00:00\\', \\'title\\': \\'WOORI FINANCIAL GROUP INC. FILES ITS ANNUAL REPORT ON FORM 20-F\\', \\'summary\\': \"Woori Financial Group Inc. has filed its annual report on Form 20-F for the year ended December 31, 2021 with the U.S. Securities and Exchange Commission. The report can be downloaded from Woori Financial Group\\'s website and the SEC\\'s website. Investors can also request a hard copy of the report free of charge.\", \\'sentiment\\': \\'Neutral\\', \\'sentimentScore\\': 5.0}]'}]\n"
     ]
    }
   ],
   "source": [
    "# For now we are calling API to get data, but otherwise we need to ensure the data is not persisted in our \n",
    "# index repository before calling again, if it is persisted then we need to delete it first\n",
    "step = \"3\"\n",
    "s3Data = []\n",
    "r = findPibData(SearchService, SearchKey, pibIndexName, cik, step, returnFields=['id', 'symbol', 'cik', 'step', 'description', 'insertedDate',\n",
    "                                                                   'pibData'])\n",
    "if r.get_count() == 0:\n",
    "    counter = 0\n",
    "    pressReleasesList = []\n",
    "    pressReleaseIndexName = 'pressreleases'\n",
    "    # Create the index if it does not exist\n",
    "    createPressReleaseIndex(SearchService, SearchKey, pressReleaseIndexName)\n",
    "    print(f\"Processing ticker : {symbol}\")\n",
    "    pr = pressReleases(apikey=apikey, symbol=symbol, limit=25)\n",
    "    for pressRelease in pr:\n",
    "        symbol = pressRelease['symbol']\n",
    "        releaseDate = pressRelease['date']\n",
    "        title = pressRelease['title']\n",
    "        content = pressRelease['text']\n",
    "        todayYmd = today.strftime(\"%Y-%m-%d\")\n",
    "        id = f\"{symbol}-{counter}\"\n",
    "        pressReleasesList.append({\n",
    "            \"id\": id,\n",
    "            \"symbol\": symbol,\n",
    "            \"releaseDate\": releaseDate,\n",
    "            \"title\": title,\n",
    "            \"content\": content,\n",
    "        })\n",
    "        counter = counter + 1\n",
    "\n",
    "    mergeDocs(SearchService, SearchKey, pressReleaseIndexName, pressReleasesList)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "    rawPressReleasesDoc = [Document(page_content=t['content']) for t in pressReleasesList[:25]]\n",
    "    pressReleasesDocs = splitter.split_documents(rawPressReleasesDoc)\n",
    "    print(\"Number of documents chunks generated from Press releases : \", len(pressReleasesDocs))\n",
    "\n",
    "    pressReleasesPib = []\n",
    "    last25PressReleases = pressReleasesList[:25]\n",
    "    i = 0\n",
    "    for pDocs in pressReleasesDocs:\n",
    "        try:\n",
    "            outputAnswer = summarizePressReleases(llmChat, [pDocs])\n",
    "            jsonStep = json.loads(outputAnswer)\n",
    "            pressReleasesPib.append({\n",
    "                    \"releaseDate\": last25PressReleases[i]['releaseDate'],\n",
    "                    \"title\": last25PressReleases[i]['title'],\n",
    "                    \"summary\": jsonStep['summary'],\n",
    "                    \"sentiment\": jsonStep['sentiment'],\n",
    "                    \"sentimentScore\": jsonStep['sentiment score']\n",
    "            })\n",
    "            i = i + 1\n",
    "        except Exception as e:\n",
    "            i = i + 1\n",
    "            continue\n",
    "    \n",
    "    s3Data.append({\n",
    "                'id' : str(uuid.uuid4()),\n",
    "                'symbol': symbol,\n",
    "                'cik': cik,\n",
    "                'step': step,\n",
    "                'description': 'Press Releases',\n",
    "                'insertedDate': today.strftime(\"%Y-%m-%d\"),\n",
    "                'pibData' : str(pressReleasesPib)\n",
    "        })\n",
    "    mergeDocs(SearchService, SearchKey, pibIndexName, s3Data)\n",
    "else:\n",
    "    print('Found existing data')\n",
    "    for s in r:\n",
    "        s3Data.append(\n",
    "            {\n",
    "                'id' : s['id'],\n",
    "                'symbol': s['symbol'],\n",
    "                'cik': s['cik'],\n",
    "                'step': s['step'],\n",
    "                'description': s['description'],\n",
    "                'insertedDate': s['insertedDate'],\n",
    "                'pibData' : s['pibData']\n",
    "            })\n",
    "\n",
    "print(s3Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Paid Data - Get Stock News - Limit it to cover for current year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For now we are calling API to get data, but otherwise we need to ensure the data is not persisted in our \n",
    "# # index repository before calling again, if it is persisted then we need to delete it first\n",
    "# counter = 0\n",
    "# stockNewsList = []\n",
    "# stockNewsIndexName = 'stocknews'\n",
    "# # Create the index if it does not exist\n",
    "# createStockNewsIndex(SearchService, SearchKey, stockNewsIndexName)\n",
    "# print(f\"Processing ticker : {symbol}\")\n",
    "# sn = stockNews(apikey=apikey, tickers=symbol, limit=5000)\n",
    "# for news in sn:\n",
    "#     symbol = news['symbol']\n",
    "#     publishedDate = news['publishedDate']\n",
    "#     title = news['title']\n",
    "#     image = news['image']\n",
    "#     site = news['site']\n",
    "#     content = news['text']\n",
    "#     url = news['url']\n",
    "#     todayYmd = today.strftime(\"%Y-%m-%d\")\n",
    "#     id = f\"{symbol}-{todayYmd}-{counter}\"\n",
    "#     stockNewsList.append({\n",
    "#         \"id\": id,\n",
    "#         \"symbol\": symbol,\n",
    "#         \"publishedDate\": publishedDate,\n",
    "#         \"title\": title,\n",
    "#         \"image\": image,\n",
    "#         \"site\": site,\n",
    "#         \"content\": content,\n",
    "#         \"url\": url,\n",
    "#     })\n",
    "#     counter = counter + 1\n",
    "# mergeDocs(SearchService, SearchKey, stockNewsIndexName, stockNewsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group our news by Date and summarize the content and sentimet per day\n",
    "# stocksDf = pd.DataFrame.from_dict(pd.json_normalize(stockNewsList))\n",
    "# stocksDf['publishedDate'] = pd.to_datetime(stocksDf['publishedDate']).dt.date\n",
    "# stocksNewsDailyDf = stocksDf.sort_values('publishedDate').groupby('publishedDate')['content'].apply('\\n'.join).reset_index()\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "# rawNewsDocs = [Document(page_content=row['content']) for index, row in stocksNewsDailyDf.tail(10).iterrows()]\n",
    "# newsDocs = splitter.split_documents(rawNewsDocs)\n",
    "# print(\"Number of documents chunks generated from Press releases : \", len(newsDocs))\n",
    "\n",
    "# # With the data indexed, let's summarize the information\n",
    "# promptTemplate = \"\"\"You are an AI assistant tasked with summarizing news related to company and performing sentiments on those. \n",
    "#         Your summary should accurately capture the key information in the document while avoiding the omission of any domain-specific words. \n",
    "#         Please generate a concise and comprehensive summary and sentiment with score with range of 0 to 10. Your response should be in JSON format with following keys.\n",
    "#         summary: \n",
    "#         sentiment:\n",
    "#         sentiment score:\n",
    "#         Please remember to use clear language and maintain the integrity of the original information without missing any important details.\n",
    "#         {text}\n",
    "#         \"\"\"\n",
    "# customPrompt = PromptTemplate(template=promptTemplate, input_variables=[\"text\"])\n",
    "# chainType = \"map_reduce\"\n",
    "# summaryChain = load_summarize_chain(llm, chain_type=chainType, return_intermediate_steps=True, \n",
    "#                                     map_prompt=customPrompt, combine_prompt=customPrompt)\n",
    "# summary = summaryChain({\"input_documents\": newsDocs}, return_only_outputs=True)\n",
    "# outputAnswer = summary['output_text']\n",
    "# print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For the chaintype of MapReduce and Refine, we can also get insight into intermediate steps of the pipeline.\n",
    "# # This way you can inspect the results from map_reduce chain type, each top similar chunk summary\n",
    "# intermediateSteps = summary['intermediate_steps']\n",
    "# for step in intermediateSteps:\n",
    "#         display(HTML(\"<b>Chunk Summary:</b> \" + step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Public Data - Get the SEC Filings - Limit it to cover for last 3 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "filingType = \"10-K\"\n",
    "secFilingsList = secFilings(apikey=apikey, symbol=symbol, filing_type=filingType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[312], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m latestFilingDateTime \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mstrptime(secFilingsList[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mfillingDate\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m latestFilingDate \u001b[39m=\u001b[39m latestFilingDateTime\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m filingYear \u001b[39m=\u001b[39m latestFilingDateTime\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "latestFilingDateTime = datetime.strptime(secFilingsList[0]['fillingDate'], '%Y-%m-%d %H:%M:%S')\n",
    "latestFilingDate = latestFilingDateTime.strftime(\"%Y-%m-%d\")\n",
    "filingYear = latestFilingDateTime.strftime(\"%Y\")\n",
    "filingMonth = int(latestFilingDateTime.strftime(\"%m\"))\n",
    "\n",
    "if filingMonth > 0 & filingMonth <= 3:\n",
    "    filingQuarter = 1\n",
    "elif filingMonth > 3 & filingMonth <= 6:\n",
    "    filingQuarter = 2\n",
    "elif filingMonth > 6 & filingMonth <= 9:\n",
    "    filingQuarter = 3\n",
    "else:\n",
    "    filingQuarter = 4\n",
    "\n",
    "\n",
    "secFilingIndexName = 'secdata'\n",
    "secFilingList = []\n",
    "dt = pd.to_datetime(datetime.now(), format='%Y/%m/%d')\n",
    "dt1 = pd.to_datetime(latestFilingDate, format='%Y/%m/%d')\n",
    "totalDays = (dt-dt1).days\n",
    "if totalDays < 31:\n",
    "    skipIndicies = False\n",
    "else:\n",
    "    skipIndicies = True\n",
    "emptyBody = {\n",
    "        \"values\": [\n",
    "            {\n",
    "                \"recordId\": 0,\n",
    "                \"data\": {\n",
    "                    \"text\": \"\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "}\n",
    "\n",
    "secExtractBody = {\n",
    "    \"values\": [\n",
    "        {\n",
    "            \"recordId\": 0,\n",
    "            \"data\": {\n",
    "                \"text\": {\n",
    "                    \"edgar_crawler\": {\n",
    "                        \"start_year\": int(filingYear),\n",
    "                        \"end_year\": int(filingYear),\n",
    "                        \"quarters\": [filingQuarter],\n",
    "                        \"filing_types\": [\n",
    "                            \"10-K\"\n",
    "                        ],\n",
    "                        \"cik_tickers\": [cik],\n",
    "                        \"user_agent\": \"Your name (your email)\",\n",
    "                        \"raw_filings_folder\": \"RAW_FILINGS\",\n",
    "                        \"indices_folder\": \"INDICES\",\n",
    "                        \"filings_metadata_file\": \"FILINGS_METADATA.csv\",\n",
    "                        \"skip_present_indices\": skipIndicies,\n",
    "                    },\n",
    "                    \"extract_items\": {\n",
    "                        \"raw_filings_folder\": \"RAW_FILINGS\",\n",
    "                        \"extracted_filings_folder\": \"EXTRACTED_FILINGS\",\n",
    "                        \"filings_metadata_file\": \"FILINGS_METADATA.csv\",\n",
    "                        \"items_to_extract\": [\"1\",\"1A\",\"1B\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"7A\",\"8\",\"9\",\"9A\",\"9B\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\"],\n",
    "                        \"remove_tables\": True,\n",
    "                        \"skip_extracted_filings\": True\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Check if we have already processed the latest filing, if yes then skip\n",
    "createSecFilingIndex(SearchService, SearchKey, secFilingIndexName)\n",
    "r = findSecFiling(SearchService, SearchKey, secFilingIndexName, cik, filingType, latestFilingDate, returnFields=['id', 'cik', 'company', 'filingType', 'filingDate',\n",
    "                                                                                                                 'periodOfReport', 'sic', 'stateOfInc', 'fiscalYearEnd',\n",
    "                                                                                                                 'filingHtmlIndex', 'htmFilingLink', 'completeTextFilingLink',\n",
    "                                                                                                                 'item1', 'item1A', 'item1B', 'item2', 'item3', 'item4', 'item5',\n",
    "                                                                                                                 'item6', 'item7', 'item7A', 'item8', 'item9', 'item9A', 'item9B',\n",
    "                                                                                                                 'item10', 'item11', 'item12', 'item13', 'item14', 'item15',\n",
    "                                                                                                                 'sourcefile'])\n",
    "if r.get_count() == 0:\n",
    "    # Call Azure Function to perform Web-scraping and store the JSON in our blob\n",
    "    secExtract = requests.post(SecExtractionUrl, json = secExtractBody)\n",
    "    # Once the JSON is created, call the function to process the JSON and store the data in our index\n",
    "    docPersistUrl = SecDocPersistUrl + \"&indexType=cogsearchvs&indexName=\" + secFilingIndexName + \"&embeddingModelType=\" + embeddingModelType\n",
    "    secPersist = requests.post(docPersistUrl, json = emptyBody)\n",
    "    r = findSecFiling(SearchService, SearchKey, secFilingIndexName, cik, filingType, latestFilingDate, returnFields=['id', 'cik', 'company', 'filingType', 'filingDate',\n",
    "                                                                                                                 'periodOfReport', 'sic', 'stateOfInc', 'fiscalYearEnd',\n",
    "                                                                                                                 'filingHtmlIndex', 'htmFilingLink', 'completeTextFilingLink',\n",
    "                                                                                                                 'item1', 'item1A', 'item1B', 'item2', 'item3', 'item4', 'item5',\n",
    "                                                                                                                 'item6', 'item7', 'item7A', 'item8', 'item9', 'item9A', 'item9B',\n",
    "                                                                                                                 'item10', 'item11', 'item12', 'item13', 'item14', 'item15',\n",
    "                                                                                                                 'sourcefile'])\n",
    "\n",
    "lastSecData = ''\n",
    "# Retrieve the latest filing from our index\n",
    "for filing in r:\n",
    "    lastSecData = filing['item1'] + '\\n' + filing['item1A'] + '\\n' + filing['item1B'] + '\\n' + filing['item2'] + '\\n' + filing['item3'] + '\\n' + filing['item4'] + '\\n' + \\\n",
    "                filing['item5'] + '\\n' + filing['item6'] + '\\n' + filing['item7'] + '\\n' + filing['item7A'] + '\\n' + filing['item8'] + '\\n' + \\\n",
    "                filing['item9'] + '\\n' + filing['item9A'] + '\\n' + filing['item9B'] + '\\n' + filing['item10'] + '\\n' + filing['item11'] + '\\n' + filing['item12'] + '\\n' + \\\n",
    "                filing['item13'] + '\\n' + filing['item14'] + '\\n' + filing['item15']\n",
    "\n",
    "    secFilingList.append({\n",
    "        \"id\": filing['id'],\n",
    "        \"cik\": filing['cik'],\n",
    "        \"company\": filing['company'],\n",
    "        \"filingType\": filing['filingType'],\n",
    "        \"filingDate\": filing['filingDate'],\n",
    "        \"periodOfReport\": filing['periodOfReport'],\n",
    "        \"sic\": filing['sic'],\n",
    "        \"stateOfInc\": filing['stateOfInc'],\n",
    "        \"fiscalYearEnd\": filing['fiscalYearEnd'],\n",
    "        \"filingHtmlIndex\": filing['filingHtmlIndex'],\n",
    "        \"completeTextFilingLink\": filing['completeTextFilingLink'],\n",
    "        \"item1\": filing['item1'],\n",
    "        \"item1A\": filing['item1A'],\n",
    "        \"item1B\": filing['item1B'],\n",
    "        \"item2\": filing['item2'],\n",
    "        \"item3\": filing['item3'],\n",
    "        \"item4\": filing['item4'],\n",
    "        \"item5\": filing['item5'],\n",
    "        \"item6\": filing['item6'],\n",
    "        \"item7\": filing['item7'],\n",
    "        \"item7A\": filing['item7A'],\n",
    "        \"item8\": filing['item8'],\n",
    "        \"item9\": filing['item9'],\n",
    "        \"item9A\": filing['item9A'],\n",
    "        \"item9B\": filing['item9B'],\n",
    "        \"item10\": filing['item10'],\n",
    "        \"item11\": filing['item11'],\n",
    "        \"item12\": filing['item12'],\n",
    "        \"item13\": filing['item13'],\n",
    "        \"item14\": filing['item14'],\n",
    "        \"item15\": filing['item15'],\n",
    "        \"sourcefile\": filing['sourcefile']\n",
    "    })\n",
    "\n",
    "# Check if we have already processed the latest filing, if yes then skip\n",
    "secFilingsVectorIndexName = 'latestsecfilings'\n",
    "createSecFilingsVectorIndex(SearchService, SearchKey, secFilingsVectorIndexName)\n",
    "r = findLatestSecFilings(SearchService, SearchKey, secFilingsVectorIndexName, cik, symbol, latestFilingDate, filingType, returnFields=['id', 'cik', 'symbol', 'latestFilingDate', 'filingType',\n",
    "                                                                                                                 'content'])\n",
    "if r.get_count() == 0:\n",
    "    print(\"Processing latest SEC Filings for CIK : \", cik, \" and Symbol : \", symbol)\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=8000, chunk_overlap=1000)\n",
    "    rawDocs = splitter.create_documents([lastSecData])\n",
    "    docs = splitter.split_documents(rawDocs)\n",
    "    print(\"Number of documents chunks generated from Last SEC Filings : \", len(docs))\n",
    "\n",
    "    # Store the last index of the earning call transcript in vector Index\n",
    "    indexSecFilingsSections(OpenAiService, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey,\n",
    "                         embeddingModelType, OpenAiEmbedding, secFilingsVectorIndexName, docs, cik,\n",
    "                         symbol, latestFilingDate, filingType)\n",
    "else:\n",
    "    print(\"Latest SEC Filings for CIK : \", cik, \" and Symbol : \", symbol, \" already processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSummaries(docs):\n",
    "    chainType = \"map_reduce\"\n",
    "    summaryChain = load_summarize_chain(llm, chain_type=chainType)\n",
    "    summary = summaryChain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[315], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m splitter \u001b[39m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[39m=\u001b[39m\u001b[39m8000\u001b[39m, chunk_overlap\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[39m# Item 1 - Describes the business of the company\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m rawItemDocs1 \u001b[39m=\u001b[39m [Document(page_content\u001b[39m=\u001b[39msecFilingList[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mitem1\u001b[39m\u001b[39m'\u001b[39m])]\n\u001b[0;32m     15\u001b[0m itemDocs1 \u001b[39m=\u001b[39m splitter\u001b[39m.\u001b[39msplit_documents(rawItemDocs1)\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNumber of documents chunks generated from Item1 : \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(itemDocs1))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "step = \"4\"\n",
    "s4Data = []\n",
    "\n",
    "r = findPibData(SearchService, SearchKey, pibIndexName, cik, step, returnFields=['id', 'symbol', 'cik', 'step', 'description', 'insertedDate',\n",
    "                                                                   'pibData'])\n",
    "\n",
    "if r.get_count() == 0:\n",
    "        secFilingsPib = []\n",
    "\n",
    "        # For different section of extracted data, process summarization and generate common answers to questions\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=8000, chunk_overlap=0)\n",
    "\n",
    "        # Item 1 - Describes the business of the company\n",
    "        rawItemDocs1 = [Document(page_content=secFilingList[0]['item1'])]\n",
    "        itemDocs1 = splitter.split_documents(rawItemDocs1)\n",
    "        print(\"Number of documents chunks generated from Item1 : \", len(itemDocs1))\n",
    "        summary1 = generateSummaries(itemDocs1)\n",
    "        outputAnswer1 = summary1['output_text']\n",
    "        secFilingsPib.append({\n",
    "                        \"section\": \"item1\",\n",
    "                        \"summaryType\": \"Business Description\",\n",
    "                        \"summary\": outputAnswer1\n",
    "                })\n",
    "\n",
    "        # Item 1A - Risk Factors\n",
    "        rawItemDocs2 = [Document(page_content=secFilingList[0]['item1A'])]\n",
    "        itemDocs2 = splitter.split_documents(rawItemDocs2)\n",
    "        print(\"Number of documents chunks generated from Item1A : \", len(itemDocs2))\n",
    "        summary2 = generateSummaries(itemDocs2)\n",
    "        outputAnswer2 = summary2['output_text']\n",
    "        secFilingsPib.append({\n",
    "                        \"section\": \"item1A\",\n",
    "                        \"summaryType\": \"Risk Factors\",\n",
    "                        \"summary\": outputAnswer2\n",
    "                })\n",
    "\n",
    "        rawItemDocs2 = [Document(page_content=secFilingList[0]['item3'])]\n",
    "        itemDocs2 = splitter.split_documents(rawItemDocs2)\n",
    "        print(\"Number of documents chunks generated from Item3 : \", len(itemDocs2))\n",
    "        summary2 = generateSummaries(itemDocs2)\n",
    "        outputAnswer2 = summary2['output_text']\n",
    "        secFilingsPib.append({\n",
    "                        \"section\": \"item3\",\n",
    "                        \"summaryType\": \"Legal Proceedings\",\n",
    "                        \"summary\": outputAnswer2\n",
    "                })\n",
    "\n",
    "        # Item 6 - Consolidated Financial Data\n",
    "        rawItemDocs3 = [Document(page_content=secFilingList[0]['item5'])]\n",
    "        itemDocs3 = splitter.split_documents(rawItemDocs3)\n",
    "        print(\"Number of documents chunks generated from Item5 : \", len(itemDocs3))\n",
    "        summary3 = generateSummaries(itemDocs3)\n",
    "        outputAnswer3 = summary3['output_text']\n",
    "        secFilingsPib.append({\n",
    "                        \"section\": \"item5\",\n",
    "                        \"summaryType\": \"Market\",\n",
    "                        \"summary\": outputAnswer3\n",
    "                })\n",
    "\n",
    "        # Item 7 - Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
    "        rawItemDocs4 = [Document(page_content=secFilingList[0]['item7'])]\n",
    "        itemDocs4 = splitter.split_documents(rawItemDocs4)\n",
    "        print(\"Number of documents chunks generated from Item7 : \", len(itemDocs4))\n",
    "        summary4 = generateSummaries(itemDocs4)\n",
    "        outputAnswer4 = summary4['output_text']\n",
    "        secFilingsPib.append({\n",
    "                        \"section\": \"item7\",\n",
    "                        \"summaryType\": \"Management Discussion\",\n",
    "                        \"summary\": outputAnswer4\n",
    "                })\n",
    "\n",
    "        # Item 7a - Market risk disclosures\n",
    "        rawItemDocs5 = [Document(page_content=secFilingList[0]['item7A'])]\n",
    "        itemDocs5= splitter.split_documents(rawItemDocs5)\n",
    "        print(\"Number of documents chunks generated from Item7A : \", len(itemDocs5))\n",
    "        summary5 = generateSummaries(itemDocs5)\n",
    "        outputAnswer5 = summary5['output_text']\n",
    "        secFilingsPib.append({\n",
    "                        \"section\": \"item7A\",\n",
    "                        \"summaryType\": \"Risk Disclosures\",\n",
    "                        \"summary\": outputAnswer5\n",
    "                })\n",
    "\n",
    "        # Item 9 - Disagreements with accountants and changes in accounting\n",
    "        section9 = secFilingList[0]['item9'] + \"\\n \" + secFilingList[0]['item9A'] + \"\\n \" + secFilingList[0]['item9B']\n",
    "        rawItemDocs6 = [Document(page_content=section9)]\n",
    "        itemDocs6 = splitter.split_documents(rawItemDocs6)\n",
    "        print(\"Number of documents chunks generated from Item9 : \", len(itemDocs6))\n",
    "        summary6 = generateSummaries(itemDocs6)\n",
    "        outputAnswer6 = summary6['output_text']\n",
    "        secFilingsPib.append({\n",
    "                        \"section\": \"item9\",\n",
    "                        \"summaryType\": \"Accounting Disclosures\",\n",
    "                        \"summary\": outputAnswer6\n",
    "                })\n",
    "        \n",
    "        s4Data.append({\n",
    "                'id' : str(uuid.uuid4()),\n",
    "                'symbol': symbol,\n",
    "                'cik': cik,\n",
    "                'step': step,\n",
    "                'description': 'SEC Filings',\n",
    "                'insertedDate': today.strftime(\"%Y-%m-%d\"),\n",
    "                'pibData' : str(secFilingsPib)\n",
    "        })\n",
    "        mergeDocs(SearchService, SearchKey, pibIndexName, s4Data)\n",
    "else:\n",
    "        print(\"Step 4 data already exists in the index\")\n",
    "        for item in r:\n",
    "                s4Data.append({\n",
    "                        'id' : item['id'],\n",
    "                        'symbol': item['symbol'],\n",
    "                        'cik': item['cik'],\n",
    "                        'step': item['step'],\n",
    "                        'description': item['description'],\n",
    "                        'insertedDate': item['insertedDate'],\n",
    "                        'pibData' : item['pibData']\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Private Data - Equity Research Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azure.search.documents import SearchClient\n",
    "# from azure.core.credentials import AzureKeyCredential\n",
    "# step = \"5\"\n",
    "# searchClient = SearchClient(endpoint=f\"https://{SearchService}.search.windows.net\",\n",
    "#         index_name=pibIndexName,\n",
    "#         credential=AzureKeyCredential(SearchKey))\n",
    "# r = searchClient.search(  \n",
    "#     search_text=\"\",\n",
    "#     filter=\"cik eq '\" + cik + \"' and step eq '\" + step + \"'\",\n",
    "#     select=[\"id\"],\n",
    "#     semantic_configuration_name=\"semanticConfig\",\n",
    "#     include_total_count=True\n",
    "# )\n",
    "# if r.get_count() > 0:\n",
    "#     for doc in r:\n",
    "#         searchClient.delete_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[324], line 42\u001b[0m\n\u001b[0;32m     32\u001b[0m researchReport\u001b[39m.\u001b[39mappend({\n\u001b[0;32m     33\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mkey\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mPB Recommendation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     34\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m: companyRating[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mratingDetailsPBRecommendation\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     35\u001b[0m })\n\u001b[0;32m     36\u001b[0m researchReport\u001b[39m.\u001b[39mappend({\n\u001b[0;32m     37\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mkey\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mPE Recommendation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m: companyRating[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mratingDetailsPERecommendation\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     39\u001b[0m })\n\u001b[0;32m     40\u001b[0m researchReport\u001b[39m.\u001b[39mappend({\n\u001b[0;32m     41\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mkey\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mAltman ZScore\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m---> 42\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m: fScore[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39maltmanZScore\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     43\u001b[0m })\n\u001b[0;32m     44\u001b[0m researchReport\u001b[39m.\u001b[39mappend({\n\u001b[0;32m     45\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mkey\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mPiotroski Score\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     46\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m: fScore[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mpiotroskiScore\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     47\u001b[0m })\n\u001b[0;32m     48\u001b[0m researchReport\u001b[39m.\u001b[39mappend({\n\u001b[0;32m     49\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mkey\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mEnvironmental Score\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     50\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m: esgScores[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39menvironmentalScore\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     51\u001b[0m })\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "step = \"5\"\n",
    "s5Data = []\n",
    "r = findPibData(SearchService, SearchKey, pibIndexName, cik, step, returnFields=['id', 'symbol', 'cik', 'step', 'description', 'insertedDate',\n",
    "                                                                   'pibData'])\n",
    "\n",
    "if r.get_count() == 0:\n",
    "    companyRating = rating(apikey=apikey, symbol=symbol)\n",
    "    fScore = financialScore(apikey=apikey, symbol=symbol)\n",
    "    esgScores = esgScore(apikey=apikey, symbol=symbol)\n",
    "    esgRating = esgRatings(apikey=apikey, symbol=symbol)\n",
    "    ugConsensus = upgradeDowngrades(apikey=apikey, symbol=symbol)\n",
    "    priceConsensus = priceTarget(apikey=apikey, symbol=symbol)\n",
    "    #ratingsDf = pd.DataFrame.from_dict(pd.json_normalize(companyRating))\n",
    "    researchReport = []\n",
    "\n",
    "    try:\n",
    "        researchReport.append({\n",
    "            \"key\": \"Overall Recommendation\",\n",
    "            \"value\": companyRating[0]['ratingRecommendation']\n",
    "        })\n",
    "        researchReport.append({\n",
    "            \"key\": \"DCF Recommendation\",\n",
    "            \"value\": companyRating[0]['ratingDetailsDCFRecommendation']\n",
    "        })\n",
    "        researchReport.append({\n",
    "            \"key\": \"ROE Recommendation\",\n",
    "            \"value\": companyRating[0]['ratingDetailsROERecommendation']\n",
    "        })\n",
    "        researchReport.append({\n",
    "            \"key\": \"ROA Recommendation\",\n",
    "            \"value\": companyRating[0]['ratingDetailsROARecommendation']\n",
    "        })\n",
    "        researchReport.append({\n",
    "            \"key\": \"PB Recommendation\",\n",
    "            \"value\": companyRating[0]['ratingDetailsPBRecommendation']\n",
    "        })\n",
    "        researchReport.append({\n",
    "            \"key\": \"PE Recommendation\",\n",
    "            \"value\": companyRating[0]['ratingDetailsPERecommendation']\n",
    "        })\n",
    "    except:\n",
    "        logging.info('No data found for companyRating')\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        researchReport.append({\n",
    "            \"key\": \"Altman ZScore\",\n",
    "            \"value\": fScore[0]['altmanZScore']\n",
    "        })\n",
    "        researchReport.append({\n",
    "            \"key\": \"Piotroski Score\",\n",
    "            \"value\": fScore[0]['piotroskiScore']\n",
    "        })\n",
    "    except:\n",
    "        logging.info('No data found for fScore')\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        researchReport.append({\n",
    "            \"key\": \"Environmental Score\",\n",
    "            \"value\": esgScores[0]['environmentalScore']\n",
    "        })\n",
    "        researchReport.append({\n",
    "            \"key\": \"Social Score\",\n",
    "            \"value\": esgScores[0]['socialScore']\n",
    "        })\n",
    "        researchReport.append({\n",
    "            \"key\": \"Governance Score\",\n",
    "            \"value\": esgScores[0]['governanceScore']\n",
    "        })\n",
    "        researchReport.append({\n",
    "            \"key\": \"ESG Score\",\n",
    "            \"value\": esgScores[0]['ESGScore']\n",
    "        })\n",
    "    except:\n",
    "        logging.info('No data found for esgScores')\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        researchReport.append({\n",
    "            \"key\": \"ESG RIsk Rating\",\n",
    "            \"value\": esgRating[0]['ESGRiskRating']\n",
    "        })\n",
    "    except:\n",
    "        logging.info('No data found for esgRating')\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        researchReport.append({\n",
    "            \"key\": \"Analyst Consensus Buy\",\n",
    "            \"value\": ugConsensus[0]['buy']\n",
    "        })\n",
    "        researchReport.append({\n",
    "            \"key\": \"Analyst Consensus Sell\",\n",
    "            \"value\": ugConsensus[0]['sell']\n",
    "        })\n",
    "        researchReport.append({\n",
    "            \"key\": \"Analyst Consensus Strong Buy\",\n",
    "            \"value\": ugConsensus[0]['strongBuy']\n",
    "        })\n",
    "        researchReport.append({\n",
    "            \"key\": \"Analyst Consensus Strong Sell\",\n",
    "            \"value\": ugConsensus[0]['strongSell']\n",
    "        })\n",
    "        researchReport.append({\n",
    "            \"key\": \"Analyst Consensus Hold\",\n",
    "            \"value\": ugConsensus[0]['hold']\n",
    "        })\n",
    "        researchReport.append({\n",
    "            \"key\": \"Analyst Consensus\",\n",
    "            \"value\": ugConsensus[0]['consensus']\n",
    "        })\n",
    "    except:\n",
    "        logging.info('No data found for ugConsensus')\n",
    "        pass\n",
    "    # researchReport.append({\n",
    "    #     \"key\": \"Price Target Consensus\",\n",
    "    #     \"value\": priceConsensus[0]['targetConsensus']\n",
    "    # })\n",
    "    # researchReport.append({\n",
    "    #     \"key\": \"Price Target Median\",\n",
    "    #     \"value\": priceConsensus[0]['targetMedian']\n",
    "    # })\n",
    "  \n",
    "    s5Data.append({\n",
    "                'id' : str(uuid.uuid4()),\n",
    "                'symbol': symbol,\n",
    "                'cik': cik,\n",
    "                'step': step,\n",
    "                'description': 'Research Report',\n",
    "                'insertedDate': today.strftime(\"%Y-%m-%d\"),\n",
    "                'pibData' : str(researchReport)\n",
    "        })\n",
    "    mergeDocs(SearchService, SearchKey, pibIndexName, s5Data)\n",
    "else:\n",
    "    for s in r:\n",
    "        s5Data.append(\n",
    "            {\n",
    "                'id' : s['id'],\n",
    "                'symbol': s['symbol'],\n",
    "                'cik': s['cik'],\n",
    "                'step': s['step'],\n",
    "                'description': s['description'],\n",
    "                'insertedDate': s['insertedDate'],\n",
    "                'pibData' : s['pibData']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "searchClient = SearchClient(endpoint=f\"https://{SearchService}.search.windows.net\",\n",
    "    index_name=pibIndexName,\n",
    "    credential=AzureKeyCredential(SearchKey))\n",
    "\n",
    "r = searchClient.search(  \n",
    "    search_text=\"\",\n",
    "    filter=\"cik eq '\" + \"789019\" + \"' and step eq '\" + \"2\" + \"'\",\n",
    "    select=[\"id\"],\n",
    "    semantic_configuration_name=\"semanticConfig\",\n",
    "    include_total_count=True\n",
    ")\n",
    "if r.get_count() > 0:\n",
    "    for doc in r:\n",
    "        searchClient.delete_documents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Paid Data - Investor Presentations - Financial Reports (Balance Sheet, Income Statement and Cash Flow) for last 3 years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deletePibData(SearchService, SearchKey, pibIndexName, \"896159\", \"1\", returnFields=['id', 'symbol', 'cik', 'step', 'description', 'insertedDate',\n",
    "#                                                                     'pibData'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
