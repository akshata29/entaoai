inputs:
  question:
    type: string
    default: What is the advantage of fine-tuning BERT over using feature-based
      approaches?
    is_chat_input: false
  indexType:
    type: string
    default: cogsearchvs
    is_chat_input: false
  indexNs:
    type: string
    default: 8fe8ee44933240fa8bc1d72858e4d1eb
    is_chat_input: false
  postBody:
    type: object
    default:
      values:
      - recordId: 0
        data:
          text: ""
          approach: rtr
          overrides:
            semantic_ranker: true
            semantic_captions: false
            top: 3
            temperature: 0
            promptTemplate: "Given the following extracted parts of a long document and a
              question, create a final answer.\ 

              \        If you don't know the answer, just say that
              you don't know. Don't try to make up an answer.\ 

              \        If the answer is not contained within the
              text below, say \"I don't know\".


              \        {summaries}

              \        Question: {question}

              \        "
            chainType: stuff
            tokenLength: 1000
            embeddingModelType: azureopenai
            deploymentType: gpt3516k
            searchType: hybridrerank
    is_chat_input: false
  chainType:
    type: string
    default: stuff
    is_chat_input: false
outputs:
  output:
    type: string
    reference: ${followup_questions.output}
    evaluation_only: false
    is_chat_output: false
  answer:
    type: string
    reference: ${followup_questions.output.values[0].data.answer}
  context:
    type: string
    reference: ${followup_questions.output.values[0].data.data_points}
nodes:
- name: create_llm
  type: python
  source:
    type: code
    path: create_llm.py
  inputs:
    conn: entaoai
    overrides: ${parse_postBody.output}
  use_variants: false
- name: parse_postBody
  type: python
  source:
    type: code
    path: parse_postBody.py
  inputs:
    postBody: ${inputs.postBody}
  use_variants: false
- name: search_question_from_vectordb
  type: python
  source:
    type: code
    path: search_question_from_vectordb.py
  inputs:
    question: ${inputs.question}
    embeddedQuestion: ${embed_the_question.output}
    indexType: ${inputs.indexType}
    indexNs: ${inputs.indexNs}
    conn: entaoai
    overrides: ${parse_postBody.output}
  skip:
    when: ${check_cache_answer.output.existingAnswer}
    is: true
    return: ""
- name: execute_langchain
  type: python
  source:
    type: code
    path: execute_langchain.py
  inputs:
    llm: ${create_llm.output}
    overrides: ${parse_postBody.output}
    promptTemplate: ${extract_prompttemplate.output}
    question: ${inputs.question}
    retrievedDocs: ${search_question_from_vectordb.output}
  use_variants: false
  skip:
    when: ${check_cache_answer.output.existingAnswer}
    is: true
    return: ""
- name: followup_questions
  type: python
  source:
    type: code
    path: followup_questions.py
  inputs:
    llm: ${create_llm.output}
    modifiedAnswer: ${execute_langchain.output}
    overrides: ${parse_postBody.output}
    promptTemplate: ${extract_prompttemplate.output}
    question: ${inputs.question}
    retrievedDocs: ${search_question_from_vectordb.output}
    existingAnswer: ${check_cache_answer.output.existingAnswer}
    jsonAnswer: ${check_cache_answer.output.jsonAnswer}
    conn: entaoai
    indexType: ${inputs.indexType}
    indexNs: ${inputs.indexNs}
    embeddedQuestion: ${embed_the_question.output}
  use_variants: false
- name: embed_the_question
  type: python
  source:
    type: code
    path: embed_the_question.py
  inputs:
    conn: entaoai
    question: ${inputs.question}
    overrides: ${parse_postBody.output}
- name: extract_prompttemplate
  type: python
  source:
    type: code
    path: extract_prompttemplate.py
  inputs:
    overrides: ${parse_postBody.output}
  use_variants: false
  skip:
    when: ${check_cache_answer.output.existingAnswer}
    is: true
    return: ""
- name: check_cache_answer
  type: python
  source:
    type: code
    path: check_cache_answer.py
  inputs:
    question: ${inputs.question}
    embeddedQuestion: ${embed_the_question.output}
    indexType: ${inputs.indexType}
    indexNs: ${inputs.indexNs}
    conn: entaoai
