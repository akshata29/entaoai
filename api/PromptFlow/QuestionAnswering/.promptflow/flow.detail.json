{
  "flow_runs": [
    {
      "run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_0",
      "status": "Completed",
      "error": null,
      "inputs": {
        "chainType": "stuff",
        "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
        "indexType": "cogsearchvs",
        "postBody": {
          "values": [
            {
              "data": {
                "approach": "rtr",
                "overrides": {
                  "chainType": "stuff",
                  "deploymentType": "gpt3516k",
                  "embeddingModelType": "azureopenai",
                  "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
                  "searchType": "hybridrerank",
                  "semantic_captions": false,
                  "semantic_ranker": true,
                  "temperature": 0,
                  "tokenLength": 1000,
                  "top": 3
                },
                "text": ""
              },
              "recordId": 0
            }
          ]
        },
        "question": "How are sentence pairs represented in the input to BERT?"
      },
      "output": {
        "answer": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B.",
        "context": [
          "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.",
          "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
          "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008)."
        ],
        "output": {
          "values": [
            {
              "recordId": 0,
              "data": {
                "data_points": [
                  "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.",
                  "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                  "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008)."
                ],
                "answer": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B.",
                "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).']<br>        Question: How are sentence pairs represented in the input to BERT?<br>        ",
                "sources": "\nBert.pdf",
                "nextQuestions": "<What are the two unsupervised tasks used in pre-training BERT?>\n<What is the difference between the feature-based and fine-tuning approaches for applying pre-trained language representations to downstream tasks?>\n<What is the architecture of BERT and how does it differ from other models like OpenAI GPT?>",
                "error": ""
              }
            }
          ]
        }
      },
      "metrics": null,
      "request": null,
      "parent_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7",
      "root_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7",
      "source_run_id": null,
      "flow_id": "default_flow_id",
      "start_time": "2023-09-20T16:48:29.189634Z",
      "end_time": "2023-09-20T16:48:36.379590Z",
      "index": 0,
      "api_calls": [
        {
          "name": "parseBody",
          "type": "Tool",
          "inputs": {
            "postBody": {
              "values": [
                {
                  "data": {
                    "approach": "rtr",
                    "overrides": {
                      "chainType": "stuff",
                      "deploymentType": "gpt3516k",
                      "embeddingModelType": "azureopenai",
                      "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
                      "searchType": "hybridrerank",
                      "semantic_captions": false,
                      "semantic_ranker": true,
                      "temperature": 0,
                      "tokenLength": 1000,
                      "top": 3
                    },
                    "text": ""
                  },
                  "recordId": 0
                }
              ]
            }
          },
          "output": {
            "chainType": "stuff",
            "deploymentType": "gpt3516k",
            "embeddingModelType": "azureopenai",
            "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
            "searchType": "hybridrerank",
            "semantic_captions": false,
            "semantic_ranker": true,
            "temperature": 0,
            "tokenLength": 1000,
            "top": 3
          },
          "start_time": 1695246509.193627,
          "end_time": 1695246509.195653,
          "error": null,
          "children": null,
          "node_name": "parse_postBody"
        },
        {
          "name": "createLlm",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            }
          },
          "output": {
            "model": "gpt-3.5-turbo",
            "request_timeout": null,
            "max_tokens": 1000,
            "stream": false,
            "n": 1,
            "temperature": 0.3,
            "engine": "chat16k",
            "_type": "azure-openai-chat"
          },
          "start_time": 1695246509.202186,
          "end_time": 1695246509.203201,
          "error": null,
          "children": null,
          "node_name": "create_llm"
        },
        {
          "name": "embedQuestion",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            },
            "question": "How are sentence pairs represented in the input to BERT?"
          },
          "output": [
            -0.011118830181658268,
            -0.01631597802042961,
            0.02559560351073742,
            -0.002060397295281291,
            -0.005500198341906071,
            0.031210750341415405,
            0.015424241311848164,
            -0.011599531397223473,
            -0.01773718185722828,
            -0.027727408334612846,
            0.028535543009638786,
            0.04366718605160713,
            -0.035139963030815125,
            0.005427048075944185,
            -0.0009474692051298916,
            0.008422723039984703,
            0.011244230903685093,
            0.009614026173949242,
            0.027797074988484383,
            -0.016956912353634834,
            -0.0016798420110717416,
            -0.006102816667407751,
            -0.016552845016121864,
            -0.0035739096347242594,
            -0.010742628946900368,
            -0.00849238969385624,
            0.029538746923208237,
            -0.020328788086771965,
            -0.003713243408128619,
            -0.009676726534962654,
            0.0021788307931274176,
            0.02357526309788227,
            -0.017918314784765244,
            -0.009948426857590675,
            -0.009258724749088287,
            0.006694984622299671,
            -0.0029294912237674,
            -0.016915112733840942,
            0.02438339963555336,
            -0.023212995380163193,
            0.0071164690889418125,
            0.021694257855415344,
            0.003977977205067873,
            -0.015270974487066269,
            -0.005953032989054918,
            0.01672004535794258,
            0.010652062483131886,
            -0.0065695843659341335,
            -0.012059332802891731,
            0.0093214251101017,
            0.00438204500824213,
            0.015856176614761353,
            -0.03945930674672127,
            -0.020997589454054832,
            -0.02251632697880268,
            -0.0034084508661180735,
            -0.002352998126298189,
            0.032966356724500656,
            -0.0272118728607893,
            -0.013543237000703812,
            0.0005050846957601607,
            -0.008130121976137161,
            0.0009370191837660968,
            0.008053488098084927,
            -0.022404860705137253,
            -0.024466998875141144,
            -0.008548122830688953,
            0.008018654771149158,
            0.019437052309513092,
            0.007398620247840881,
            0.01959032006561756,
            0.010038994252681732,
            0.0009866568725556135,
            -0.001406399649567902,
            0.0428033173084259,
            -0.01388460397720337,
            0.002535873558372259,
            -0.014755439944565296,
            0.001035423600114882,
            0.03592022880911827,
            0.012735101394355297,
            -0.002624698681756854,
            -0.0010606779251247644,
            0.026487339287996292,
            0.015897976234555244,
            -0.012742067687213421,
            0.010436095297336578,
            0.025275135412812233,
            0.002154447603970766,
            0.0040302276611328125,
            0.027379073202610016,
            0.03536289557814598,
            0.012944101355969906,
            0.0067054349929094315,
            -0.0038839271292090416,
            0.012247432954609394,
            -0.01118153054267168,
            0.038818370550870895,
            0.02634800411760807,
            -0.009523458778858185,
            -0.010157427750527859,
            0.008513289503753185,
            -0.04497692361474037,
            -0.005552448332309723,
            -0.024606334045529366,
            0.014852973632514477,
            0.010749596171081066,
            -0.01130693033337593,
            0.003262150567024946,
            0.001316703506745398,
            -0.019074784591794014,
            0.030207548290491104,
            0.006221250165253878,
            -0.024216199293732643,
            -0.0005220660241320729,
            -0.002276364481076598,
            0.015479975380003452,
            -0.039319973438978195,
            -0.009669759310781956,
            0.012602734379470348,
            0.017165912315249443,
            -0.004075510893017054,
            0.010721729137003422,
            -0.034582626074552536,
            0.03288275748491287,
            -0.003422384150326252,
            -0.010819262824952602,
            -0.03380235657095909,
            -0.006520817521959543,
            -0.04288691654801369,
            0.027420874685049057,
            0.022641727700829506,
            0.015939775854349136,
            -0.022753195837140083,
            0.005636048503220081,
            0.01854531653225422,
            -0.02760200761258602,
            -0.02050992287695408,
            -0.01959032006561756,
            -0.004643295891582966,
            0.02257206104695797,
            0.003828193759545684,
            -0.014253838919103146,
            -0.023993264883756638,
            -0.010192261077463627,
            0.018601050600409508,
            0.018782183527946472,
            0.020412389189004898,
            0.0024139564484357834,
            -0.010408228263258934,
            0.026334071531891823,
            -0.001065902877599001,
            -0.029761681333184242,
            0.013807971030473709,
            0.007726054172962904,
            0.0011695323046296835,
            0.00037707184674218297,
            0.009753359481692314,
            -0.009732459671795368,
            -0.005601215176284313,
            -0.020412389189004898,
            0.017012646421790123,
            0.017096245661377907,
            -0.009307491593062878,
            0.024118665605783463,
            0.03349582478404045,
            0.0020900056697428226,
            -0.014866907149553299,
            0.007328953128308058,
            -0.029204346239566803,
            -0.026222603395581245,
            0.018252715468406677,
            -0.01959032006561756,
            -0.003438059240579605,
            0.0040128109976649284,
            0.00180088821798563,
            0.03391382470726967,
            -0.02806180901825428,
            -0.02987314760684967,
            -0.015619308687746525,
            0.0193952526897192,
            0.0065521677024662495,
            0.020119788125157356,
            0.023547397926449776,
            -0.03098781779408455,
            -0.02559560351073742,
            -0.003594809677451849,
            -0.01974358595907688,
            0.01012259442359209,
            -0.034192491322755814,
            0.02382606454193592,
            0.007102536037564278,
            0.000681428937241435,
            -0.01843385025858879,
            -0.6148517727851868,
            -0.03820530325174332,
            -0.009300525300204754,
            -0.028535543009638786,
            0.01698477938771248,
            0.005514131393283606,
            -0.011627398431301117,
            0.03349582478404045,
            -0.024104731157422066,
            0.033579424023628235,
            -0.0008451460162177682,
            -0.023296596482396126,
            -0.0038212270010262728,
            0.007733020931482315,
            -0.010498794727027416,
            -0.012992868199944496,
            0.006503400858491659,
            0.004162594676017761,
            0.00922389142215252,
            0.022335194051265717,
            -0.03500062972307205,
            0.00739165348932147,
            -0.026069337502121925,
            -0.004152144771069288,
            -0.025372669100761414,
            0.0031245583668351173,
            0.006506884470582008,
            -0.021095123142004013,
            -0.009816059842705727,
            0.017151979729533195,
            -0.02045418880879879,
            -0.001589275081641972,
            0.021889325231313705,
            -0.030207548290491104,
            0.025010401383042336,
            0.021471325308084488,
            0.0021439974661916494,
            0.009084558114409447,
            -0.0007767854258418083,
            0.027880676090717316,
            -0.01596764288842678,
            -0.002070847200229764,
            -0.0024035065434873104,
            -0.0031228167936205864,
            0.017360979691147804,
            -0.000540788983926177,
            0.0186428502202034,
            -0.010498794727027416,
            0.006841285154223442,
            -0.004591045901179314,
            -0.008708356879651546,
            -0.003521659644320607,
            -0.007572787348181009,
            -0.002574190264567733,
            0.03550222888588905,
            0.011418397538363934,
            0.0342482253909111,
            -0.0024975568521767855,
            0.0014272996922954917,
            -0.011919998563826084,
            0.008631723001599312,
            0.017305247485637665,
            -0.012303167022764683,
            -0.01918625272810459,
            0.018823985010385513,
            0.009565259329974651,
            -0.006287433672696352,
            0.003291758941486478,
            0.00914725847542286,
            -0.04558999091386795,
            0.0003217737830709666,
            0.015229174867272377,
            0.0004941992810927331,
            -0.015284908004105091,
            0.00554199842736125,
            0.012338000349700451,
            0.0033718757331371307,
            0.011188496835529804,
            -0.002103938953951001,
            0.019757520407438278,
            -0.017444580793380737,
            -0.012247432954609394,
            -0.028842078521847725,
            0.002976516494527459,
            -0.0024052481167018414,
            0.0001037927286233753,
            -0.01738884672522545,
            0.017193779349327087,
            -0.01478330697864294,
            -0.0036505432799458504,
            0.0033718757331371307,
            0.03199101984500885,
            -0.028842078521847725,
            -0.042050912976264954,
            0.004918479826301336,
            0.011376597918570042,
            0.013125235214829445,
            -0.0074613201431930065,
            0.03112715110182762,
            -0.03722996637225151,
            0.014246871694922447,
            -0.020565655082464218,
            0.012351933866739273,
            -0.005886849481612444,
            0.02675207331776619,
            0.03600383177399635,
            -0.007475253660231829,
            0.0011259905295446515,
            -0.025707069784402847,
            0.0035303679760545492,
            -0.005911232437938452,
            -0.010401261039078236,
            -0.0035390763077884912,
            -0.0003302644472569227,
            -0.014212038367986679,
            -0.04040677472949028,
            0.011843365617096424,
            0.01065902877599001,
            0.01631597802042961,
            -0.003516434459015727,
            0.02599967084825039,
            0.01843385025858879,
            0.015521774999797344,
            -0.02886994555592537,
            0.01868464983999729,
            0.012045399285852909,
            0.0024522733874619007,
            0.006210800260305405,
            0.020844323560595512,
            0.02886994555592537,
            -0.011495031416416168,
            0.02172212488949299,
            0.010213160887360573,
            -0.010213160887360573,
            -0.002133547328412533,
            0.01571684330701828,
            0.0012836117530241609,
            -0.007726054172962904,
            0.01097949594259262,
            -0.0035094679333269596,
            -0.02130412496626377,
            -0.008144055493175983,
            -0.00489758001640439,
            -0.012553967535495758,
            -0.028591277077794075,
            -0.015647174790501595,
            -0.021931126713752747,
            -0.0011024780105799437,
            0.0001773239200701937,
            -0.02266959473490715,
            0.011599531397223473,
            -0.018475649878382683,
            -0.02528906799852848,
            -0.017347047105431557,
            -0.023394130170345306,
            -0.014616106636822224,
            -0.011655264534056187,
            -0.032213952392339706,
            -0.008617790415883064,
            -0.029092878103256226,
            0.006754201836884022,
            0.02534480206668377,
            -0.027588075026869774,
            -0.017765048891305923,
            -0.009676726534962654,
            -0.013877637684345245,
            0.0418279804289341,
            0.030792750418186188,
            0.0045875622890889645,
            -0.016190577298402786,
            0.013034668751060963,
            -0.03792663663625717,
            0.0009213441517204046,
            0.02201472595334053,
            0.0011190238874405622,
            0.01848958246409893,
            -0.02605540305376053,
            0.009906627237796783,
            -0.00020399325876496732,
            -0.032576221972703934,
            -0.009857860393822193,
            0.040573976933956146,
            -0.002218889305368066,
            0.006458117626607418,
            0.012233499437570572,
            0.0017199005233123899,
            0.003911793697625399,
            -0.0036261598579585552,
            -0.021025456488132477,
            0.015842242166399956,
            0.011237263679504395,
            0.019534585997462273,
            -0.017904382199048996,
            0.01627417653799057,
            -0.004322828259319067,
            0.02237699367105961,
            0.007948988117277622,
            0.027615942060947418,
            -0.014532506465911865,
            0.018754318356513977,
            0.025163667276501656,
            0.009850893169641495,
            -0.0022711395286023617,
            -0.0011756281601265073,
            -0.006064499728381634,
            -0.00982302613556385,
            -0.0011277322191745043,
            -0.03553009778261185,
            0.023338396102190018,
            0.0015709876315668225,
            0.02220979332923889,
            -0.02906501293182373,
            -0.033217158168554306,
            -0.01804371550679207,
            -0.008206755854189396,
            0.0012374575017020106,
            -0.009084558114409447,
            0.03265982121229172,
            -0.0279642753303051,
            0.00955132581293583,
            -0.006064499728381634,
            -0.012909268029034138,
            0.038706906139850616,
            0.012254400178790092,
            0.006893535144627094,
            0.006043599918484688,
            -0.0029660663567483425,
            -0.00960009265691042,
            0.015521774999797344,
            -0.020342722535133362,
            0.019228052347898483,
            0.02076072245836258,
            0.029956746846437454,
            -0.0038665104657411575,
            -0.015424241311848164,
            -0.010533628053963184,
            0.02630620449781418,
            0.009356258437037468,
            0.03279915452003479,
            0.003523401217535138,
            -0.012156866490840912,
            0.0030531499069184065,
            -0.004110344685614109,
            -0.0040406775660812855,
            0.053977880626916885,
            -0.01005292683839798,
            0.01995258778333664,
            0.030820617452263832,
            -0.01141143124550581,
            -0.0074404203332960606,
            0.017263446003198624,
            -0.011947865597903728,
            -0.017360979691147804,
            0.007551887072622776,
            0.017207713797688484,
            0.0009274400072172284,
            0.024801399558782578,
            0.0003629207785706967,
            -0.011195464059710503,
            0.010011127218604088,
            0.004267094656825066,
            -0.01249823346734047,
            0.02024518884718418,
            0.01787651516497135,
            0.023561330512166023,
            -0.014643972739577293,
            -0.030151814222335815,
            -0.028298676013946533,
            -0.012017532251775265,
            0.00023577875981573015,
            -0.017514247447252274,
            -0.004256644751876593,
            -0.010247994214296341,
            0.006573067978024483,
            0.003876960603520274,
            0.004364628344774246,
            -0.02363099716603756,
            0.01733311451971531,
            0.005022980272769928,
            0.02181965857744217,
            -0.006931852083653212,
            -0.020886123180389404,
            0.028131475672125816,
            -0.0040580942295491695,
            -0.011146697215735912,
            -0.015438174828886986,
            -0.019910788163542747,
            -0.010617229156196117,
            -0.028075743466615677,
            0.017597848549485207,
            0.008882524445652962,
            0.0077887545339763165,
            -0.020273054018616676,
            -0.003695826744660735,
            -0.002946907887235284,
            0.0015431208303198218,
            0.008861623704433441,
            -0.01118153054267168,
            0.013076468370854855,
            0.01125816348940134,
            -0.012045399285852909,
            0.005364347714930773,
            -0.016761844977736473,
            0.014448905363678932,
            0.049909334629774094,
            -0.006653184536844492,
            -0.02172212488949299,
            -0.01985505409538746,
            -0.018768250942230225,
            -0.018670717254281044,
            0.02332446351647377,
            0.010387328453361988,
            -0.019116584211587906,
            0.01153683103621006,
            0.024759599938988686,
            0.02372853085398674,
            -0.009425925090909004,
            0.0016868087695911527,
            0.011919998563826084,
            -0.0067054349929094315,
            -0.017402781173586845,
            -0.03753650188446045,
            -0.029455145820975304,
            0.0005564640159718692,
            0.025804603472352028,
            0.06325750797986984,
            0.0035094679333269596,
            0.0036993101239204407,
            -0.019464919343590736,
            0.0037480767350643873,
            -0.018015848472714424,
            -0.01592584326863289,
            0.0011303446954116225,
            -0.015633242204785347,
            -0.021485257893800735,
            -0.00836002267897129,
            0.0035652013029903173,
            0.0056499820202589035,
            0.02126232348382473,
            0.0009013149538077414,
            0.0013184451963752508,
            -0.05768415704369545,
            0.02559560351073742,
            -0.000957048439886421,
            0.003974494058638811,
            -0.014337439090013504,
            0.020816456526517868,
            0.03461049497127533,
            0.005099613685160875,
            -0.01752818003296852,
            0.01899118535220623,
            0.02509400062263012,
            0.018712516874074936,
            -0.04010024294257164,
            0.0060157328844070435,
            0.006917918566614389,
            -0.008798924274742603,
            0.00041408237302675843,
            -0.013139168731868267,
            0.022599928081035614,
            0.01708231307566166,
            0.006802968215197325,
            0.03380235657095909,
            0.0037202101666480303,
            -0.003915277309715748,
            0.012066299095749855,
            0.008645656518638134,
            -0.006245633587241173,
            -0.0037027932703495026,
            0.00836002267897129,
            -0.0035808763932436705,
            0.018503516912460327,
            0.02282286249101162,
            0.0013524078531190753,
            0.007238386198878288,
            -0.014379238709807396,
            -0.01783471554517746,
            -0.004451712127774954,
            -0.0074891867116093636,
            -0.007384686730802059,
            -0.011982698924839497,
            -0.009405025281012058,
            -0.025261200964450836,
            -0.023254796862602234,
            -0.003335300600156188,
            -0.00876409001648426,
            0.018475649878382683,
            -0.01139053050428629,
            0.009614026173949242,
            -0.005716165527701378,
            -0.003869993845000863,
            -0.017165912315249443,
            -0.013362103141844273,
            -0.0016040793852880597,
            -0.02634800411760807,
            -0.006130683235824108,
            -0.04840452969074249,
            -0.020328788086771965,
            0.023686731234192848,
            0.024397332221269608,
            0.029148612171411514,
            -0.013313336297869682,
            0.005172763951122761,
            -0.00545143149793148,
            -0.010059894062578678,
            -0.008018654771149158,
            0.016538910567760468,
            -0.005611665081232786,
            -0.016455311328172684,
            0.02609720453619957,
            -0.034136757254600525,
            -0.026111137121915817,
            -0.01950671896338463,
            0.02161065861582756,
            -0.011550764553248882,
            0.01779291406273842,
            -0.01468577329069376,
            -0.022237660363316536,
            -0.0067054349929094315,
            0.029287945479154587,
            -0.004845329560339451,
            0.02746267430484295,
            -0.0020969724282622337,
            -0.02605540305376053,
            0.020872190594673157,
            -0.03722996637225151,
            0.0004397720331326127,
            -0.03472195938229561,
            -0.012888368219137192,
            -0.003988427575677633,
            -0.0021770892199128866,
            0.012331033125519753,
            -0.014072705060243607,
            -0.003828193759545684,
            -0.0120941661298275,
            -0.01315310224890709,
            -0.01251913420855999,
            -0.0035025011748075485,
            0.046119458973407745,
            0.013055568560957909,
            0.004500478971749544,
            -0.014797240495681763,
            0.009202991612255573,
            -0.020523855462670326,
            -0.006771618500351906,
            -0.015340641140937805,
            0.02553986944258213,
            0.00311759184114635,
            -0.014866907149553299,
            0.03867903724312782,
            -0.01282566785812378,
            -0.011926965788006783,
            -0.015438174828886986,
            0.035335030406713486,
            -0.0234359297901392,
            0.045617856085300446,
            0.0035460428334772587,
            0.0027309407014399767,
            -0.017500314861536026,
            0.008276422508060932,
            -0.005280747544020414,
            -0.010895895771682262,
            -0.03148941695690155,
            0.018614983186125755,
            0.026361938565969467,
            -0.012303167022764683,
            -0.003533851122483611,
            -0.022530261427164078,
            -0.01612091064453125,
            -0.020091921091079712,
            -0.024271931499242783,
            0.04508838802576065,
            -0.0038978606462478638,
            0.003025283105671406,
            0.007168719545006752,
            0.009363225661218166,
            -0.0028371827211230993,
            -0.0002782320079859346,
            -0.037257835268974304,
            -0.020426321774721146,
            0.0015509583754464984,
            0.0397937074303627,
            0.0193952526897192,
            0.024620266631245613,
            0.04701119288802147,
            -0.011947865597903728,
            0.018127316609025,
            0.020217321813106537,
            -0.0008625627378933132,
            -0.0031837753485888243,
            -0.030346881598234177,
            -0.008234621956944466,
            -0.015995509922504425,
            0.014191138558089733,
            0.007067702244967222,
            -0.0076424540020525455,
            0.006956235505640507,
            0.01985505409538746,
            0.015006240457296371,
            0.011077029630541801,
            -0.003838643664494157,
            -0.0021370307076722383,
            0.0010876738233491778,
            -0.019172318279743195,
            -0.01256790105253458,
            0.01869858428835869,
            -0.018308449536561966,
            0.00748222041875124,
            -0.03380235657095909,
            -0.004695545881986618,
            0.0031559085473418236,
            -0.018865784630179405,
            0.011940899305045605,
            -0.00720355287194252,
            0.019074784591794014,
            -0.019269851967692375,
            0.010282827541232109,
            -0.016998711973428726,
            0.013821904547512531,
            -0.013229736126959324,
            -0.010888929478824139,
            -0.023463796824216843,
            -0.001105090486817062,
            0.01677577942609787,
            0.019966520369052887,
            -0.011620431207120419,
            0.0018583633936941624,
            -0.030514081940054893,
            0.015326708555221558,
            -0.008137088268995285,
            -0.029148612171411514,
            -0.003768976777791977,
            0.001748638111166656,
            0.007879321463406086,
            -0.016817579045891762,
            -0.04946346580982208,
            -0.034638360142707825,
            -0.013362103141844273,
            -0.009634925983846188,
            -0.001558795920573175,
            -0.016399577260017395,
            0.015298841521143913,
            -0.011042196303606033,
            -0.04135424643754959,
            -0.004496995359659195,
            -0.0026560488622635603,
            0.03023541532456875,
            0.022335194051265717,
            0.005200630519539118,
            0.012985901907086372,
            0.030904216691851616,
            0.013389969244599342,
            -0.011007362976670265,
            0.00028258617385290563,
            -0.019019052386283875,
            0.009920559823513031,
            0.011188496835529804,
            -0.023867864161729813,
            -0.0428311824798584,
            0.0299846138805151,
            0.027393007650971413,
            -0.017667515203356743,
            -0.029343679547309875,
            0.015856176614761353,
            -0.018782183527946472,
            -0.001386370393447578,
            0.00997629389166832,
            -0.011425363831222057,
            -0.021122990176081657,
            -0.009035791270434856,
            -0.012170800007879734,
            0.014295638538897038,
            0.01468577329069376,
            -0.005298164207488298,
            -0.017848648130893707,
            0.013013768941164017,
            -0.020119788125157356,
            0.005218047648668289,
            0.010881963185966015,
            -0.010847128927707672,
            0.024007197469472885,
            0.01471363939344883,
            0.022711394354701042,
            0.014267771504819393,
            0.03304995596408844,
            0.026334071531891823,
            -0.030709149315953255,
            0.018378116190433502,
            -0.013452669605612755,
            -0.003040958195924759,
            -0.007224452681839466,
            0.005360864568501711,
            0.004469128791242838,
            0.0304862167686224,
            -0.022711394354701042,
            -0.00927265826612711,
            0.004099894315004349,
            -0.009767292998731136,
            0.017653580754995346,
            0.008729256689548492,
            -0.00025994444149546325,
            0.004911513067781925,
            -0.011522897519171238,
            0.007928088307380676,
            0.034638360142707825,
            0.006123716477304697,
            -0.02176392637193203,
            -0.0007519666105508804,
            -0.0016493628500029445,
            0.0037445933558046818,
            -0.02690533921122551,
            0.009453792124986649,
            0.029510879889130592,
            -0.0418279804289341,
            -0.00914029125124216,
            0.027518408372998238,
            0.011334797367453575,
            0.011049163527786732,
            -0.029148612171411514,
            0.011090963147580624,
            0.013167035765945911,
            0.021025456488132477,
            -0.013410869985818863,
            -0.010206194594502449,
            -0.007503120228648186,
            0.015326708555221558,
            -0.03006821498274803,
            -0.017165912315249443,
            -0.019924720749258995,
            -0.011857299134135246,
            0.0158701092004776,
            0.021039390936493874,
            0.007565820589661598,
            -0.007872354239225388,
            0.010540595278143883,
            -0.029343679547309875,
            0.010241027921438217,
            0.009753359481692314,
            -0.004260128363966942,
            -0.00318725872784853,
            0.014769373461604118,
            -0.0038665104657411575,
            0.010450027883052826,
            0.0176257137209177,
            0.020133720710873604,
            -0.0065626176074147224,
            0.01000415999442339,
            -0.030597683042287827,
            -0.015897976234555244,
            -0.007955954410135746,
            0.004124277736991644,
            -0.00038055519689805806,
            -0.02800607495009899,
            0.006447667255997658,
            0.004775662906467915,
            0.00568133220076561,
            -0.0022397893480956554,
            -0.015354574657976627,
            -0.005329514387995005,
            0.002267656149342656,
            -0.011676165275275707,
            0.00816495530307293,
            0.016566777601838112,
            -0.023310529068112373,
            0.037007033824920654,
            0.0067054349929094315,
            0.04951919987797737,
            0.0018670717254281044,
            -0.01606517657637596,
            -0.03491702675819397,
            -0.026125071570277214,
            -0.017556047067046165,
            -0.026877472177147865,
            -0.03304995596408844,
            -0.025484135374426842,
            0.02790854312479496,
            0.019562453031539917,
            -0.027128273621201515,
            -0.03260408714413643,
            -0.018196983262896538,
            -0.019659986719489098,
            0.00997629389166832,
            0.01646924391388893,
            0.01779291406273842,
            0.010575428605079651,
            0.01617664471268654,
            -0.02841014415025711,
            0.03714636713266373,
            0.010087761096656322,
            -0.0006901372689753771,
            -0.013550203293561935,
            0.012581833638250828,
            0.011397497728466988,
            -0.01612091064453125,
            0.004803529474884272,
            0.02473173290491104,
            -0.04751279577612877,
            -0.0007872354472056031,
            0.03784303739666939,
            -0.006451150868088007,
            0.02403506450355053,
            0.03918064013123512,
            -0.019715720787644386,
            -0.03480556234717369,
            -0.01430957205593586,
            0.018322382122278214,
            0.033022090792655945,
            0.01692904531955719,
            0.003636609762907028,
            -0.0019855054561048746,
            -0.01342480257153511,
            0.0052772643975913525,
            0.013717403635382652,
            -0.026724206283688545,
            0.0027675158344209194,
            0.0054792980663478374,
            0.0037724601570516825,
            -0.02397933229804039,
            -0.017221646383404732,
            0.011676165275275707,
            0.012059332802891731,
            -0.014978373423218727,
            0.020523855462670326,
            -0.004420361947268248,
            -0.02967808023095131,
            0.0049393801018595695,
            0.02679387293756008,
            0.0015971126267686486,
            -0.020900055766105652,
            0.0110839968547225,
            -0.027936408296227455,
            0.0032029335852712393,
            0.02161065861582756,
            0.012958034873008728,
            -0.0052842311561107635,
            -0.002953874645754695,
            0.005691782105714083,
            -0.02573493681848049,
            -0.00224153115414083,
            -0.007691220846027136,
            0.0058694323524832726,
            -0.018461717292666435,
            0.04625879228115082,
            0.02917647920548916,
            -0.05105186998844147,
            0.017458513379096985,
            -0.013780103996396065,
            -0.008833757601678371,
            -0.019032984972000122,
            0.0032691170927137136,
            0.005033430177718401,
            -0.021136924624443054,
            0.02101152390241623,
            0.0037480767350643873,
            -0.03505636006593704,
            -0.0019611220341175795,
            -0.0012052366510033607,
            0.008680489845573902,
            -0.01047789491713047,
            0.02574886940419674,
            0.2039845585823059,
            -0.03405315801501274,
            0.01833631657063961,
            0.0029573580250144005,
            0.013780103996396065,
            0.0176257137209177,
            0.0037550434935837984,
            0.017709314823150635,
            -0.00217186426743865,
            -0.001844429993070662,
            0.00048462007544003427,
            0.008060455322265625,
            -0.013222768902778625,
            0.0024365982972085476,
            0.0224187932908535,
            0.0010824487544596195,
            -0.012066299095749855,
            -0.046175193041563034,
            -0.00687611848115921,
            -0.0056778485886752605,
            -0.005225013941526413,
            0.014699706807732582,
            -0.003213383723050356,
            -0.036505430936813354,
            0.014351372607052326,
            -0.012916235253214836,
            -0.006691501475870609,
            -0.026919273659586906,
            0.006419800687581301,
            0.03472195938229561,
            -0.007377719972282648,
            0.01858711615204811,
            -0.008241589181125164,
            0.015479975380003452,
            -0.009690659120678902,
            -0.020384522154927254,
            -0.02272532880306244,
            0.02222372591495514,
            0.011056129820644855,
            -0.009425925090909004,
            0.035084228962659836,
            0.010401261039078236,
            -0.008025621995329857,
            -0.012331033125519753,
            -0.00031154145835898817,
            -0.00023947982117533684,
            0.000322644627885893,
            -0.013814937323331833,
            -0.002095230622217059,
            0.03023541532456875,
            -0.010512728244066238,
            0.02654307149350643,
            0.039124906063079834,
            0.042691849172115326,
            0.0030461831483989954,
            -0.014267771504819393,
            0.002694365568459034,
            0.00595651613548398,
            0.005886849481612444,
            0.01949278637766838,
            -0.025762803852558136,
            0.0186428502202034,
            0.002546323463320732,
            0.03112715110182762,
            0.03179595246911049,
            0.015856176614761353,
            -0.019882921129465103,
            0.012393733486533165,
            0.015730775892734528,
            -0.004723412916064262,
            -0.015382441692054272,
            -0.016817579045891762,
            -0.005925165954977274,
            -0.004653745796531439,
            -0.03502849489450455,
            -0.02126232348382473,
            0.051414139568805695,
            0.01818304881453514,
            0.009565259329974651,
            0.034081026911735535,
            -0.018754318356513977,
            -0.005834599025547504,
            0.011369630694389343,
            -0.014894773252308369,
            -0.026779938489198685,
            -0.043054115027189255,
            0.017305247485637665,
            -0.01395427156239748,
            -0.017946181818842888,
            -0.016483178362250328,
            0.0023791231215000153,
            -0.021081190556287766,
            -0.008959157392382622,
            0.008617790415883064,
            -0.0004689450142905116,
            -0.010735662654042244,
            -0.019172318279743195,
            0.03940357267856598,
            -0.0014569080667570233,
            -0.021596726030111313,
            -0.03452689200639725,
            0.04703906178474426,
            0.03842823579907417,
            0.02644553780555725,
            -0.0008155376417562366,
            0.004946346860378981,
            -0.01019922737032175,
            -0.008157989010214806,
            0.022502394393086433,
            -0.0013027702225372195,
            0.001785213127732277,
            -0.012386767193675041,
            0.0019053885480389,
            0.007997754961252213,
            -0.0018931968370452523,
            -0.011327831074595451,
            0.005221530795097351,
            -0.014393172226846218,
            0.012637567706406116,
            -0.005221530795097351,
            0.0005899911629967391,
            -0.03388595953583717,
            -0.039821576327085495,
            0.017946181818842888,
            -0.0014830331783741713,
            -0.004685095977038145,
            -0.016413511708378792,
            -0.02619473822414875,
            -0.01430957205593586,
            -0.007405586540699005,
            0.011327831074595451,
            -0.03265982121229172,
            0.03530716150999069,
            -0.021582791581749916,
            -0.02906501293182373,
            0.00651036761701107,
            0.01950671896338463,
            -0.017542114481329918,
            -0.01779291406273842,
            0.009941460564732552,
            0.009460759349167347,
            -5.8073856052942574e-05,
            0.006141133140772581,
            -0.006984102074056864,
            0.01627417653799057,
            -0.0468439944088459,
            0.0076215541921556,
            -0.014434972777962685,
            -0.0014351372374221683,
            -0.03625463321805,
            -0.01478330697864294,
            -0.007649420760571957,
            0.0015466042095795274,
            -0.019311651587486267,
            0.023463796824216843,
            -0.002380864694714546,
            -0.018670717254281044,
            -0.015159507282078266,
            -0.013264569453895092,
            0.0021109057124704123,
            -0.028786344453692436,
            -0.015591442584991455,
            -0.0022641727700829506,
            -0.025219401344656944,
            -0.009077590890228748,
            -0.009579192847013474,
            -0.17745541036128998,
            0.010847128927707672,
            0.02076072245836258,
            -0.032074619084596634,
            -0.005552448332309723,
            0.02711433917284012,
            0.032269686460494995,
            -0.009850893169641495,
            -0.020482055842876434,
            0.003523401217535138,
            -0.0010632904013618827,
            -0.017012646421790123,
            -0.02856341004371643,
            -0.0007136498461477458,
            0.004350695293396711,
            0.01306950207799673,
            0.0018235299503430724,
            0.02045418880879879,
            0.024898933246731758,
            0.02825687639415264,
            0.03143368661403656,
            -0.044614654034376144,
            0.013536269776523113,
            -0.007321986369788647,
            0.02064925618469715,
            -0.009767292998731136,
            -0.010171361267566681,
            0.03137795254588127,
            -0.017151979729533195,
            -0.014671839773654938,
            0.012128999456763268,
            0.02725367434322834,
            0.012101132422685623,
            -0.004552728962153196,
            0.00025515485322102904,
            0.020886123180389404,
            0.01123029738664627,
            -0.01334816962480545,
            -0.0011651781387627125,
            -0.008415755815804005,
            0.05252880975604057,
            -0.00992752704769373,
            -0.0076145874336361885,
            -0.014121471904218197,
            -0.023644929751753807,
            -0.011097930371761322,
            0.007670320570468903,
            -0.0022746229078620672,
            0.016009442508220673,
            0.018364183604717255,
            -0.01748638041317463,
            -0.020384522154927254,
            0.020635321736335754,
            0.006398900877684355,
            0.007005002349615097,
            0.02679387293756008,
            0.011662231758236885,
            0.00821372214704752,
            -0.026166871190071106,
            0.015535708516836166,
            0.004977697040885687,
            -0.033022090792655945,
            0.026013603433966637,
            0.002319906372576952,
            -0.014978373423218727,
            -0.0016868087695911527,
            0.0018479133723303676,
            0.004333278629928827,
            -0.021986858919262886,
            0.011202430352568626,
            -0.03647756576538086,
            -0.023463796824216843,
            0.0003302644472569227,
            0.0047408295795321465,
            0.022599928081035614,
            -0.00012757742661051452,
            -0.019367385655641556,
            -0.0012609701370820403,
            0.001722512999549508,
            -0.01123029738664627,
            -0.033022090792655945,
            0.0317402184009552,
            -0.0096976263448596,
            0.026975005865097046,
            0.0032081587705761194,
            -0.012087199836969376,
            0.005521098151803017,
            0.018322382122278214,
            -0.019715720787644386,
            -0.026431605219841003,
            0.011432331055402756,
            0.0060226996429264545,
            0.017542114481329918,
            0.0019785386975854635,
            0.007412553299218416,
            0.01779291406273842,
            0.010220128111541271,
            -0.015284908004105091,
            0.022181926295161247,
            -0.012853534892201424,
            -0.02105332352221012,
            -0.017347047105431557,
            0.006381483748555183,
            0.009307491593062878,
            0.012783868238329887,
            0.02533086948096752,
            -0.019618187099695206,
            0.006694984622299671,
            0.029037145897746086,
            0.011481097899377346,
            -0.006043599918484688,
            -0.010833196341991425,
            0.00023664960463065654,
            0.02629227191209793,
            0.017820781096816063,
            0.009133324958384037,
            -0.03193528577685356,
            -0.0027918992564082146,
            0.005256364122033119,
            -0.02599967084825039,
            0.043499983847141266,
            -0.010603295639157295,
            0.003612226340919733,
            0.014978373423218727,
            -0.0007920250645838678,
            -0.01804371550679207,
            -0.11425364017486572,
            -0.008206755854189396,
            -0.002555032027885318,
            0.004354178439825773,
            -0.014490705914795399,
            -0.004120794590562582,
            0.02172212488949299,
            0.029483012855052948,
            -0.026027537882328033,
            0.010624195449054241,
            -0.020579589530825615,
            -0.02711433917284012,
            0.013090401887893677,
            0.004803529474884272,
            -0.005319064483046532,
            0.008185855112969875,
            0.0033701341599226,
            -0.0047582462430000305,
            -0.009251758456230164,
            0.026682404801249504,
            -0.0026473405305296183,
            -0.01959032006561756,
            0.014393172226846218,
            -0.024048998951911926,
            0.015842242166399956,
            0.00677858479321003,
            -0.029705947265028954,
            0.0015788250602781773,
            0.021889325231313705,
            0.011028262786567211,
            0.007398620247840881,
            -0.03455476090312004,
            -0.00881285686045885,
            -0.003410192672163248,
            -0.0019837636500597,
            -0.003326592268422246,
            -0.012909268029034138,
            -0.022558128461241722,
            0.005782349035143852,
            -0.03405315801501274,
            0.0029051078017801046,
            0.013292435556650162,
            -0.004169561434537172,
            -0.010157427750527859,
            -0.010136527009308338,
            -0.00012942795001436025,
            -0.014434972777962685,
            0.030151814222335815,
            0.023770330473780632,
            -0.00803258828818798,
            -0.012198666110634804,
            -0.02045418880879879,
            -0.02499646693468094,
            -0.005367831327021122,
            0.020941857248544693,
            0.033133555203676224,
            0.0186428502202034,
            0.013598970137536526,
            -0.004517895635217428,
            -0.012965002097189426,
            -0.000530774355866015,
            -0.005092646926641464,
            -0.01698477938771248,
            0.005144897382706404,
            0.018628917634487152,
            -0.001567504252307117,
            -0.05473028123378754,
            -0.011167597025632858,
            -0.008673523552715778,
            -0.005350414663553238,
            -0.014156305231153965,
            0.01793224923312664,
            0.013194902800023556,
            0.02155492454767227,
            -0.01858711615204811,
            -0.01012259442359209,
            -0.014281705021858215,
            -0.0021091641392558813,
            0.0022066975943744183,
            -0.018726451322436333,
            -0.01090286299586296,
            -0.0274487417191267,
            0.007698187604546547,
            -0.002624698681756854,
            -0.010220128111541271,
            -0.012804768048226833,
            -0.0012182991486042738,
            -0.02499646693468094,
            -0.011745831929147243,
            -0.029761681333184242,
            0.009662793017923832,
            0.008645656518638134,
            -0.029761681333184242,
            -0.008234621956944466,
            -0.004180011339485645,
            -0.0024644650984555483,
            -0.0031994504388421774,
            -0.01390550471842289,
            0.020217321813106537,
            0.005987866315990686,
            -0.015075907111167908,
            0.0043750787153840065,
            -0.04391798377037048,
            0.016162710264325142,
            0.0032952423207461834,
            -0.01698477938771248,
            0.011655264534056187,
            0.018823985010385513,
            0.03692343458533287,
            -0.017040513455867767,
            0.002717007417231798,
            0.008854657411575317,
            -0.007363786455243826,
            0.021485257893800735,
            -0.01738884672522545,
            -0.018977250903844833,
            -0.006447667255997658,
            -0.010993429459631443,
            0.005925165954977274,
            -0.004507445730268955,
            0.029204346239566803,
            0.005803248845040798,
            -0.01499230694025755,
            -0.016538910567760468,
            0.009572225622832775,
            -0.00025798508431762457,
            -0.007774821016937494,
            0.013828870840370655,
            -0.010234060697257519,
            0.026320137083530426,
            -0.026612738147377968,
            -0.011780665256083012,
            0.020077988505363464,
            -0.02403506450355053,
            -0.0008969607879407704,
            0.030040347948670387,
            -0.0010589362354949117,
            0.016664311289787292,
            0.02423013187944889,
            0.008596889674663544,
            0.03622676432132721,
            0.004639812745153904,
            -0.014755439944565296,
            -0.03600383177399635,
            0.0020307889208197594,
            -0.012540034018456936,
            -0.00260728201828897,
            0.022251592949032784,
            -0.0027222323697060347,
            0.003500759368762374,
            0.015479975380003452,
            -0.006398900877684355,
            0.008778023533523083,
            0.02091399021446705,
            0.019074784591794014,
            -0.029009278863668442,
            -0.015953710302710533,
            -0.00849238969385624,
            -0.0035286261700093746,
            0.02278106100857258,
            0.002276364481076598,
            -0.0034415426198393106,
            0.0367841012775898,
            -0.015048041008412838,
            0.022279459983110428,
            0.003981460817158222,
            0.0076424540020525455,
            -0.008750157430768013,
            0.012428566813468933,
            0.007039835676550865,
            -0.02912074513733387,
            -0.02827080897986889,
            -0.010568462312221527,
            0.009251758456230164,
            -0.011773698963224888,
            0.02565133571624756,
            0.029037145897746086,
            0.005252880975604057,
            -0.0007088602287694812,
            0.0114601980894804,
            -0.005998316220939159,
            0.01478330697864294,
            0.012484300881624222,
            -0.02459239959716797,
            -0.016845446079969406,
            -0.014448905363678932,
            0.024453066289424896,
            0.002779707545414567,
            -0.004451712127774954,
            0.012170800007879734,
            -0.005378281231969595,
            0.02195899188518524,
            0.012184732593595982,
            0.001861846656538546,
            0.009760326705873013,
            0.03555796295404434,
            0.00401629414409399,
            0.01490870676934719,
            0.007349853403866291,
            -0.019269851967692375,
            0.006796001922339201,
            -0.021749991923570633,
            0.006127200089395046,
            0.015828309580683708,
            -0.012463400140404701,
            -0.010094727389514446,
            -0.023157263174653053,
            0.021136924624443054,
            -0.013229736126959324,
            -0.00457362923771143,
            0.003953594248741865,
            0.03770370036363602,
            -0.0011442780960351229,
            0.015298841521143913,
            -0.01219169981777668,
            0.01080532930791378,
            -0.034276094287633896,
            0.02765774168074131,
            -0.007210519630461931,
            -0.036505430936813354,
            -0.026570938527584076,
            0.033022090792655945,
            0.021931126713752747,
            -0.01518737431615591,
            0.044558919966220856,
            -0.012212599627673626,
            0.018559250980615616,
            0.010819262824952602,
            0.024717800319194794,
            -0.011906065978109837,
            0.02638980560004711,
            0.02278106100857258,
            0.017597848549485207,
            -0.008680489845573902,
            0.01989685371518135,
            -0.01874038390815258,
            -0.021136924624443054,
            -0.012310133315622807,
            -0.006607901304960251,
            0.03279915452003479,
            -0.0008817211491987109,
            0.10160214453935623,
            0.021248390898108482,
            -0.0030949499923735857,
            0.012115065939724445,
            -0.015702908858656883,
            0.011871231719851494,
            0.013362103141844273,
            0.018921518698334694,
            -0.011474130675196648,
            -0.0010545820696279407,
            0.006339683663100004,
            -0.015591442584991455,
            -0.013689536601305008,
            -0.02423013187944889,
            0.0035355929285287857,
            -0.015605375170707703,
            0.003765493631362915,
            0.008617790415883064,
            -0.008610823191702366,
            -0.0030200581531971693,
            0.024299798533320427,
            -0.00043737722444348037,
            0.011223330162465572,
            0.01602337695658207,
            -0.013118268921971321,
            -0.009669759310781956,
            0.007018935866653919,
            -0.016455311328172684,
            -0.008673523552715778,
            -0.03112715110182762,
            0.004406428430229425,
            -0.016566777601838112,
            -0.04166077822446823,
            -0.013571103103458881,
            0.02815934270620346,
            0.01097949594259262,
            -0.001757346442900598,
            0.013341202400624752,
            0.014121471904218197,
            5.785614484921098e-05,
            0.009293558076024055,
            0.036449696868658066,
            -0.0029138161335140467,
            -0.00964885950088501,
            -0.013647736981511116,
            -0.007997754961252213,
            0.0014725831570103765,
            -0.013731337152421474,
            -0.02312939614057541
          ],
          "start_time": 1695246509.205183,
          "end_time": 1695246509.980058,
          "error": null,
          "children": [
            {
              "name": "openai.api_resources.embedding.Embedding.create",
              "type": "LLM",
              "inputs": {
                "input": "How are sentence pairs represented in the input to BERT?",
                "engine": "embedding"
              },
              "output": {
                "object": "list",
                "data": [
                  {
                    "object": "embedding",
                    "index": 0,
                    "embedding": [
                      -0.011118830181658268,
                      -0.01631597802042961,
                      0.02559560351073742,
                      -0.002060397295281291,
                      -0.005500198341906071,
                      0.031210750341415405,
                      0.015424241311848164,
                      -0.011599531397223473,
                      -0.01773718185722828,
                      -0.027727408334612846,
                      0.028535543009638786,
                      0.04366718605160713,
                      -0.035139963030815125,
                      0.005427048075944185,
                      -0.0009474692051298916,
                      0.008422723039984703,
                      0.011244230903685093,
                      0.009614026173949242,
                      0.027797074988484383,
                      -0.016956912353634834,
                      -0.0016798420110717416,
                      -0.006102816667407751,
                      -0.016552845016121864,
                      -0.0035739096347242594,
                      -0.010742628946900368,
                      -0.00849238969385624,
                      0.029538746923208237,
                      -0.020328788086771965,
                      -0.003713243408128619,
                      -0.009676726534962654,
                      0.0021788307931274176,
                      0.02357526309788227,
                      -0.017918314784765244,
                      -0.009948426857590675,
                      -0.009258724749088287,
                      0.006694984622299671,
                      -0.0029294912237674,
                      -0.016915112733840942,
                      0.02438339963555336,
                      -0.023212995380163193,
                      0.0071164690889418125,
                      0.021694257855415344,
                      0.003977977205067873,
                      -0.015270974487066269,
                      -0.005953032989054918,
                      0.01672004535794258,
                      0.010652062483131886,
                      -0.0065695843659341335,
                      -0.012059332802891731,
                      0.0093214251101017,
                      0.00438204500824213,
                      0.015856176614761353,
                      -0.03945930674672127,
                      -0.020997589454054832,
                      -0.02251632697880268,
                      -0.0034084508661180735,
                      -0.002352998126298189,
                      0.032966356724500656,
                      -0.0272118728607893,
                      -0.013543237000703812,
                      0.0005050846957601607,
                      -0.008130121976137161,
                      0.0009370191837660968,
                      0.008053488098084927,
                      -0.022404860705137253,
                      -0.024466998875141144,
                      -0.008548122830688953,
                      0.008018654771149158,
                      0.019437052309513092,
                      0.007398620247840881,
                      0.01959032006561756,
                      0.010038994252681732,
                      0.0009866568725556135,
                      -0.001406399649567902,
                      0.0428033173084259,
                      -0.01388460397720337,
                      0.002535873558372259,
                      -0.014755439944565296,
                      0.001035423600114882,
                      0.03592022880911827,
                      0.012735101394355297,
                      -0.002624698681756854,
                      -0.0010606779251247644,
                      0.026487339287996292,
                      0.015897976234555244,
                      -0.012742067687213421,
                      0.010436095297336578,
                      0.025275135412812233,
                      0.002154447603970766,
                      0.0040302276611328125,
                      0.027379073202610016,
                      0.03536289557814598,
                      0.012944101355969906,
                      0.0067054349929094315,
                      -0.0038839271292090416,
                      0.012247432954609394,
                      -0.01118153054267168,
                      0.038818370550870895,
                      0.02634800411760807,
                      -0.009523458778858185,
                      -0.010157427750527859,
                      0.008513289503753185,
                      -0.04497692361474037,
                      -0.005552448332309723,
                      -0.024606334045529366,
                      0.014852973632514477,
                      0.010749596171081066,
                      -0.01130693033337593,
                      0.003262150567024946,
                      0.001316703506745398,
                      -0.019074784591794014,
                      0.030207548290491104,
                      0.006221250165253878,
                      -0.024216199293732643,
                      -0.0005220660241320729,
                      -0.002276364481076598,
                      0.015479975380003452,
                      -0.039319973438978195,
                      -0.009669759310781956,
                      0.012602734379470348,
                      0.017165912315249443,
                      -0.004075510893017054,
                      0.010721729137003422,
                      -0.034582626074552536,
                      0.03288275748491287,
                      -0.003422384150326252,
                      -0.010819262824952602,
                      -0.03380235657095909,
                      -0.006520817521959543,
                      -0.04288691654801369,
                      0.027420874685049057,
                      0.022641727700829506,
                      0.015939775854349136,
                      -0.022753195837140083,
                      0.005636048503220081,
                      0.01854531653225422,
                      -0.02760200761258602,
                      -0.02050992287695408,
                      -0.01959032006561756,
                      -0.004643295891582966,
                      0.02257206104695797,
                      0.003828193759545684,
                      -0.014253838919103146,
                      -0.023993264883756638,
                      -0.010192261077463627,
                      0.018601050600409508,
                      0.018782183527946472,
                      0.020412389189004898,
                      0.0024139564484357834,
                      -0.010408228263258934,
                      0.026334071531891823,
                      -0.001065902877599001,
                      -0.029761681333184242,
                      0.013807971030473709,
                      0.007726054172962904,
                      0.0011695323046296835,
                      0.00037707184674218297,
                      0.009753359481692314,
                      -0.009732459671795368,
                      -0.005601215176284313,
                      -0.020412389189004898,
                      0.017012646421790123,
                      0.017096245661377907,
                      -0.009307491593062878,
                      0.024118665605783463,
                      0.03349582478404045,
                      0.0020900056697428226,
                      -0.014866907149553299,
                      0.007328953128308058,
                      -0.029204346239566803,
                      -0.026222603395581245,
                      0.018252715468406677,
                      -0.01959032006561756,
                      -0.003438059240579605,
                      0.0040128109976649284,
                      0.00180088821798563,
                      0.03391382470726967,
                      -0.02806180901825428,
                      -0.02987314760684967,
                      -0.015619308687746525,
                      0.0193952526897192,
                      0.0065521677024662495,
                      0.020119788125157356,
                      0.023547397926449776,
                      -0.03098781779408455,
                      -0.02559560351073742,
                      -0.003594809677451849,
                      -0.01974358595907688,
                      0.01012259442359209,
                      -0.034192491322755814,
                      0.02382606454193592,
                      0.007102536037564278,
                      0.000681428937241435,
                      -0.01843385025858879,
                      -0.6148517727851868,
                      -0.03820530325174332,
                      -0.009300525300204754,
                      -0.028535543009638786,
                      0.01698477938771248,
                      0.005514131393283606,
                      -0.011627398431301117,
                      0.03349582478404045,
                      -0.024104731157422066,
                      0.033579424023628235,
                      -0.0008451460162177682,
                      -0.023296596482396126,
                      -0.0038212270010262728,
                      0.007733020931482315,
                      -0.010498794727027416,
                      -0.012992868199944496,
                      0.006503400858491659,
                      0.004162594676017761,
                      0.00922389142215252,
                      0.022335194051265717,
                      -0.03500062972307205,
                      0.00739165348932147,
                      -0.026069337502121925,
                      -0.004152144771069288,
                      -0.025372669100761414,
                      0.0031245583668351173,
                      0.006506884470582008,
                      -0.021095123142004013,
                      -0.009816059842705727,
                      0.017151979729533195,
                      -0.02045418880879879,
                      -0.001589275081641972,
                      0.021889325231313705,
                      -0.030207548290491104,
                      0.025010401383042336,
                      0.021471325308084488,
                      0.0021439974661916494,
                      0.009084558114409447,
                      -0.0007767854258418083,
                      0.027880676090717316,
                      -0.01596764288842678,
                      -0.002070847200229764,
                      -0.0024035065434873104,
                      -0.0031228167936205864,
                      0.017360979691147804,
                      -0.000540788983926177,
                      0.0186428502202034,
                      -0.010498794727027416,
                      0.006841285154223442,
                      -0.004591045901179314,
                      -0.008708356879651546,
                      -0.003521659644320607,
                      -0.007572787348181009,
                      -0.002574190264567733,
                      0.03550222888588905,
                      0.011418397538363934,
                      0.0342482253909111,
                      -0.0024975568521767855,
                      0.0014272996922954917,
                      -0.011919998563826084,
                      0.008631723001599312,
                      0.017305247485637665,
                      -0.012303167022764683,
                      -0.01918625272810459,
                      0.018823985010385513,
                      0.009565259329974651,
                      -0.006287433672696352,
                      0.003291758941486478,
                      0.00914725847542286,
                      -0.04558999091386795,
                      0.0003217737830709666,
                      0.015229174867272377,
                      0.0004941992810927331,
                      -0.015284908004105091,
                      0.00554199842736125,
                      0.012338000349700451,
                      0.0033718757331371307,
                      0.011188496835529804,
                      -0.002103938953951001,
                      0.019757520407438278,
                      -0.017444580793380737,
                      -0.012247432954609394,
                      -0.028842078521847725,
                      0.002976516494527459,
                      -0.0024052481167018414,
                      0.0001037927286233753,
                      -0.01738884672522545,
                      0.017193779349327087,
                      -0.01478330697864294,
                      -0.0036505432799458504,
                      0.0033718757331371307,
                      0.03199101984500885,
                      -0.028842078521847725,
                      -0.042050912976264954,
                      0.004918479826301336,
                      0.011376597918570042,
                      0.013125235214829445,
                      -0.0074613201431930065,
                      0.03112715110182762,
                      -0.03722996637225151,
                      0.014246871694922447,
                      -0.020565655082464218,
                      0.012351933866739273,
                      -0.005886849481612444,
                      0.02675207331776619,
                      0.03600383177399635,
                      -0.007475253660231829,
                      0.0011259905295446515,
                      -0.025707069784402847,
                      0.0035303679760545492,
                      -0.005911232437938452,
                      -0.010401261039078236,
                      -0.0035390763077884912,
                      -0.0003302644472569227,
                      -0.014212038367986679,
                      -0.04040677472949028,
                      0.011843365617096424,
                      0.01065902877599001,
                      0.01631597802042961,
                      -0.003516434459015727,
                      0.02599967084825039,
                      0.01843385025858879,
                      0.015521774999797344,
                      -0.02886994555592537,
                      0.01868464983999729,
                      0.012045399285852909,
                      0.0024522733874619007,
                      0.006210800260305405,
                      0.020844323560595512,
                      0.02886994555592537,
                      -0.011495031416416168,
                      0.02172212488949299,
                      0.010213160887360573,
                      -0.010213160887360573,
                      -0.002133547328412533,
                      0.01571684330701828,
                      0.0012836117530241609,
                      -0.007726054172962904,
                      0.01097949594259262,
                      -0.0035094679333269596,
                      -0.02130412496626377,
                      -0.008144055493175983,
                      -0.00489758001640439,
                      -0.012553967535495758,
                      -0.028591277077794075,
                      -0.015647174790501595,
                      -0.021931126713752747,
                      -0.0011024780105799437,
                      0.0001773239200701937,
                      -0.02266959473490715,
                      0.011599531397223473,
                      -0.018475649878382683,
                      -0.02528906799852848,
                      -0.017347047105431557,
                      -0.023394130170345306,
                      -0.014616106636822224,
                      -0.011655264534056187,
                      -0.032213952392339706,
                      -0.008617790415883064,
                      -0.029092878103256226,
                      0.006754201836884022,
                      0.02534480206668377,
                      -0.027588075026869774,
                      -0.017765048891305923,
                      -0.009676726534962654,
                      -0.013877637684345245,
                      0.0418279804289341,
                      0.030792750418186188,
                      0.0045875622890889645,
                      -0.016190577298402786,
                      0.013034668751060963,
                      -0.03792663663625717,
                      0.0009213441517204046,
                      0.02201472595334053,
                      0.0011190238874405622,
                      0.01848958246409893,
                      -0.02605540305376053,
                      0.009906627237796783,
                      -0.00020399325876496732,
                      -0.032576221972703934,
                      -0.009857860393822193,
                      0.040573976933956146,
                      -0.002218889305368066,
                      0.006458117626607418,
                      0.012233499437570572,
                      0.0017199005233123899,
                      0.003911793697625399,
                      -0.0036261598579585552,
                      -0.021025456488132477,
                      0.015842242166399956,
                      0.011237263679504395,
                      0.019534585997462273,
                      -0.017904382199048996,
                      0.01627417653799057,
                      -0.004322828259319067,
                      0.02237699367105961,
                      0.007948988117277622,
                      0.027615942060947418,
                      -0.014532506465911865,
                      0.018754318356513977,
                      0.025163667276501656,
                      0.009850893169641495,
                      -0.0022711395286023617,
                      -0.0011756281601265073,
                      -0.006064499728381634,
                      -0.00982302613556385,
                      -0.0011277322191745043,
                      -0.03553009778261185,
                      0.023338396102190018,
                      0.0015709876315668225,
                      0.02220979332923889,
                      -0.02906501293182373,
                      -0.033217158168554306,
                      -0.01804371550679207,
                      -0.008206755854189396,
                      0.0012374575017020106,
                      -0.009084558114409447,
                      0.03265982121229172,
                      -0.0279642753303051,
                      0.00955132581293583,
                      -0.006064499728381634,
                      -0.012909268029034138,
                      0.038706906139850616,
                      0.012254400178790092,
                      0.006893535144627094,
                      0.006043599918484688,
                      -0.0029660663567483425,
                      -0.00960009265691042,
                      0.015521774999797344,
                      -0.020342722535133362,
                      0.019228052347898483,
                      0.02076072245836258,
                      0.029956746846437454,
                      -0.0038665104657411575,
                      -0.015424241311848164,
                      -0.010533628053963184,
                      0.02630620449781418,
                      0.009356258437037468,
                      0.03279915452003479,
                      0.003523401217535138,
                      -0.012156866490840912,
                      0.0030531499069184065,
                      -0.004110344685614109,
                      -0.0040406775660812855,
                      0.053977880626916885,
                      -0.01005292683839798,
                      0.01995258778333664,
                      0.030820617452263832,
                      -0.01141143124550581,
                      -0.0074404203332960606,
                      0.017263446003198624,
                      -0.011947865597903728,
                      -0.017360979691147804,
                      0.007551887072622776,
                      0.017207713797688484,
                      0.0009274400072172284,
                      0.024801399558782578,
                      0.0003629207785706967,
                      -0.011195464059710503,
                      0.010011127218604088,
                      0.004267094656825066,
                      -0.01249823346734047,
                      0.02024518884718418,
                      0.01787651516497135,
                      0.023561330512166023,
                      -0.014643972739577293,
                      -0.030151814222335815,
                      -0.028298676013946533,
                      -0.012017532251775265,
                      0.00023577875981573015,
                      -0.017514247447252274,
                      -0.004256644751876593,
                      -0.010247994214296341,
                      0.006573067978024483,
                      0.003876960603520274,
                      0.004364628344774246,
                      -0.02363099716603756,
                      0.01733311451971531,
                      0.005022980272769928,
                      0.02181965857744217,
                      -0.006931852083653212,
                      -0.020886123180389404,
                      0.028131475672125816,
                      -0.0040580942295491695,
                      -0.011146697215735912,
                      -0.015438174828886986,
                      -0.019910788163542747,
                      -0.010617229156196117,
                      -0.028075743466615677,
                      0.017597848549485207,
                      0.008882524445652962,
                      0.0077887545339763165,
                      -0.020273054018616676,
                      -0.003695826744660735,
                      -0.002946907887235284,
                      0.0015431208303198218,
                      0.008861623704433441,
                      -0.01118153054267168,
                      0.013076468370854855,
                      0.01125816348940134,
                      -0.012045399285852909,
                      0.005364347714930773,
                      -0.016761844977736473,
                      0.014448905363678932,
                      0.049909334629774094,
                      -0.006653184536844492,
                      -0.02172212488949299,
                      -0.01985505409538746,
                      -0.018768250942230225,
                      -0.018670717254281044,
                      0.02332446351647377,
                      0.010387328453361988,
                      -0.019116584211587906,
                      0.01153683103621006,
                      0.024759599938988686,
                      0.02372853085398674,
                      -0.009425925090909004,
                      0.0016868087695911527,
                      0.011919998563826084,
                      -0.0067054349929094315,
                      -0.017402781173586845,
                      -0.03753650188446045,
                      -0.029455145820975304,
                      0.0005564640159718692,
                      0.025804603472352028,
                      0.06325750797986984,
                      0.0035094679333269596,
                      0.0036993101239204407,
                      -0.019464919343590736,
                      0.0037480767350643873,
                      -0.018015848472714424,
                      -0.01592584326863289,
                      0.0011303446954116225,
                      -0.015633242204785347,
                      -0.021485257893800735,
                      -0.00836002267897129,
                      0.0035652013029903173,
                      0.0056499820202589035,
                      0.02126232348382473,
                      0.0009013149538077414,
                      0.0013184451963752508,
                      -0.05768415704369545,
                      0.02559560351073742,
                      -0.000957048439886421,
                      0.003974494058638811,
                      -0.014337439090013504,
                      0.020816456526517868,
                      0.03461049497127533,
                      0.005099613685160875,
                      -0.01752818003296852,
                      0.01899118535220623,
                      0.02509400062263012,
                      0.018712516874074936,
                      -0.04010024294257164,
                      0.0060157328844070435,
                      0.006917918566614389,
                      -0.008798924274742603,
                      0.00041408237302675843,
                      -0.013139168731868267,
                      0.022599928081035614,
                      0.01708231307566166,
                      0.006802968215197325,
                      0.03380235657095909,
                      0.0037202101666480303,
                      -0.003915277309715748,
                      0.012066299095749855,
                      0.008645656518638134,
                      -0.006245633587241173,
                      -0.0037027932703495026,
                      0.00836002267897129,
                      -0.0035808763932436705,
                      0.018503516912460327,
                      0.02282286249101162,
                      0.0013524078531190753,
                      0.007238386198878288,
                      -0.014379238709807396,
                      -0.01783471554517746,
                      -0.004451712127774954,
                      -0.0074891867116093636,
                      -0.007384686730802059,
                      -0.011982698924839497,
                      -0.009405025281012058,
                      -0.025261200964450836,
                      -0.023254796862602234,
                      -0.003335300600156188,
                      -0.00876409001648426,
                      0.018475649878382683,
                      -0.01139053050428629,
                      0.009614026173949242,
                      -0.005716165527701378,
                      -0.003869993845000863,
                      -0.017165912315249443,
                      -0.013362103141844273,
                      -0.0016040793852880597,
                      -0.02634800411760807,
                      -0.006130683235824108,
                      -0.04840452969074249,
                      -0.020328788086771965,
                      0.023686731234192848,
                      0.024397332221269608,
                      0.029148612171411514,
                      -0.013313336297869682,
                      0.005172763951122761,
                      -0.00545143149793148,
                      -0.010059894062578678,
                      -0.008018654771149158,
                      0.016538910567760468,
                      -0.005611665081232786,
                      -0.016455311328172684,
                      0.02609720453619957,
                      -0.034136757254600525,
                      -0.026111137121915817,
                      -0.01950671896338463,
                      0.02161065861582756,
                      -0.011550764553248882,
                      0.01779291406273842,
                      -0.01468577329069376,
                      -0.022237660363316536,
                      -0.0067054349929094315,
                      0.029287945479154587,
                      -0.004845329560339451,
                      0.02746267430484295,
                      -0.0020969724282622337,
                      -0.02605540305376053,
                      0.020872190594673157,
                      -0.03722996637225151,
                      0.0004397720331326127,
                      -0.03472195938229561,
                      -0.012888368219137192,
                      -0.003988427575677633,
                      -0.0021770892199128866,
                      0.012331033125519753,
                      -0.014072705060243607,
                      -0.003828193759545684,
                      -0.0120941661298275,
                      -0.01315310224890709,
                      -0.01251913420855999,
                      -0.0035025011748075485,
                      0.046119458973407745,
                      0.013055568560957909,
                      0.004500478971749544,
                      -0.014797240495681763,
                      0.009202991612255573,
                      -0.020523855462670326,
                      -0.006771618500351906,
                      -0.015340641140937805,
                      0.02553986944258213,
                      0.00311759184114635,
                      -0.014866907149553299,
                      0.03867903724312782,
                      -0.01282566785812378,
                      -0.011926965788006783,
                      -0.015438174828886986,
                      0.035335030406713486,
                      -0.0234359297901392,
                      0.045617856085300446,
                      0.0035460428334772587,
                      0.0027309407014399767,
                      -0.017500314861536026,
                      0.008276422508060932,
                      -0.005280747544020414,
                      -0.010895895771682262,
                      -0.03148941695690155,
                      0.018614983186125755,
                      0.026361938565969467,
                      -0.012303167022764683,
                      -0.003533851122483611,
                      -0.022530261427164078,
                      -0.01612091064453125,
                      -0.020091921091079712,
                      -0.024271931499242783,
                      0.04508838802576065,
                      -0.0038978606462478638,
                      0.003025283105671406,
                      0.007168719545006752,
                      0.009363225661218166,
                      -0.0028371827211230993,
                      -0.0002782320079859346,
                      -0.037257835268974304,
                      -0.020426321774721146,
                      0.0015509583754464984,
                      0.0397937074303627,
                      0.0193952526897192,
                      0.024620266631245613,
                      0.04701119288802147,
                      -0.011947865597903728,
                      0.018127316609025,
                      0.020217321813106537,
                      -0.0008625627378933132,
                      -0.0031837753485888243,
                      -0.030346881598234177,
                      -0.008234621956944466,
                      -0.015995509922504425,
                      0.014191138558089733,
                      0.007067702244967222,
                      -0.0076424540020525455,
                      0.006956235505640507,
                      0.01985505409538746,
                      0.015006240457296371,
                      0.011077029630541801,
                      -0.003838643664494157,
                      -0.0021370307076722383,
                      0.0010876738233491778,
                      -0.019172318279743195,
                      -0.01256790105253458,
                      0.01869858428835869,
                      -0.018308449536561966,
                      0.00748222041875124,
                      -0.03380235657095909,
                      -0.004695545881986618,
                      0.0031559085473418236,
                      -0.018865784630179405,
                      0.011940899305045605,
                      -0.00720355287194252,
                      0.019074784591794014,
                      -0.019269851967692375,
                      0.010282827541232109,
                      -0.016998711973428726,
                      0.013821904547512531,
                      -0.013229736126959324,
                      -0.010888929478824139,
                      -0.023463796824216843,
                      -0.001105090486817062,
                      0.01677577942609787,
                      0.019966520369052887,
                      -0.011620431207120419,
                      0.0018583633936941624,
                      -0.030514081940054893,
                      0.015326708555221558,
                      -0.008137088268995285,
                      -0.029148612171411514,
                      -0.003768976777791977,
                      0.001748638111166656,
                      0.007879321463406086,
                      -0.016817579045891762,
                      -0.04946346580982208,
                      -0.034638360142707825,
                      -0.013362103141844273,
                      -0.009634925983846188,
                      -0.001558795920573175,
                      -0.016399577260017395,
                      0.015298841521143913,
                      -0.011042196303606033,
                      -0.04135424643754959,
                      -0.004496995359659195,
                      -0.0026560488622635603,
                      0.03023541532456875,
                      0.022335194051265717,
                      0.005200630519539118,
                      0.012985901907086372,
                      0.030904216691851616,
                      0.013389969244599342,
                      -0.011007362976670265,
                      0.00028258617385290563,
                      -0.019019052386283875,
                      0.009920559823513031,
                      0.011188496835529804,
                      -0.023867864161729813,
                      -0.0428311824798584,
                      0.0299846138805151,
                      0.027393007650971413,
                      -0.017667515203356743,
                      -0.029343679547309875,
                      0.015856176614761353,
                      -0.018782183527946472,
                      -0.001386370393447578,
                      0.00997629389166832,
                      -0.011425363831222057,
                      -0.021122990176081657,
                      -0.009035791270434856,
                      -0.012170800007879734,
                      0.014295638538897038,
                      0.01468577329069376,
                      -0.005298164207488298,
                      -0.017848648130893707,
                      0.013013768941164017,
                      -0.020119788125157356,
                      0.005218047648668289,
                      0.010881963185966015,
                      -0.010847128927707672,
                      0.024007197469472885,
                      0.01471363939344883,
                      0.022711394354701042,
                      0.014267771504819393,
                      0.03304995596408844,
                      0.026334071531891823,
                      -0.030709149315953255,
                      0.018378116190433502,
                      -0.013452669605612755,
                      -0.003040958195924759,
                      -0.007224452681839466,
                      0.005360864568501711,
                      0.004469128791242838,
                      0.0304862167686224,
                      -0.022711394354701042,
                      -0.00927265826612711,
                      0.004099894315004349,
                      -0.009767292998731136,
                      0.017653580754995346,
                      0.008729256689548492,
                      -0.00025994444149546325,
                      0.004911513067781925,
                      -0.011522897519171238,
                      0.007928088307380676,
                      0.034638360142707825,
                      0.006123716477304697,
                      -0.02176392637193203,
                      -0.0007519666105508804,
                      -0.0016493628500029445,
                      0.0037445933558046818,
                      -0.02690533921122551,
                      0.009453792124986649,
                      0.029510879889130592,
                      -0.0418279804289341,
                      -0.00914029125124216,
                      0.027518408372998238,
                      0.011334797367453575,
                      0.011049163527786732,
                      -0.029148612171411514,
                      0.011090963147580624,
                      0.013167035765945911,
                      0.021025456488132477,
                      -0.013410869985818863,
                      -0.010206194594502449,
                      -0.007503120228648186,
                      0.015326708555221558,
                      -0.03006821498274803,
                      -0.017165912315249443,
                      -0.019924720749258995,
                      -0.011857299134135246,
                      0.0158701092004776,
                      0.021039390936493874,
                      0.007565820589661598,
                      -0.007872354239225388,
                      0.010540595278143883,
                      -0.029343679547309875,
                      0.010241027921438217,
                      0.009753359481692314,
                      -0.004260128363966942,
                      -0.00318725872784853,
                      0.014769373461604118,
                      -0.0038665104657411575,
                      0.010450027883052826,
                      0.0176257137209177,
                      0.020133720710873604,
                      -0.0065626176074147224,
                      0.01000415999442339,
                      -0.030597683042287827,
                      -0.015897976234555244,
                      -0.007955954410135746,
                      0.004124277736991644,
                      -0.00038055519689805806,
                      -0.02800607495009899,
                      0.006447667255997658,
                      0.004775662906467915,
                      0.00568133220076561,
                      -0.0022397893480956554,
                      -0.015354574657976627,
                      -0.005329514387995005,
                      0.002267656149342656,
                      -0.011676165275275707,
                      0.00816495530307293,
                      0.016566777601838112,
                      -0.023310529068112373,
                      0.037007033824920654,
                      0.0067054349929094315,
                      0.04951919987797737,
                      0.0018670717254281044,
                      -0.01606517657637596,
                      -0.03491702675819397,
                      -0.026125071570277214,
                      -0.017556047067046165,
                      -0.026877472177147865,
                      -0.03304995596408844,
                      -0.025484135374426842,
                      0.02790854312479496,
                      0.019562453031539917,
                      -0.027128273621201515,
                      -0.03260408714413643,
                      -0.018196983262896538,
                      -0.019659986719489098,
                      0.00997629389166832,
                      0.01646924391388893,
                      0.01779291406273842,
                      0.010575428605079651,
                      0.01617664471268654,
                      -0.02841014415025711,
                      0.03714636713266373,
                      0.010087761096656322,
                      -0.0006901372689753771,
                      -0.013550203293561935,
                      0.012581833638250828,
                      0.011397497728466988,
                      -0.01612091064453125,
                      0.004803529474884272,
                      0.02473173290491104,
                      -0.04751279577612877,
                      -0.0007872354472056031,
                      0.03784303739666939,
                      -0.006451150868088007,
                      0.02403506450355053,
                      0.03918064013123512,
                      -0.019715720787644386,
                      -0.03480556234717369,
                      -0.01430957205593586,
                      0.018322382122278214,
                      0.033022090792655945,
                      0.01692904531955719,
                      0.003636609762907028,
                      -0.0019855054561048746,
                      -0.01342480257153511,
                      0.0052772643975913525,
                      0.013717403635382652,
                      -0.026724206283688545,
                      0.0027675158344209194,
                      0.0054792980663478374,
                      0.0037724601570516825,
                      -0.02397933229804039,
                      -0.017221646383404732,
                      0.011676165275275707,
                      0.012059332802891731,
                      -0.014978373423218727,
                      0.020523855462670326,
                      -0.004420361947268248,
                      -0.02967808023095131,
                      0.0049393801018595695,
                      0.02679387293756008,
                      0.0015971126267686486,
                      -0.020900055766105652,
                      0.0110839968547225,
                      -0.027936408296227455,
                      0.0032029335852712393,
                      0.02161065861582756,
                      0.012958034873008728,
                      -0.0052842311561107635,
                      -0.002953874645754695,
                      0.005691782105714083,
                      -0.02573493681848049,
                      -0.00224153115414083,
                      -0.007691220846027136,
                      0.0058694323524832726,
                      -0.018461717292666435,
                      0.04625879228115082,
                      0.02917647920548916,
                      -0.05105186998844147,
                      0.017458513379096985,
                      -0.013780103996396065,
                      -0.008833757601678371,
                      -0.019032984972000122,
                      0.0032691170927137136,
                      0.005033430177718401,
                      -0.021136924624443054,
                      0.02101152390241623,
                      0.0037480767350643873,
                      -0.03505636006593704,
                      -0.0019611220341175795,
                      -0.0012052366510033607,
                      0.008680489845573902,
                      -0.01047789491713047,
                      0.02574886940419674,
                      0.2039845585823059,
                      -0.03405315801501274,
                      0.01833631657063961,
                      0.0029573580250144005,
                      0.013780103996396065,
                      0.0176257137209177,
                      0.0037550434935837984,
                      0.017709314823150635,
                      -0.00217186426743865,
                      -0.001844429993070662,
                      0.00048462007544003427,
                      0.008060455322265625,
                      -0.013222768902778625,
                      0.0024365982972085476,
                      0.0224187932908535,
                      0.0010824487544596195,
                      -0.012066299095749855,
                      -0.046175193041563034,
                      -0.00687611848115921,
                      -0.0056778485886752605,
                      -0.005225013941526413,
                      0.014699706807732582,
                      -0.003213383723050356,
                      -0.036505430936813354,
                      0.014351372607052326,
                      -0.012916235253214836,
                      -0.006691501475870609,
                      -0.026919273659586906,
                      0.006419800687581301,
                      0.03472195938229561,
                      -0.007377719972282648,
                      0.01858711615204811,
                      -0.008241589181125164,
                      0.015479975380003452,
                      -0.009690659120678902,
                      -0.020384522154927254,
                      -0.02272532880306244,
                      0.02222372591495514,
                      0.011056129820644855,
                      -0.009425925090909004,
                      0.035084228962659836,
                      0.010401261039078236,
                      -0.008025621995329857,
                      -0.012331033125519753,
                      -0.00031154145835898817,
                      -0.00023947982117533684,
                      0.000322644627885893,
                      -0.013814937323331833,
                      -0.002095230622217059,
                      0.03023541532456875,
                      -0.010512728244066238,
                      0.02654307149350643,
                      0.039124906063079834,
                      0.042691849172115326,
                      0.0030461831483989954,
                      -0.014267771504819393,
                      0.002694365568459034,
                      0.00595651613548398,
                      0.005886849481612444,
                      0.01949278637766838,
                      -0.025762803852558136,
                      0.0186428502202034,
                      0.002546323463320732,
                      0.03112715110182762,
                      0.03179595246911049,
                      0.015856176614761353,
                      -0.019882921129465103,
                      0.012393733486533165,
                      0.015730775892734528,
                      -0.004723412916064262,
                      -0.015382441692054272,
                      -0.016817579045891762,
                      -0.005925165954977274,
                      -0.004653745796531439,
                      -0.03502849489450455,
                      -0.02126232348382473,
                      0.051414139568805695,
                      0.01818304881453514,
                      0.009565259329974651,
                      0.034081026911735535,
                      -0.018754318356513977,
                      -0.005834599025547504,
                      0.011369630694389343,
                      -0.014894773252308369,
                      -0.026779938489198685,
                      -0.043054115027189255,
                      0.017305247485637665,
                      -0.01395427156239748,
                      -0.017946181818842888,
                      -0.016483178362250328,
                      0.0023791231215000153,
                      -0.021081190556287766,
                      -0.008959157392382622,
                      0.008617790415883064,
                      -0.0004689450142905116,
                      -0.010735662654042244,
                      -0.019172318279743195,
                      0.03940357267856598,
                      -0.0014569080667570233,
                      -0.021596726030111313,
                      -0.03452689200639725,
                      0.04703906178474426,
                      0.03842823579907417,
                      0.02644553780555725,
                      -0.0008155376417562366,
                      0.004946346860378981,
                      -0.01019922737032175,
                      -0.008157989010214806,
                      0.022502394393086433,
                      -0.0013027702225372195,
                      0.001785213127732277,
                      -0.012386767193675041,
                      0.0019053885480389,
                      0.007997754961252213,
                      -0.0018931968370452523,
                      -0.011327831074595451,
                      0.005221530795097351,
                      -0.014393172226846218,
                      0.012637567706406116,
                      -0.005221530795097351,
                      0.0005899911629967391,
                      -0.03388595953583717,
                      -0.039821576327085495,
                      0.017946181818842888,
                      -0.0014830331783741713,
                      -0.004685095977038145,
                      -0.016413511708378792,
                      -0.02619473822414875,
                      -0.01430957205593586,
                      -0.007405586540699005,
                      0.011327831074595451,
                      -0.03265982121229172,
                      0.03530716150999069,
                      -0.021582791581749916,
                      -0.02906501293182373,
                      0.00651036761701107,
                      0.01950671896338463,
                      -0.017542114481329918,
                      -0.01779291406273842,
                      0.009941460564732552,
                      0.009460759349167347,
                      -5.8073856052942574e-05,
                      0.006141133140772581,
                      -0.006984102074056864,
                      0.01627417653799057,
                      -0.0468439944088459,
                      0.0076215541921556,
                      -0.014434972777962685,
                      -0.0014351372374221683,
                      -0.03625463321805,
                      -0.01478330697864294,
                      -0.007649420760571957,
                      0.0015466042095795274,
                      -0.019311651587486267,
                      0.023463796824216843,
                      -0.002380864694714546,
                      -0.018670717254281044,
                      -0.015159507282078266,
                      -0.013264569453895092,
                      0.0021109057124704123,
                      -0.028786344453692436,
                      -0.015591442584991455,
                      -0.0022641727700829506,
                      -0.025219401344656944,
                      -0.009077590890228748,
                      -0.009579192847013474,
                      -0.17745541036128998,
                      0.010847128927707672,
                      0.02076072245836258,
                      -0.032074619084596634,
                      -0.005552448332309723,
                      0.02711433917284012,
                      0.032269686460494995,
                      -0.009850893169641495,
                      -0.020482055842876434,
                      0.003523401217535138,
                      -0.0010632904013618827,
                      -0.017012646421790123,
                      -0.02856341004371643,
                      -0.0007136498461477458,
                      0.004350695293396711,
                      0.01306950207799673,
                      0.0018235299503430724,
                      0.02045418880879879,
                      0.024898933246731758,
                      0.02825687639415264,
                      0.03143368661403656,
                      -0.044614654034376144,
                      0.013536269776523113,
                      -0.007321986369788647,
                      0.02064925618469715,
                      -0.009767292998731136,
                      -0.010171361267566681,
                      0.03137795254588127,
                      -0.017151979729533195,
                      -0.014671839773654938,
                      0.012128999456763268,
                      0.02725367434322834,
                      0.012101132422685623,
                      -0.004552728962153196,
                      0.00025515485322102904,
                      0.020886123180389404,
                      0.01123029738664627,
                      -0.01334816962480545,
                      -0.0011651781387627125,
                      -0.008415755815804005,
                      0.05252880975604057,
                      -0.00992752704769373,
                      -0.0076145874336361885,
                      -0.014121471904218197,
                      -0.023644929751753807,
                      -0.011097930371761322,
                      0.007670320570468903,
                      -0.0022746229078620672,
                      0.016009442508220673,
                      0.018364183604717255,
                      -0.01748638041317463,
                      -0.020384522154927254,
                      0.020635321736335754,
                      0.006398900877684355,
                      0.007005002349615097,
                      0.02679387293756008,
                      0.011662231758236885,
                      0.00821372214704752,
                      -0.026166871190071106,
                      0.015535708516836166,
                      0.004977697040885687,
                      -0.033022090792655945,
                      0.026013603433966637,
                      0.002319906372576952,
                      -0.014978373423218727,
                      -0.0016868087695911527,
                      0.0018479133723303676,
                      0.004333278629928827,
                      -0.021986858919262886,
                      0.011202430352568626,
                      -0.03647756576538086,
                      -0.023463796824216843,
                      0.0003302644472569227,
                      0.0047408295795321465,
                      0.022599928081035614,
                      -0.00012757742661051452,
                      -0.019367385655641556,
                      -0.0012609701370820403,
                      0.001722512999549508,
                      -0.01123029738664627,
                      -0.033022090792655945,
                      0.0317402184009552,
                      -0.0096976263448596,
                      0.026975005865097046,
                      0.0032081587705761194,
                      -0.012087199836969376,
                      0.005521098151803017,
                      0.018322382122278214,
                      -0.019715720787644386,
                      -0.026431605219841003,
                      0.011432331055402756,
                      0.0060226996429264545,
                      0.017542114481329918,
                      0.0019785386975854635,
                      0.007412553299218416,
                      0.01779291406273842,
                      0.010220128111541271,
                      -0.015284908004105091,
                      0.022181926295161247,
                      -0.012853534892201424,
                      -0.02105332352221012,
                      -0.017347047105431557,
                      0.006381483748555183,
                      0.009307491593062878,
                      0.012783868238329887,
                      0.02533086948096752,
                      -0.019618187099695206,
                      0.006694984622299671,
                      0.029037145897746086,
                      0.011481097899377346,
                      -0.006043599918484688,
                      -0.010833196341991425,
                      0.00023664960463065654,
                      0.02629227191209793,
                      0.017820781096816063,
                      0.009133324958384037,
                      -0.03193528577685356,
                      -0.0027918992564082146,
                      0.005256364122033119,
                      -0.02599967084825039,
                      0.043499983847141266,
                      -0.010603295639157295,
                      0.003612226340919733,
                      0.014978373423218727,
                      -0.0007920250645838678,
                      -0.01804371550679207,
                      -0.11425364017486572,
                      -0.008206755854189396,
                      -0.002555032027885318,
                      0.004354178439825773,
                      -0.014490705914795399,
                      -0.004120794590562582,
                      0.02172212488949299,
                      0.029483012855052948,
                      -0.026027537882328033,
                      0.010624195449054241,
                      -0.020579589530825615,
                      -0.02711433917284012,
                      0.013090401887893677,
                      0.004803529474884272,
                      -0.005319064483046532,
                      0.008185855112969875,
                      0.0033701341599226,
                      -0.0047582462430000305,
                      -0.009251758456230164,
                      0.026682404801249504,
                      -0.0026473405305296183,
                      -0.01959032006561756,
                      0.014393172226846218,
                      -0.024048998951911926,
                      0.015842242166399956,
                      0.00677858479321003,
                      -0.029705947265028954,
                      0.0015788250602781773,
                      0.021889325231313705,
                      0.011028262786567211,
                      0.007398620247840881,
                      -0.03455476090312004,
                      -0.00881285686045885,
                      -0.003410192672163248,
                      -0.0019837636500597,
                      -0.003326592268422246,
                      -0.012909268029034138,
                      -0.022558128461241722,
                      0.005782349035143852,
                      -0.03405315801501274,
                      0.0029051078017801046,
                      0.013292435556650162,
                      -0.004169561434537172,
                      -0.010157427750527859,
                      -0.010136527009308338,
                      -0.00012942795001436025,
                      -0.014434972777962685,
                      0.030151814222335815,
                      0.023770330473780632,
                      -0.00803258828818798,
                      -0.012198666110634804,
                      -0.02045418880879879,
                      -0.02499646693468094,
                      -0.005367831327021122,
                      0.020941857248544693,
                      0.033133555203676224,
                      0.0186428502202034,
                      0.013598970137536526,
                      -0.004517895635217428,
                      -0.012965002097189426,
                      -0.000530774355866015,
                      -0.005092646926641464,
                      -0.01698477938771248,
                      0.005144897382706404,
                      0.018628917634487152,
                      -0.001567504252307117,
                      -0.05473028123378754,
                      -0.011167597025632858,
                      -0.008673523552715778,
                      -0.005350414663553238,
                      -0.014156305231153965,
                      0.01793224923312664,
                      0.013194902800023556,
                      0.02155492454767227,
                      -0.01858711615204811,
                      -0.01012259442359209,
                      -0.014281705021858215,
                      -0.0021091641392558813,
                      0.0022066975943744183,
                      -0.018726451322436333,
                      -0.01090286299586296,
                      -0.0274487417191267,
                      0.007698187604546547,
                      -0.002624698681756854,
                      -0.010220128111541271,
                      -0.012804768048226833,
                      -0.0012182991486042738,
                      -0.02499646693468094,
                      -0.011745831929147243,
                      -0.029761681333184242,
                      0.009662793017923832,
                      0.008645656518638134,
                      -0.029761681333184242,
                      -0.008234621956944466,
                      -0.004180011339485645,
                      -0.0024644650984555483,
                      -0.0031994504388421774,
                      -0.01390550471842289,
                      0.020217321813106537,
                      0.005987866315990686,
                      -0.015075907111167908,
                      0.0043750787153840065,
                      -0.04391798377037048,
                      0.016162710264325142,
                      0.0032952423207461834,
                      -0.01698477938771248,
                      0.011655264534056187,
                      0.018823985010385513,
                      0.03692343458533287,
                      -0.017040513455867767,
                      0.002717007417231798,
                      0.008854657411575317,
                      -0.007363786455243826,
                      0.021485257893800735,
                      -0.01738884672522545,
                      -0.018977250903844833,
                      -0.006447667255997658,
                      -0.010993429459631443,
                      0.005925165954977274,
                      -0.004507445730268955,
                      0.029204346239566803,
                      0.005803248845040798,
                      -0.01499230694025755,
                      -0.016538910567760468,
                      0.009572225622832775,
                      -0.00025798508431762457,
                      -0.007774821016937494,
                      0.013828870840370655,
                      -0.010234060697257519,
                      0.026320137083530426,
                      -0.026612738147377968,
                      -0.011780665256083012,
                      0.020077988505363464,
                      -0.02403506450355053,
                      -0.0008969607879407704,
                      0.030040347948670387,
                      -0.0010589362354949117,
                      0.016664311289787292,
                      0.02423013187944889,
                      0.008596889674663544,
                      0.03622676432132721,
                      0.004639812745153904,
                      -0.014755439944565296,
                      -0.03600383177399635,
                      0.0020307889208197594,
                      -0.012540034018456936,
                      -0.00260728201828897,
                      0.022251592949032784,
                      -0.0027222323697060347,
                      0.003500759368762374,
                      0.015479975380003452,
                      -0.006398900877684355,
                      0.008778023533523083,
                      0.02091399021446705,
                      0.019074784591794014,
                      -0.029009278863668442,
                      -0.015953710302710533,
                      -0.00849238969385624,
                      -0.0035286261700093746,
                      0.02278106100857258,
                      0.002276364481076598,
                      -0.0034415426198393106,
                      0.0367841012775898,
                      -0.015048041008412838,
                      0.022279459983110428,
                      0.003981460817158222,
                      0.0076424540020525455,
                      -0.008750157430768013,
                      0.012428566813468933,
                      0.007039835676550865,
                      -0.02912074513733387,
                      -0.02827080897986889,
                      -0.010568462312221527,
                      0.009251758456230164,
                      -0.011773698963224888,
                      0.02565133571624756,
                      0.029037145897746086,
                      0.005252880975604057,
                      -0.0007088602287694812,
                      0.0114601980894804,
                      -0.005998316220939159,
                      0.01478330697864294,
                      0.012484300881624222,
                      -0.02459239959716797,
                      -0.016845446079969406,
                      -0.014448905363678932,
                      0.024453066289424896,
                      0.002779707545414567,
                      -0.004451712127774954,
                      0.012170800007879734,
                      -0.005378281231969595,
                      0.02195899188518524,
                      0.012184732593595982,
                      0.001861846656538546,
                      0.009760326705873013,
                      0.03555796295404434,
                      0.00401629414409399,
                      0.01490870676934719,
                      0.007349853403866291,
                      -0.019269851967692375,
                      0.006796001922339201,
                      -0.021749991923570633,
                      0.006127200089395046,
                      0.015828309580683708,
                      -0.012463400140404701,
                      -0.010094727389514446,
                      -0.023157263174653053,
                      0.021136924624443054,
                      -0.013229736126959324,
                      -0.00457362923771143,
                      0.003953594248741865,
                      0.03770370036363602,
                      -0.0011442780960351229,
                      0.015298841521143913,
                      -0.01219169981777668,
                      0.01080532930791378,
                      -0.034276094287633896,
                      0.02765774168074131,
                      -0.007210519630461931,
                      -0.036505430936813354,
                      -0.026570938527584076,
                      0.033022090792655945,
                      0.021931126713752747,
                      -0.01518737431615591,
                      0.044558919966220856,
                      -0.012212599627673626,
                      0.018559250980615616,
                      0.010819262824952602,
                      0.024717800319194794,
                      -0.011906065978109837,
                      0.02638980560004711,
                      0.02278106100857258,
                      0.017597848549485207,
                      -0.008680489845573902,
                      0.01989685371518135,
                      -0.01874038390815258,
                      -0.021136924624443054,
                      -0.012310133315622807,
                      -0.006607901304960251,
                      0.03279915452003479,
                      -0.0008817211491987109,
                      0.10160214453935623,
                      0.021248390898108482,
                      -0.0030949499923735857,
                      0.012115065939724445,
                      -0.015702908858656883,
                      0.011871231719851494,
                      0.013362103141844273,
                      0.018921518698334694,
                      -0.011474130675196648,
                      -0.0010545820696279407,
                      0.006339683663100004,
                      -0.015591442584991455,
                      -0.013689536601305008,
                      -0.02423013187944889,
                      0.0035355929285287857,
                      -0.015605375170707703,
                      0.003765493631362915,
                      0.008617790415883064,
                      -0.008610823191702366,
                      -0.0030200581531971693,
                      0.024299798533320427,
                      -0.00043737722444348037,
                      0.011223330162465572,
                      0.01602337695658207,
                      -0.013118268921971321,
                      -0.009669759310781956,
                      0.007018935866653919,
                      -0.016455311328172684,
                      -0.008673523552715778,
                      -0.03112715110182762,
                      0.004406428430229425,
                      -0.016566777601838112,
                      -0.04166077822446823,
                      -0.013571103103458881,
                      0.02815934270620346,
                      0.01097949594259262,
                      -0.001757346442900598,
                      0.013341202400624752,
                      0.014121471904218197,
                      5.785614484921098e-05,
                      0.009293558076024055,
                      0.036449696868658066,
                      -0.0029138161335140467,
                      -0.00964885950088501,
                      -0.013647736981511116,
                      -0.007997754961252213,
                      0.0014725831570103765,
                      -0.013731337152421474,
                      -0.02312939614057541
                    ]
                  }
                ],
                "model": "ada",
                "usage": {
                  "prompt_tokens": 12,
                  "total_tokens": 12
                }
              },
              "start_time": 1695246509.207655,
              "end_time": 1695246509.967509,
              "error": null,
              "children": null,
              "node_name": null
            }
          ],
          "node_name": "embed_the_question"
        },
        {
          "name": "checkCacheAnswer",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "embeddedQuestion": [
              -0.011118830181658268,
              -0.01631597802042961,
              0.02559560351073742,
              -0.002060397295281291,
              -0.005500198341906071,
              0.031210750341415405,
              0.015424241311848164,
              -0.011599531397223473,
              -0.01773718185722828,
              -0.027727408334612846,
              0.028535543009638786,
              0.04366718605160713,
              -0.035139963030815125,
              0.005427048075944185,
              -0.0009474692051298916,
              0.008422723039984703,
              0.011244230903685093,
              0.009614026173949242,
              0.027797074988484383,
              -0.016956912353634834,
              -0.0016798420110717416,
              -0.006102816667407751,
              -0.016552845016121864,
              -0.0035739096347242594,
              -0.010742628946900368,
              -0.00849238969385624,
              0.029538746923208237,
              -0.020328788086771965,
              -0.003713243408128619,
              -0.009676726534962654,
              0.0021788307931274176,
              0.02357526309788227,
              -0.017918314784765244,
              -0.009948426857590675,
              -0.009258724749088287,
              0.006694984622299671,
              -0.0029294912237674,
              -0.016915112733840942,
              0.02438339963555336,
              -0.023212995380163193,
              0.0071164690889418125,
              0.021694257855415344,
              0.003977977205067873,
              -0.015270974487066269,
              -0.005953032989054918,
              0.01672004535794258,
              0.010652062483131886,
              -0.0065695843659341335,
              -0.012059332802891731,
              0.0093214251101017,
              0.00438204500824213,
              0.015856176614761353,
              -0.03945930674672127,
              -0.020997589454054832,
              -0.02251632697880268,
              -0.0034084508661180735,
              -0.002352998126298189,
              0.032966356724500656,
              -0.0272118728607893,
              -0.013543237000703812,
              0.0005050846957601607,
              -0.008130121976137161,
              0.0009370191837660968,
              0.008053488098084927,
              -0.022404860705137253,
              -0.024466998875141144,
              -0.008548122830688953,
              0.008018654771149158,
              0.019437052309513092,
              0.007398620247840881,
              0.01959032006561756,
              0.010038994252681732,
              0.0009866568725556135,
              -0.001406399649567902,
              0.0428033173084259,
              -0.01388460397720337,
              0.002535873558372259,
              -0.014755439944565296,
              0.001035423600114882,
              0.03592022880911827,
              0.012735101394355297,
              -0.002624698681756854,
              -0.0010606779251247644,
              0.026487339287996292,
              0.015897976234555244,
              -0.012742067687213421,
              0.010436095297336578,
              0.025275135412812233,
              0.002154447603970766,
              0.0040302276611328125,
              0.027379073202610016,
              0.03536289557814598,
              0.012944101355969906,
              0.0067054349929094315,
              -0.0038839271292090416,
              0.012247432954609394,
              -0.01118153054267168,
              0.038818370550870895,
              0.02634800411760807,
              -0.009523458778858185,
              -0.010157427750527859,
              0.008513289503753185,
              -0.04497692361474037,
              -0.005552448332309723,
              -0.024606334045529366,
              0.014852973632514477,
              0.010749596171081066,
              -0.01130693033337593,
              0.003262150567024946,
              0.001316703506745398,
              -0.019074784591794014,
              0.030207548290491104,
              0.006221250165253878,
              -0.024216199293732643,
              -0.0005220660241320729,
              -0.002276364481076598,
              0.015479975380003452,
              -0.039319973438978195,
              -0.009669759310781956,
              0.012602734379470348,
              0.017165912315249443,
              -0.004075510893017054,
              0.010721729137003422,
              -0.034582626074552536,
              0.03288275748491287,
              -0.003422384150326252,
              -0.010819262824952602,
              -0.03380235657095909,
              -0.006520817521959543,
              -0.04288691654801369,
              0.027420874685049057,
              0.022641727700829506,
              0.015939775854349136,
              -0.022753195837140083,
              0.005636048503220081,
              0.01854531653225422,
              -0.02760200761258602,
              -0.02050992287695408,
              -0.01959032006561756,
              -0.004643295891582966,
              0.02257206104695797,
              0.003828193759545684,
              -0.014253838919103146,
              -0.023993264883756638,
              -0.010192261077463627,
              0.018601050600409508,
              0.018782183527946472,
              0.020412389189004898,
              0.0024139564484357834,
              -0.010408228263258934,
              0.026334071531891823,
              -0.001065902877599001,
              -0.029761681333184242,
              0.013807971030473709,
              0.007726054172962904,
              0.0011695323046296835,
              0.00037707184674218297,
              0.009753359481692314,
              -0.009732459671795368,
              -0.005601215176284313,
              -0.020412389189004898,
              0.017012646421790123,
              0.017096245661377907,
              -0.009307491593062878,
              0.024118665605783463,
              0.03349582478404045,
              0.0020900056697428226,
              -0.014866907149553299,
              0.007328953128308058,
              -0.029204346239566803,
              -0.026222603395581245,
              0.018252715468406677,
              -0.01959032006561756,
              -0.003438059240579605,
              0.0040128109976649284,
              0.00180088821798563,
              0.03391382470726967,
              -0.02806180901825428,
              -0.02987314760684967,
              -0.015619308687746525,
              0.0193952526897192,
              0.0065521677024662495,
              0.020119788125157356,
              0.023547397926449776,
              -0.03098781779408455,
              -0.02559560351073742,
              -0.003594809677451849,
              -0.01974358595907688,
              0.01012259442359209,
              -0.034192491322755814,
              0.02382606454193592,
              0.007102536037564278,
              0.000681428937241435,
              -0.01843385025858879,
              -0.6148517727851868,
              -0.03820530325174332,
              -0.009300525300204754,
              -0.028535543009638786,
              0.01698477938771248,
              0.005514131393283606,
              -0.011627398431301117,
              0.03349582478404045,
              -0.024104731157422066,
              0.033579424023628235,
              -0.0008451460162177682,
              -0.023296596482396126,
              -0.0038212270010262728,
              0.007733020931482315,
              -0.010498794727027416,
              -0.012992868199944496,
              0.006503400858491659,
              0.004162594676017761,
              0.00922389142215252,
              0.022335194051265717,
              -0.03500062972307205,
              0.00739165348932147,
              -0.026069337502121925,
              -0.004152144771069288,
              -0.025372669100761414,
              0.0031245583668351173,
              0.006506884470582008,
              -0.021095123142004013,
              -0.009816059842705727,
              0.017151979729533195,
              -0.02045418880879879,
              -0.001589275081641972,
              0.021889325231313705,
              -0.030207548290491104,
              0.025010401383042336,
              0.021471325308084488,
              0.0021439974661916494,
              0.009084558114409447,
              -0.0007767854258418083,
              0.027880676090717316,
              -0.01596764288842678,
              -0.002070847200229764,
              -0.0024035065434873104,
              -0.0031228167936205864,
              0.017360979691147804,
              -0.000540788983926177,
              0.0186428502202034,
              -0.010498794727027416,
              0.006841285154223442,
              -0.004591045901179314,
              -0.008708356879651546,
              -0.003521659644320607,
              -0.007572787348181009,
              -0.002574190264567733,
              0.03550222888588905,
              0.011418397538363934,
              0.0342482253909111,
              -0.0024975568521767855,
              0.0014272996922954917,
              -0.011919998563826084,
              0.008631723001599312,
              0.017305247485637665,
              -0.012303167022764683,
              -0.01918625272810459,
              0.018823985010385513,
              0.009565259329974651,
              -0.006287433672696352,
              0.003291758941486478,
              0.00914725847542286,
              -0.04558999091386795,
              0.0003217737830709666,
              0.015229174867272377,
              0.0004941992810927331,
              -0.015284908004105091,
              0.00554199842736125,
              0.012338000349700451,
              0.0033718757331371307,
              0.011188496835529804,
              -0.002103938953951001,
              0.019757520407438278,
              -0.017444580793380737,
              -0.012247432954609394,
              -0.028842078521847725,
              0.002976516494527459,
              -0.0024052481167018414,
              0.0001037927286233753,
              -0.01738884672522545,
              0.017193779349327087,
              -0.01478330697864294,
              -0.0036505432799458504,
              0.0033718757331371307,
              0.03199101984500885,
              -0.028842078521847725,
              -0.042050912976264954,
              0.004918479826301336,
              0.011376597918570042,
              0.013125235214829445,
              -0.0074613201431930065,
              0.03112715110182762,
              -0.03722996637225151,
              0.014246871694922447,
              -0.020565655082464218,
              0.012351933866739273,
              -0.005886849481612444,
              0.02675207331776619,
              0.03600383177399635,
              -0.007475253660231829,
              0.0011259905295446515,
              -0.025707069784402847,
              0.0035303679760545492,
              -0.005911232437938452,
              -0.010401261039078236,
              -0.0035390763077884912,
              -0.0003302644472569227,
              -0.014212038367986679,
              -0.04040677472949028,
              0.011843365617096424,
              0.01065902877599001,
              0.01631597802042961,
              -0.003516434459015727,
              0.02599967084825039,
              0.01843385025858879,
              0.015521774999797344,
              -0.02886994555592537,
              0.01868464983999729,
              0.012045399285852909,
              0.0024522733874619007,
              0.006210800260305405,
              0.020844323560595512,
              0.02886994555592537,
              -0.011495031416416168,
              0.02172212488949299,
              0.010213160887360573,
              -0.010213160887360573,
              -0.002133547328412533,
              0.01571684330701828,
              0.0012836117530241609,
              -0.007726054172962904,
              0.01097949594259262,
              -0.0035094679333269596,
              -0.02130412496626377,
              -0.008144055493175983,
              -0.00489758001640439,
              -0.012553967535495758,
              -0.028591277077794075,
              -0.015647174790501595,
              -0.021931126713752747,
              -0.0011024780105799437,
              0.0001773239200701937,
              -0.02266959473490715,
              0.011599531397223473,
              -0.018475649878382683,
              -0.02528906799852848,
              -0.017347047105431557,
              -0.023394130170345306,
              -0.014616106636822224,
              -0.011655264534056187,
              -0.032213952392339706,
              -0.008617790415883064,
              -0.029092878103256226,
              0.006754201836884022,
              0.02534480206668377,
              -0.027588075026869774,
              -0.017765048891305923,
              -0.009676726534962654,
              -0.013877637684345245,
              0.0418279804289341,
              0.030792750418186188,
              0.0045875622890889645,
              -0.016190577298402786,
              0.013034668751060963,
              -0.03792663663625717,
              0.0009213441517204046,
              0.02201472595334053,
              0.0011190238874405622,
              0.01848958246409893,
              -0.02605540305376053,
              0.009906627237796783,
              -0.00020399325876496732,
              -0.032576221972703934,
              -0.009857860393822193,
              0.040573976933956146,
              -0.002218889305368066,
              0.006458117626607418,
              0.012233499437570572,
              0.0017199005233123899,
              0.003911793697625399,
              -0.0036261598579585552,
              -0.021025456488132477,
              0.015842242166399956,
              0.011237263679504395,
              0.019534585997462273,
              -0.017904382199048996,
              0.01627417653799057,
              -0.004322828259319067,
              0.02237699367105961,
              0.007948988117277622,
              0.027615942060947418,
              -0.014532506465911865,
              0.018754318356513977,
              0.025163667276501656,
              0.009850893169641495,
              -0.0022711395286023617,
              -0.0011756281601265073,
              -0.006064499728381634,
              -0.00982302613556385,
              -0.0011277322191745043,
              -0.03553009778261185,
              0.023338396102190018,
              0.0015709876315668225,
              0.02220979332923889,
              -0.02906501293182373,
              -0.033217158168554306,
              -0.01804371550679207,
              -0.008206755854189396,
              0.0012374575017020106,
              -0.009084558114409447,
              0.03265982121229172,
              -0.0279642753303051,
              0.00955132581293583,
              -0.006064499728381634,
              -0.012909268029034138,
              0.038706906139850616,
              0.012254400178790092,
              0.006893535144627094,
              0.006043599918484688,
              -0.0029660663567483425,
              -0.00960009265691042,
              0.015521774999797344,
              -0.020342722535133362,
              0.019228052347898483,
              0.02076072245836258,
              0.029956746846437454,
              -0.0038665104657411575,
              -0.015424241311848164,
              -0.010533628053963184,
              0.02630620449781418,
              0.009356258437037468,
              0.03279915452003479,
              0.003523401217535138,
              -0.012156866490840912,
              0.0030531499069184065,
              -0.004110344685614109,
              -0.0040406775660812855,
              0.053977880626916885,
              -0.01005292683839798,
              0.01995258778333664,
              0.030820617452263832,
              -0.01141143124550581,
              -0.0074404203332960606,
              0.017263446003198624,
              -0.011947865597903728,
              -0.017360979691147804,
              0.007551887072622776,
              0.017207713797688484,
              0.0009274400072172284,
              0.024801399558782578,
              0.0003629207785706967,
              -0.011195464059710503,
              0.010011127218604088,
              0.004267094656825066,
              -0.01249823346734047,
              0.02024518884718418,
              0.01787651516497135,
              0.023561330512166023,
              -0.014643972739577293,
              -0.030151814222335815,
              -0.028298676013946533,
              -0.012017532251775265,
              0.00023577875981573015,
              -0.017514247447252274,
              -0.004256644751876593,
              -0.010247994214296341,
              0.006573067978024483,
              0.003876960603520274,
              0.004364628344774246,
              -0.02363099716603756,
              0.01733311451971531,
              0.005022980272769928,
              0.02181965857744217,
              -0.006931852083653212,
              -0.020886123180389404,
              0.028131475672125816,
              -0.0040580942295491695,
              -0.011146697215735912,
              -0.015438174828886986,
              -0.019910788163542747,
              -0.010617229156196117,
              -0.028075743466615677,
              0.017597848549485207,
              0.008882524445652962,
              0.0077887545339763165,
              -0.020273054018616676,
              -0.003695826744660735,
              -0.002946907887235284,
              0.0015431208303198218,
              0.008861623704433441,
              -0.01118153054267168,
              0.013076468370854855,
              0.01125816348940134,
              -0.012045399285852909,
              0.005364347714930773,
              -0.016761844977736473,
              0.014448905363678932,
              0.049909334629774094,
              -0.006653184536844492,
              -0.02172212488949299,
              -0.01985505409538746,
              -0.018768250942230225,
              -0.018670717254281044,
              0.02332446351647377,
              0.010387328453361988,
              -0.019116584211587906,
              0.01153683103621006,
              0.024759599938988686,
              0.02372853085398674,
              -0.009425925090909004,
              0.0016868087695911527,
              0.011919998563826084,
              -0.0067054349929094315,
              -0.017402781173586845,
              -0.03753650188446045,
              -0.029455145820975304,
              0.0005564640159718692,
              0.025804603472352028,
              0.06325750797986984,
              0.0035094679333269596,
              0.0036993101239204407,
              -0.019464919343590736,
              0.0037480767350643873,
              -0.018015848472714424,
              -0.01592584326863289,
              0.0011303446954116225,
              -0.015633242204785347,
              -0.021485257893800735,
              -0.00836002267897129,
              0.0035652013029903173,
              0.0056499820202589035,
              0.02126232348382473,
              0.0009013149538077414,
              0.0013184451963752508,
              -0.05768415704369545,
              0.02559560351073742,
              -0.000957048439886421,
              0.003974494058638811,
              -0.014337439090013504,
              0.020816456526517868,
              0.03461049497127533,
              0.005099613685160875,
              -0.01752818003296852,
              0.01899118535220623,
              0.02509400062263012,
              0.018712516874074936,
              -0.04010024294257164,
              0.0060157328844070435,
              0.006917918566614389,
              -0.008798924274742603,
              0.00041408237302675843,
              -0.013139168731868267,
              0.022599928081035614,
              0.01708231307566166,
              0.006802968215197325,
              0.03380235657095909,
              0.0037202101666480303,
              -0.003915277309715748,
              0.012066299095749855,
              0.008645656518638134,
              -0.006245633587241173,
              -0.0037027932703495026,
              0.00836002267897129,
              -0.0035808763932436705,
              0.018503516912460327,
              0.02282286249101162,
              0.0013524078531190753,
              0.007238386198878288,
              -0.014379238709807396,
              -0.01783471554517746,
              -0.004451712127774954,
              -0.0074891867116093636,
              -0.007384686730802059,
              -0.011982698924839497,
              -0.009405025281012058,
              -0.025261200964450836,
              -0.023254796862602234,
              -0.003335300600156188,
              -0.00876409001648426,
              0.018475649878382683,
              -0.01139053050428629,
              0.009614026173949242,
              -0.005716165527701378,
              -0.003869993845000863,
              -0.017165912315249443,
              -0.013362103141844273,
              -0.0016040793852880597,
              -0.02634800411760807,
              -0.006130683235824108,
              -0.04840452969074249,
              -0.020328788086771965,
              0.023686731234192848,
              0.024397332221269608,
              0.029148612171411514,
              -0.013313336297869682,
              0.005172763951122761,
              -0.00545143149793148,
              -0.010059894062578678,
              -0.008018654771149158,
              0.016538910567760468,
              -0.005611665081232786,
              -0.016455311328172684,
              0.02609720453619957,
              -0.034136757254600525,
              -0.026111137121915817,
              -0.01950671896338463,
              0.02161065861582756,
              -0.011550764553248882,
              0.01779291406273842,
              -0.01468577329069376,
              -0.022237660363316536,
              -0.0067054349929094315,
              0.029287945479154587,
              -0.004845329560339451,
              0.02746267430484295,
              -0.0020969724282622337,
              -0.02605540305376053,
              0.020872190594673157,
              -0.03722996637225151,
              0.0004397720331326127,
              -0.03472195938229561,
              -0.012888368219137192,
              -0.003988427575677633,
              -0.0021770892199128866,
              0.012331033125519753,
              -0.014072705060243607,
              -0.003828193759545684,
              -0.0120941661298275,
              -0.01315310224890709,
              -0.01251913420855999,
              -0.0035025011748075485,
              0.046119458973407745,
              0.013055568560957909,
              0.004500478971749544,
              -0.014797240495681763,
              0.009202991612255573,
              -0.020523855462670326,
              -0.006771618500351906,
              -0.015340641140937805,
              0.02553986944258213,
              0.00311759184114635,
              -0.014866907149553299,
              0.03867903724312782,
              -0.01282566785812378,
              -0.011926965788006783,
              -0.015438174828886986,
              0.035335030406713486,
              -0.0234359297901392,
              0.045617856085300446,
              0.0035460428334772587,
              0.0027309407014399767,
              -0.017500314861536026,
              0.008276422508060932,
              -0.005280747544020414,
              -0.010895895771682262,
              -0.03148941695690155,
              0.018614983186125755,
              0.026361938565969467,
              -0.012303167022764683,
              -0.003533851122483611,
              -0.022530261427164078,
              -0.01612091064453125,
              -0.020091921091079712,
              -0.024271931499242783,
              0.04508838802576065,
              -0.0038978606462478638,
              0.003025283105671406,
              0.007168719545006752,
              0.009363225661218166,
              -0.0028371827211230993,
              -0.0002782320079859346,
              -0.037257835268974304,
              -0.020426321774721146,
              0.0015509583754464984,
              0.0397937074303627,
              0.0193952526897192,
              0.024620266631245613,
              0.04701119288802147,
              -0.011947865597903728,
              0.018127316609025,
              0.020217321813106537,
              -0.0008625627378933132,
              -0.0031837753485888243,
              -0.030346881598234177,
              -0.008234621956944466,
              -0.015995509922504425,
              0.014191138558089733,
              0.007067702244967222,
              -0.0076424540020525455,
              0.006956235505640507,
              0.01985505409538746,
              0.015006240457296371,
              0.011077029630541801,
              -0.003838643664494157,
              -0.0021370307076722383,
              0.0010876738233491778,
              -0.019172318279743195,
              -0.01256790105253458,
              0.01869858428835869,
              -0.018308449536561966,
              0.00748222041875124,
              -0.03380235657095909,
              -0.004695545881986618,
              0.0031559085473418236,
              -0.018865784630179405,
              0.011940899305045605,
              -0.00720355287194252,
              0.019074784591794014,
              -0.019269851967692375,
              0.010282827541232109,
              -0.016998711973428726,
              0.013821904547512531,
              -0.013229736126959324,
              -0.010888929478824139,
              -0.023463796824216843,
              -0.001105090486817062,
              0.01677577942609787,
              0.019966520369052887,
              -0.011620431207120419,
              0.0018583633936941624,
              -0.030514081940054893,
              0.015326708555221558,
              -0.008137088268995285,
              -0.029148612171411514,
              -0.003768976777791977,
              0.001748638111166656,
              0.007879321463406086,
              -0.016817579045891762,
              -0.04946346580982208,
              -0.034638360142707825,
              -0.013362103141844273,
              -0.009634925983846188,
              -0.001558795920573175,
              -0.016399577260017395,
              0.015298841521143913,
              -0.011042196303606033,
              -0.04135424643754959,
              -0.004496995359659195,
              -0.0026560488622635603,
              0.03023541532456875,
              0.022335194051265717,
              0.005200630519539118,
              0.012985901907086372,
              0.030904216691851616,
              0.013389969244599342,
              -0.011007362976670265,
              0.00028258617385290563,
              -0.019019052386283875,
              0.009920559823513031,
              0.011188496835529804,
              -0.023867864161729813,
              -0.0428311824798584,
              0.0299846138805151,
              0.027393007650971413,
              -0.017667515203356743,
              -0.029343679547309875,
              0.015856176614761353,
              -0.018782183527946472,
              -0.001386370393447578,
              0.00997629389166832,
              -0.011425363831222057,
              -0.021122990176081657,
              -0.009035791270434856,
              -0.012170800007879734,
              0.014295638538897038,
              0.01468577329069376,
              -0.005298164207488298,
              -0.017848648130893707,
              0.013013768941164017,
              -0.020119788125157356,
              0.005218047648668289,
              0.010881963185966015,
              -0.010847128927707672,
              0.024007197469472885,
              0.01471363939344883,
              0.022711394354701042,
              0.014267771504819393,
              0.03304995596408844,
              0.026334071531891823,
              -0.030709149315953255,
              0.018378116190433502,
              -0.013452669605612755,
              -0.003040958195924759,
              -0.007224452681839466,
              0.005360864568501711,
              0.004469128791242838,
              0.0304862167686224,
              -0.022711394354701042,
              -0.00927265826612711,
              0.004099894315004349,
              -0.009767292998731136,
              0.017653580754995346,
              0.008729256689548492,
              -0.00025994444149546325,
              0.004911513067781925,
              -0.011522897519171238,
              0.007928088307380676,
              0.034638360142707825,
              0.006123716477304697,
              -0.02176392637193203,
              -0.0007519666105508804,
              -0.0016493628500029445,
              0.0037445933558046818,
              -0.02690533921122551,
              0.009453792124986649,
              0.029510879889130592,
              -0.0418279804289341,
              -0.00914029125124216,
              0.027518408372998238,
              0.011334797367453575,
              0.011049163527786732,
              -0.029148612171411514,
              0.011090963147580624,
              0.013167035765945911,
              0.021025456488132477,
              -0.013410869985818863,
              -0.010206194594502449,
              -0.007503120228648186,
              0.015326708555221558,
              -0.03006821498274803,
              -0.017165912315249443,
              -0.019924720749258995,
              -0.011857299134135246,
              0.0158701092004776,
              0.021039390936493874,
              0.007565820589661598,
              -0.007872354239225388,
              0.010540595278143883,
              -0.029343679547309875,
              0.010241027921438217,
              0.009753359481692314,
              -0.004260128363966942,
              -0.00318725872784853,
              0.014769373461604118,
              -0.0038665104657411575,
              0.010450027883052826,
              0.0176257137209177,
              0.020133720710873604,
              -0.0065626176074147224,
              0.01000415999442339,
              -0.030597683042287827,
              -0.015897976234555244,
              -0.007955954410135746,
              0.004124277736991644,
              -0.00038055519689805806,
              -0.02800607495009899,
              0.006447667255997658,
              0.004775662906467915,
              0.00568133220076561,
              -0.0022397893480956554,
              -0.015354574657976627,
              -0.005329514387995005,
              0.002267656149342656,
              -0.011676165275275707,
              0.00816495530307293,
              0.016566777601838112,
              -0.023310529068112373,
              0.037007033824920654,
              0.0067054349929094315,
              0.04951919987797737,
              0.0018670717254281044,
              -0.01606517657637596,
              -0.03491702675819397,
              -0.026125071570277214,
              -0.017556047067046165,
              -0.026877472177147865,
              -0.03304995596408844,
              -0.025484135374426842,
              0.02790854312479496,
              0.019562453031539917,
              -0.027128273621201515,
              -0.03260408714413643,
              -0.018196983262896538,
              -0.019659986719489098,
              0.00997629389166832,
              0.01646924391388893,
              0.01779291406273842,
              0.010575428605079651,
              0.01617664471268654,
              -0.02841014415025711,
              0.03714636713266373,
              0.010087761096656322,
              -0.0006901372689753771,
              -0.013550203293561935,
              0.012581833638250828,
              0.011397497728466988,
              -0.01612091064453125,
              0.004803529474884272,
              0.02473173290491104,
              -0.04751279577612877,
              -0.0007872354472056031,
              0.03784303739666939,
              -0.006451150868088007,
              0.02403506450355053,
              0.03918064013123512,
              -0.019715720787644386,
              -0.03480556234717369,
              -0.01430957205593586,
              0.018322382122278214,
              0.033022090792655945,
              0.01692904531955719,
              0.003636609762907028,
              -0.0019855054561048746,
              -0.01342480257153511,
              0.0052772643975913525,
              0.013717403635382652,
              -0.026724206283688545,
              0.0027675158344209194,
              0.0054792980663478374,
              0.0037724601570516825,
              -0.02397933229804039,
              -0.017221646383404732,
              0.011676165275275707,
              0.012059332802891731,
              -0.014978373423218727,
              0.020523855462670326,
              -0.004420361947268248,
              -0.02967808023095131,
              0.0049393801018595695,
              0.02679387293756008,
              0.0015971126267686486,
              -0.020900055766105652,
              0.0110839968547225,
              -0.027936408296227455,
              0.0032029335852712393,
              0.02161065861582756,
              0.012958034873008728,
              -0.0052842311561107635,
              -0.002953874645754695,
              0.005691782105714083,
              -0.02573493681848049,
              -0.00224153115414083,
              -0.007691220846027136,
              0.0058694323524832726,
              -0.018461717292666435,
              0.04625879228115082,
              0.02917647920548916,
              -0.05105186998844147,
              0.017458513379096985,
              -0.013780103996396065,
              -0.008833757601678371,
              -0.019032984972000122,
              0.0032691170927137136,
              0.005033430177718401,
              -0.021136924624443054,
              0.02101152390241623,
              0.0037480767350643873,
              -0.03505636006593704,
              -0.0019611220341175795,
              -0.0012052366510033607,
              0.008680489845573902,
              -0.01047789491713047,
              0.02574886940419674,
              0.2039845585823059,
              -0.03405315801501274,
              0.01833631657063961,
              0.0029573580250144005,
              0.013780103996396065,
              0.0176257137209177,
              0.0037550434935837984,
              0.017709314823150635,
              -0.00217186426743865,
              -0.001844429993070662,
              0.00048462007544003427,
              0.008060455322265625,
              -0.013222768902778625,
              0.0024365982972085476,
              0.0224187932908535,
              0.0010824487544596195,
              -0.012066299095749855,
              -0.046175193041563034,
              -0.00687611848115921,
              -0.0056778485886752605,
              -0.005225013941526413,
              0.014699706807732582,
              -0.003213383723050356,
              -0.036505430936813354,
              0.014351372607052326,
              -0.012916235253214836,
              -0.006691501475870609,
              -0.026919273659586906,
              0.006419800687581301,
              0.03472195938229561,
              -0.007377719972282648,
              0.01858711615204811,
              -0.008241589181125164,
              0.015479975380003452,
              -0.009690659120678902,
              -0.020384522154927254,
              -0.02272532880306244,
              0.02222372591495514,
              0.011056129820644855,
              -0.009425925090909004,
              0.035084228962659836,
              0.010401261039078236,
              -0.008025621995329857,
              -0.012331033125519753,
              -0.00031154145835898817,
              -0.00023947982117533684,
              0.000322644627885893,
              -0.013814937323331833,
              -0.002095230622217059,
              0.03023541532456875,
              -0.010512728244066238,
              0.02654307149350643,
              0.039124906063079834,
              0.042691849172115326,
              0.0030461831483989954,
              -0.014267771504819393,
              0.002694365568459034,
              0.00595651613548398,
              0.005886849481612444,
              0.01949278637766838,
              -0.025762803852558136,
              0.0186428502202034,
              0.002546323463320732,
              0.03112715110182762,
              0.03179595246911049,
              0.015856176614761353,
              -0.019882921129465103,
              0.012393733486533165,
              0.015730775892734528,
              -0.004723412916064262,
              -0.015382441692054272,
              -0.016817579045891762,
              -0.005925165954977274,
              -0.004653745796531439,
              -0.03502849489450455,
              -0.02126232348382473,
              0.051414139568805695,
              0.01818304881453514,
              0.009565259329974651,
              0.034081026911735535,
              -0.018754318356513977,
              -0.005834599025547504,
              0.011369630694389343,
              -0.014894773252308369,
              -0.026779938489198685,
              -0.043054115027189255,
              0.017305247485637665,
              -0.01395427156239748,
              -0.017946181818842888,
              -0.016483178362250328,
              0.0023791231215000153,
              -0.021081190556287766,
              -0.008959157392382622,
              0.008617790415883064,
              -0.0004689450142905116,
              -0.010735662654042244,
              -0.019172318279743195,
              0.03940357267856598,
              -0.0014569080667570233,
              -0.021596726030111313,
              -0.03452689200639725,
              0.04703906178474426,
              0.03842823579907417,
              0.02644553780555725,
              -0.0008155376417562366,
              0.004946346860378981,
              -0.01019922737032175,
              -0.008157989010214806,
              0.022502394393086433,
              -0.0013027702225372195,
              0.001785213127732277,
              -0.012386767193675041,
              0.0019053885480389,
              0.007997754961252213,
              -0.0018931968370452523,
              -0.011327831074595451,
              0.005221530795097351,
              -0.014393172226846218,
              0.012637567706406116,
              -0.005221530795097351,
              0.0005899911629967391,
              -0.03388595953583717,
              -0.039821576327085495,
              0.017946181818842888,
              -0.0014830331783741713,
              -0.004685095977038145,
              -0.016413511708378792,
              -0.02619473822414875,
              -0.01430957205593586,
              -0.007405586540699005,
              0.011327831074595451,
              -0.03265982121229172,
              0.03530716150999069,
              -0.021582791581749916,
              -0.02906501293182373,
              0.00651036761701107,
              0.01950671896338463,
              -0.017542114481329918,
              -0.01779291406273842,
              0.009941460564732552,
              0.009460759349167347,
              -5.8073856052942574e-05,
              0.006141133140772581,
              -0.006984102074056864,
              0.01627417653799057,
              -0.0468439944088459,
              0.0076215541921556,
              -0.014434972777962685,
              -0.0014351372374221683,
              -0.03625463321805,
              -0.01478330697864294,
              -0.007649420760571957,
              0.0015466042095795274,
              -0.019311651587486267,
              0.023463796824216843,
              -0.002380864694714546,
              -0.018670717254281044,
              -0.015159507282078266,
              -0.013264569453895092,
              0.0021109057124704123,
              -0.028786344453692436,
              -0.015591442584991455,
              -0.0022641727700829506,
              -0.025219401344656944,
              -0.009077590890228748,
              -0.009579192847013474,
              -0.17745541036128998,
              0.010847128927707672,
              0.02076072245836258,
              -0.032074619084596634,
              -0.005552448332309723,
              0.02711433917284012,
              0.032269686460494995,
              -0.009850893169641495,
              -0.020482055842876434,
              0.003523401217535138,
              -0.0010632904013618827,
              -0.017012646421790123,
              -0.02856341004371643,
              -0.0007136498461477458,
              0.004350695293396711,
              0.01306950207799673,
              0.0018235299503430724,
              0.02045418880879879,
              0.024898933246731758,
              0.02825687639415264,
              0.03143368661403656,
              -0.044614654034376144,
              0.013536269776523113,
              -0.007321986369788647,
              0.02064925618469715,
              -0.009767292998731136,
              -0.010171361267566681,
              0.03137795254588127,
              -0.017151979729533195,
              -0.014671839773654938,
              0.012128999456763268,
              0.02725367434322834,
              0.012101132422685623,
              -0.004552728962153196,
              0.00025515485322102904,
              0.020886123180389404,
              0.01123029738664627,
              -0.01334816962480545,
              -0.0011651781387627125,
              -0.008415755815804005,
              0.05252880975604057,
              -0.00992752704769373,
              -0.0076145874336361885,
              -0.014121471904218197,
              -0.023644929751753807,
              -0.011097930371761322,
              0.007670320570468903,
              -0.0022746229078620672,
              0.016009442508220673,
              0.018364183604717255,
              -0.01748638041317463,
              -0.020384522154927254,
              0.020635321736335754,
              0.006398900877684355,
              0.007005002349615097,
              0.02679387293756008,
              0.011662231758236885,
              0.00821372214704752,
              -0.026166871190071106,
              0.015535708516836166,
              0.004977697040885687,
              -0.033022090792655945,
              0.026013603433966637,
              0.002319906372576952,
              -0.014978373423218727,
              -0.0016868087695911527,
              0.0018479133723303676,
              0.004333278629928827,
              -0.021986858919262886,
              0.011202430352568626,
              -0.03647756576538086,
              -0.023463796824216843,
              0.0003302644472569227,
              0.0047408295795321465,
              0.022599928081035614,
              -0.00012757742661051452,
              -0.019367385655641556,
              -0.0012609701370820403,
              0.001722512999549508,
              -0.01123029738664627,
              -0.033022090792655945,
              0.0317402184009552,
              -0.0096976263448596,
              0.026975005865097046,
              0.0032081587705761194,
              -0.012087199836969376,
              0.005521098151803017,
              0.018322382122278214,
              -0.019715720787644386,
              -0.026431605219841003,
              0.011432331055402756,
              0.0060226996429264545,
              0.017542114481329918,
              0.0019785386975854635,
              0.007412553299218416,
              0.01779291406273842,
              0.010220128111541271,
              -0.015284908004105091,
              0.022181926295161247,
              -0.012853534892201424,
              -0.02105332352221012,
              -0.017347047105431557,
              0.006381483748555183,
              0.009307491593062878,
              0.012783868238329887,
              0.02533086948096752,
              -0.019618187099695206,
              0.006694984622299671,
              0.029037145897746086,
              0.011481097899377346,
              -0.006043599918484688,
              -0.010833196341991425,
              0.00023664960463065654,
              0.02629227191209793,
              0.017820781096816063,
              0.009133324958384037,
              -0.03193528577685356,
              -0.0027918992564082146,
              0.005256364122033119,
              -0.02599967084825039,
              0.043499983847141266,
              -0.010603295639157295,
              0.003612226340919733,
              0.014978373423218727,
              -0.0007920250645838678,
              -0.01804371550679207,
              -0.11425364017486572,
              -0.008206755854189396,
              -0.002555032027885318,
              0.004354178439825773,
              -0.014490705914795399,
              -0.004120794590562582,
              0.02172212488949299,
              0.029483012855052948,
              -0.026027537882328033,
              0.010624195449054241,
              -0.020579589530825615,
              -0.02711433917284012,
              0.013090401887893677,
              0.004803529474884272,
              -0.005319064483046532,
              0.008185855112969875,
              0.0033701341599226,
              -0.0047582462430000305,
              -0.009251758456230164,
              0.026682404801249504,
              -0.0026473405305296183,
              -0.01959032006561756,
              0.014393172226846218,
              -0.024048998951911926,
              0.015842242166399956,
              0.00677858479321003,
              -0.029705947265028954,
              0.0015788250602781773,
              0.021889325231313705,
              0.011028262786567211,
              0.007398620247840881,
              -0.03455476090312004,
              -0.00881285686045885,
              -0.003410192672163248,
              -0.0019837636500597,
              -0.003326592268422246,
              -0.012909268029034138,
              -0.022558128461241722,
              0.005782349035143852,
              -0.03405315801501274,
              0.0029051078017801046,
              0.013292435556650162,
              -0.004169561434537172,
              -0.010157427750527859,
              -0.010136527009308338,
              -0.00012942795001436025,
              -0.014434972777962685,
              0.030151814222335815,
              0.023770330473780632,
              -0.00803258828818798,
              -0.012198666110634804,
              -0.02045418880879879,
              -0.02499646693468094,
              -0.005367831327021122,
              0.020941857248544693,
              0.033133555203676224,
              0.0186428502202034,
              0.013598970137536526,
              -0.004517895635217428,
              -0.012965002097189426,
              -0.000530774355866015,
              -0.005092646926641464,
              -0.01698477938771248,
              0.005144897382706404,
              0.018628917634487152,
              -0.001567504252307117,
              -0.05473028123378754,
              -0.011167597025632858,
              -0.008673523552715778,
              -0.005350414663553238,
              -0.014156305231153965,
              0.01793224923312664,
              0.013194902800023556,
              0.02155492454767227,
              -0.01858711615204811,
              -0.01012259442359209,
              -0.014281705021858215,
              -0.0021091641392558813,
              0.0022066975943744183,
              -0.018726451322436333,
              -0.01090286299586296,
              -0.0274487417191267,
              0.007698187604546547,
              -0.002624698681756854,
              -0.010220128111541271,
              -0.012804768048226833,
              -0.0012182991486042738,
              -0.02499646693468094,
              -0.011745831929147243,
              -0.029761681333184242,
              0.009662793017923832,
              0.008645656518638134,
              -0.029761681333184242,
              -0.008234621956944466,
              -0.004180011339485645,
              -0.0024644650984555483,
              -0.0031994504388421774,
              -0.01390550471842289,
              0.020217321813106537,
              0.005987866315990686,
              -0.015075907111167908,
              0.0043750787153840065,
              -0.04391798377037048,
              0.016162710264325142,
              0.0032952423207461834,
              -0.01698477938771248,
              0.011655264534056187,
              0.018823985010385513,
              0.03692343458533287,
              -0.017040513455867767,
              0.002717007417231798,
              0.008854657411575317,
              -0.007363786455243826,
              0.021485257893800735,
              -0.01738884672522545,
              -0.018977250903844833,
              -0.006447667255997658,
              -0.010993429459631443,
              0.005925165954977274,
              -0.004507445730268955,
              0.029204346239566803,
              0.005803248845040798,
              -0.01499230694025755,
              -0.016538910567760468,
              0.009572225622832775,
              -0.00025798508431762457,
              -0.007774821016937494,
              0.013828870840370655,
              -0.010234060697257519,
              0.026320137083530426,
              -0.026612738147377968,
              -0.011780665256083012,
              0.020077988505363464,
              -0.02403506450355053,
              -0.0008969607879407704,
              0.030040347948670387,
              -0.0010589362354949117,
              0.016664311289787292,
              0.02423013187944889,
              0.008596889674663544,
              0.03622676432132721,
              0.004639812745153904,
              -0.014755439944565296,
              -0.03600383177399635,
              0.0020307889208197594,
              -0.012540034018456936,
              -0.00260728201828897,
              0.022251592949032784,
              -0.0027222323697060347,
              0.003500759368762374,
              0.015479975380003452,
              -0.006398900877684355,
              0.008778023533523083,
              0.02091399021446705,
              0.019074784591794014,
              -0.029009278863668442,
              -0.015953710302710533,
              -0.00849238969385624,
              -0.0035286261700093746,
              0.02278106100857258,
              0.002276364481076598,
              -0.0034415426198393106,
              0.0367841012775898,
              -0.015048041008412838,
              0.022279459983110428,
              0.003981460817158222,
              0.0076424540020525455,
              -0.008750157430768013,
              0.012428566813468933,
              0.007039835676550865,
              -0.02912074513733387,
              -0.02827080897986889,
              -0.010568462312221527,
              0.009251758456230164,
              -0.011773698963224888,
              0.02565133571624756,
              0.029037145897746086,
              0.005252880975604057,
              -0.0007088602287694812,
              0.0114601980894804,
              -0.005998316220939159,
              0.01478330697864294,
              0.012484300881624222,
              -0.02459239959716797,
              -0.016845446079969406,
              -0.014448905363678932,
              0.024453066289424896,
              0.002779707545414567,
              -0.004451712127774954,
              0.012170800007879734,
              -0.005378281231969595,
              0.02195899188518524,
              0.012184732593595982,
              0.001861846656538546,
              0.009760326705873013,
              0.03555796295404434,
              0.00401629414409399,
              0.01490870676934719,
              0.007349853403866291,
              -0.019269851967692375,
              0.006796001922339201,
              -0.021749991923570633,
              0.006127200089395046,
              0.015828309580683708,
              -0.012463400140404701,
              -0.010094727389514446,
              -0.023157263174653053,
              0.021136924624443054,
              -0.013229736126959324,
              -0.00457362923771143,
              0.003953594248741865,
              0.03770370036363602,
              -0.0011442780960351229,
              0.015298841521143913,
              -0.01219169981777668,
              0.01080532930791378,
              -0.034276094287633896,
              0.02765774168074131,
              -0.007210519630461931,
              -0.036505430936813354,
              -0.026570938527584076,
              0.033022090792655945,
              0.021931126713752747,
              -0.01518737431615591,
              0.044558919966220856,
              -0.012212599627673626,
              0.018559250980615616,
              0.010819262824952602,
              0.024717800319194794,
              -0.011906065978109837,
              0.02638980560004711,
              0.02278106100857258,
              0.017597848549485207,
              -0.008680489845573902,
              0.01989685371518135,
              -0.01874038390815258,
              -0.021136924624443054,
              -0.012310133315622807,
              -0.006607901304960251,
              0.03279915452003479,
              -0.0008817211491987109,
              0.10160214453935623,
              0.021248390898108482,
              -0.0030949499923735857,
              0.012115065939724445,
              -0.015702908858656883,
              0.011871231719851494,
              0.013362103141844273,
              0.018921518698334694,
              -0.011474130675196648,
              -0.0010545820696279407,
              0.006339683663100004,
              -0.015591442584991455,
              -0.013689536601305008,
              -0.02423013187944889,
              0.0035355929285287857,
              -0.015605375170707703,
              0.003765493631362915,
              0.008617790415883064,
              -0.008610823191702366,
              -0.0030200581531971693,
              0.024299798533320427,
              -0.00043737722444348037,
              0.011223330162465572,
              0.01602337695658207,
              -0.013118268921971321,
              -0.009669759310781956,
              0.007018935866653919,
              -0.016455311328172684,
              -0.008673523552715778,
              -0.03112715110182762,
              0.004406428430229425,
              -0.016566777601838112,
              -0.04166077822446823,
              -0.013571103103458881,
              0.02815934270620346,
              0.01097949594259262,
              -0.001757346442900598,
              0.013341202400624752,
              0.014121471904218197,
              5.785614484921098e-05,
              0.009293558076024055,
              0.036449696868658066,
              -0.0029138161335140467,
              -0.00964885950088501,
              -0.013647736981511116,
              -0.007997754961252213,
              0.0014725831570103765,
              -0.013731337152421474,
              -0.02312939614057541
            ],
            "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
            "indexType": "cogsearchvs",
            "question": "How are sentence pairs represented in the input to BERT?"
          },
          "output": {
            "jsonAnswer": null,
            "existingAnswer": false
          },
          "start_time": 1695246510.003197,
          "end_time": 1695246512.094216,
          "error": null,
          "children": null,
          "node_name": "check_cache_answer"
        },
        {
          "name": "searchVectorDb",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "embeddedQuestion": [
              -0.011118830181658268,
              -0.01631597802042961,
              0.02559560351073742,
              -0.002060397295281291,
              -0.005500198341906071,
              0.031210750341415405,
              0.015424241311848164,
              -0.011599531397223473,
              -0.01773718185722828,
              -0.027727408334612846,
              0.028535543009638786,
              0.04366718605160713,
              -0.035139963030815125,
              0.005427048075944185,
              -0.0009474692051298916,
              0.008422723039984703,
              0.011244230903685093,
              0.009614026173949242,
              0.027797074988484383,
              -0.016956912353634834,
              -0.0016798420110717416,
              -0.006102816667407751,
              -0.016552845016121864,
              -0.0035739096347242594,
              -0.010742628946900368,
              -0.00849238969385624,
              0.029538746923208237,
              -0.020328788086771965,
              -0.003713243408128619,
              -0.009676726534962654,
              0.0021788307931274176,
              0.02357526309788227,
              -0.017918314784765244,
              -0.009948426857590675,
              -0.009258724749088287,
              0.006694984622299671,
              -0.0029294912237674,
              -0.016915112733840942,
              0.02438339963555336,
              -0.023212995380163193,
              0.0071164690889418125,
              0.021694257855415344,
              0.003977977205067873,
              -0.015270974487066269,
              -0.005953032989054918,
              0.01672004535794258,
              0.010652062483131886,
              -0.0065695843659341335,
              -0.012059332802891731,
              0.0093214251101017,
              0.00438204500824213,
              0.015856176614761353,
              -0.03945930674672127,
              -0.020997589454054832,
              -0.02251632697880268,
              -0.0034084508661180735,
              -0.002352998126298189,
              0.032966356724500656,
              -0.0272118728607893,
              -0.013543237000703812,
              0.0005050846957601607,
              -0.008130121976137161,
              0.0009370191837660968,
              0.008053488098084927,
              -0.022404860705137253,
              -0.024466998875141144,
              -0.008548122830688953,
              0.008018654771149158,
              0.019437052309513092,
              0.007398620247840881,
              0.01959032006561756,
              0.010038994252681732,
              0.0009866568725556135,
              -0.001406399649567902,
              0.0428033173084259,
              -0.01388460397720337,
              0.002535873558372259,
              -0.014755439944565296,
              0.001035423600114882,
              0.03592022880911827,
              0.012735101394355297,
              -0.002624698681756854,
              -0.0010606779251247644,
              0.026487339287996292,
              0.015897976234555244,
              -0.012742067687213421,
              0.010436095297336578,
              0.025275135412812233,
              0.002154447603970766,
              0.0040302276611328125,
              0.027379073202610016,
              0.03536289557814598,
              0.012944101355969906,
              0.0067054349929094315,
              -0.0038839271292090416,
              0.012247432954609394,
              -0.01118153054267168,
              0.038818370550870895,
              0.02634800411760807,
              -0.009523458778858185,
              -0.010157427750527859,
              0.008513289503753185,
              -0.04497692361474037,
              -0.005552448332309723,
              -0.024606334045529366,
              0.014852973632514477,
              0.010749596171081066,
              -0.01130693033337593,
              0.003262150567024946,
              0.001316703506745398,
              -0.019074784591794014,
              0.030207548290491104,
              0.006221250165253878,
              -0.024216199293732643,
              -0.0005220660241320729,
              -0.002276364481076598,
              0.015479975380003452,
              -0.039319973438978195,
              -0.009669759310781956,
              0.012602734379470348,
              0.017165912315249443,
              -0.004075510893017054,
              0.010721729137003422,
              -0.034582626074552536,
              0.03288275748491287,
              -0.003422384150326252,
              -0.010819262824952602,
              -0.03380235657095909,
              -0.006520817521959543,
              -0.04288691654801369,
              0.027420874685049057,
              0.022641727700829506,
              0.015939775854349136,
              -0.022753195837140083,
              0.005636048503220081,
              0.01854531653225422,
              -0.02760200761258602,
              -0.02050992287695408,
              -0.01959032006561756,
              -0.004643295891582966,
              0.02257206104695797,
              0.003828193759545684,
              -0.014253838919103146,
              -0.023993264883756638,
              -0.010192261077463627,
              0.018601050600409508,
              0.018782183527946472,
              0.020412389189004898,
              0.0024139564484357834,
              -0.010408228263258934,
              0.026334071531891823,
              -0.001065902877599001,
              -0.029761681333184242,
              0.013807971030473709,
              0.007726054172962904,
              0.0011695323046296835,
              0.00037707184674218297,
              0.009753359481692314,
              -0.009732459671795368,
              -0.005601215176284313,
              -0.020412389189004898,
              0.017012646421790123,
              0.017096245661377907,
              -0.009307491593062878,
              0.024118665605783463,
              0.03349582478404045,
              0.0020900056697428226,
              -0.014866907149553299,
              0.007328953128308058,
              -0.029204346239566803,
              -0.026222603395581245,
              0.018252715468406677,
              -0.01959032006561756,
              -0.003438059240579605,
              0.0040128109976649284,
              0.00180088821798563,
              0.03391382470726967,
              -0.02806180901825428,
              -0.02987314760684967,
              -0.015619308687746525,
              0.0193952526897192,
              0.0065521677024662495,
              0.020119788125157356,
              0.023547397926449776,
              -0.03098781779408455,
              -0.02559560351073742,
              -0.003594809677451849,
              -0.01974358595907688,
              0.01012259442359209,
              -0.034192491322755814,
              0.02382606454193592,
              0.007102536037564278,
              0.000681428937241435,
              -0.01843385025858879,
              -0.6148517727851868,
              -0.03820530325174332,
              -0.009300525300204754,
              -0.028535543009638786,
              0.01698477938771248,
              0.005514131393283606,
              -0.011627398431301117,
              0.03349582478404045,
              -0.024104731157422066,
              0.033579424023628235,
              -0.0008451460162177682,
              -0.023296596482396126,
              -0.0038212270010262728,
              0.007733020931482315,
              -0.010498794727027416,
              -0.012992868199944496,
              0.006503400858491659,
              0.004162594676017761,
              0.00922389142215252,
              0.022335194051265717,
              -0.03500062972307205,
              0.00739165348932147,
              -0.026069337502121925,
              -0.004152144771069288,
              -0.025372669100761414,
              0.0031245583668351173,
              0.006506884470582008,
              -0.021095123142004013,
              -0.009816059842705727,
              0.017151979729533195,
              -0.02045418880879879,
              -0.001589275081641972,
              0.021889325231313705,
              -0.030207548290491104,
              0.025010401383042336,
              0.021471325308084488,
              0.0021439974661916494,
              0.009084558114409447,
              -0.0007767854258418083,
              0.027880676090717316,
              -0.01596764288842678,
              -0.002070847200229764,
              -0.0024035065434873104,
              -0.0031228167936205864,
              0.017360979691147804,
              -0.000540788983926177,
              0.0186428502202034,
              -0.010498794727027416,
              0.006841285154223442,
              -0.004591045901179314,
              -0.008708356879651546,
              -0.003521659644320607,
              -0.007572787348181009,
              -0.002574190264567733,
              0.03550222888588905,
              0.011418397538363934,
              0.0342482253909111,
              -0.0024975568521767855,
              0.0014272996922954917,
              -0.011919998563826084,
              0.008631723001599312,
              0.017305247485637665,
              -0.012303167022764683,
              -0.01918625272810459,
              0.018823985010385513,
              0.009565259329974651,
              -0.006287433672696352,
              0.003291758941486478,
              0.00914725847542286,
              -0.04558999091386795,
              0.0003217737830709666,
              0.015229174867272377,
              0.0004941992810927331,
              -0.015284908004105091,
              0.00554199842736125,
              0.012338000349700451,
              0.0033718757331371307,
              0.011188496835529804,
              -0.002103938953951001,
              0.019757520407438278,
              -0.017444580793380737,
              -0.012247432954609394,
              -0.028842078521847725,
              0.002976516494527459,
              -0.0024052481167018414,
              0.0001037927286233753,
              -0.01738884672522545,
              0.017193779349327087,
              -0.01478330697864294,
              -0.0036505432799458504,
              0.0033718757331371307,
              0.03199101984500885,
              -0.028842078521847725,
              -0.042050912976264954,
              0.004918479826301336,
              0.011376597918570042,
              0.013125235214829445,
              -0.0074613201431930065,
              0.03112715110182762,
              -0.03722996637225151,
              0.014246871694922447,
              -0.020565655082464218,
              0.012351933866739273,
              -0.005886849481612444,
              0.02675207331776619,
              0.03600383177399635,
              -0.007475253660231829,
              0.0011259905295446515,
              -0.025707069784402847,
              0.0035303679760545492,
              -0.005911232437938452,
              -0.010401261039078236,
              -0.0035390763077884912,
              -0.0003302644472569227,
              -0.014212038367986679,
              -0.04040677472949028,
              0.011843365617096424,
              0.01065902877599001,
              0.01631597802042961,
              -0.003516434459015727,
              0.02599967084825039,
              0.01843385025858879,
              0.015521774999797344,
              -0.02886994555592537,
              0.01868464983999729,
              0.012045399285852909,
              0.0024522733874619007,
              0.006210800260305405,
              0.020844323560595512,
              0.02886994555592537,
              -0.011495031416416168,
              0.02172212488949299,
              0.010213160887360573,
              -0.010213160887360573,
              -0.002133547328412533,
              0.01571684330701828,
              0.0012836117530241609,
              -0.007726054172962904,
              0.01097949594259262,
              -0.0035094679333269596,
              -0.02130412496626377,
              -0.008144055493175983,
              -0.00489758001640439,
              -0.012553967535495758,
              -0.028591277077794075,
              -0.015647174790501595,
              -0.021931126713752747,
              -0.0011024780105799437,
              0.0001773239200701937,
              -0.02266959473490715,
              0.011599531397223473,
              -0.018475649878382683,
              -0.02528906799852848,
              -0.017347047105431557,
              -0.023394130170345306,
              -0.014616106636822224,
              -0.011655264534056187,
              -0.032213952392339706,
              -0.008617790415883064,
              -0.029092878103256226,
              0.006754201836884022,
              0.02534480206668377,
              -0.027588075026869774,
              -0.017765048891305923,
              -0.009676726534962654,
              -0.013877637684345245,
              0.0418279804289341,
              0.030792750418186188,
              0.0045875622890889645,
              -0.016190577298402786,
              0.013034668751060963,
              -0.03792663663625717,
              0.0009213441517204046,
              0.02201472595334053,
              0.0011190238874405622,
              0.01848958246409893,
              -0.02605540305376053,
              0.009906627237796783,
              -0.00020399325876496732,
              -0.032576221972703934,
              -0.009857860393822193,
              0.040573976933956146,
              -0.002218889305368066,
              0.006458117626607418,
              0.012233499437570572,
              0.0017199005233123899,
              0.003911793697625399,
              -0.0036261598579585552,
              -0.021025456488132477,
              0.015842242166399956,
              0.011237263679504395,
              0.019534585997462273,
              -0.017904382199048996,
              0.01627417653799057,
              -0.004322828259319067,
              0.02237699367105961,
              0.007948988117277622,
              0.027615942060947418,
              -0.014532506465911865,
              0.018754318356513977,
              0.025163667276501656,
              0.009850893169641495,
              -0.0022711395286023617,
              -0.0011756281601265073,
              -0.006064499728381634,
              -0.00982302613556385,
              -0.0011277322191745043,
              -0.03553009778261185,
              0.023338396102190018,
              0.0015709876315668225,
              0.02220979332923889,
              -0.02906501293182373,
              -0.033217158168554306,
              -0.01804371550679207,
              -0.008206755854189396,
              0.0012374575017020106,
              -0.009084558114409447,
              0.03265982121229172,
              -0.0279642753303051,
              0.00955132581293583,
              -0.006064499728381634,
              -0.012909268029034138,
              0.038706906139850616,
              0.012254400178790092,
              0.006893535144627094,
              0.006043599918484688,
              -0.0029660663567483425,
              -0.00960009265691042,
              0.015521774999797344,
              -0.020342722535133362,
              0.019228052347898483,
              0.02076072245836258,
              0.029956746846437454,
              -0.0038665104657411575,
              -0.015424241311848164,
              -0.010533628053963184,
              0.02630620449781418,
              0.009356258437037468,
              0.03279915452003479,
              0.003523401217535138,
              -0.012156866490840912,
              0.0030531499069184065,
              -0.004110344685614109,
              -0.0040406775660812855,
              0.053977880626916885,
              -0.01005292683839798,
              0.01995258778333664,
              0.030820617452263832,
              -0.01141143124550581,
              -0.0074404203332960606,
              0.017263446003198624,
              -0.011947865597903728,
              -0.017360979691147804,
              0.007551887072622776,
              0.017207713797688484,
              0.0009274400072172284,
              0.024801399558782578,
              0.0003629207785706967,
              -0.011195464059710503,
              0.010011127218604088,
              0.004267094656825066,
              -0.01249823346734047,
              0.02024518884718418,
              0.01787651516497135,
              0.023561330512166023,
              -0.014643972739577293,
              -0.030151814222335815,
              -0.028298676013946533,
              -0.012017532251775265,
              0.00023577875981573015,
              -0.017514247447252274,
              -0.004256644751876593,
              -0.010247994214296341,
              0.006573067978024483,
              0.003876960603520274,
              0.004364628344774246,
              -0.02363099716603756,
              0.01733311451971531,
              0.005022980272769928,
              0.02181965857744217,
              -0.006931852083653212,
              -0.020886123180389404,
              0.028131475672125816,
              -0.0040580942295491695,
              -0.011146697215735912,
              -0.015438174828886986,
              -0.019910788163542747,
              -0.010617229156196117,
              -0.028075743466615677,
              0.017597848549485207,
              0.008882524445652962,
              0.0077887545339763165,
              -0.020273054018616676,
              -0.003695826744660735,
              -0.002946907887235284,
              0.0015431208303198218,
              0.008861623704433441,
              -0.01118153054267168,
              0.013076468370854855,
              0.01125816348940134,
              -0.012045399285852909,
              0.005364347714930773,
              -0.016761844977736473,
              0.014448905363678932,
              0.049909334629774094,
              -0.006653184536844492,
              -0.02172212488949299,
              -0.01985505409538746,
              -0.018768250942230225,
              -0.018670717254281044,
              0.02332446351647377,
              0.010387328453361988,
              -0.019116584211587906,
              0.01153683103621006,
              0.024759599938988686,
              0.02372853085398674,
              -0.009425925090909004,
              0.0016868087695911527,
              0.011919998563826084,
              -0.0067054349929094315,
              -0.017402781173586845,
              -0.03753650188446045,
              -0.029455145820975304,
              0.0005564640159718692,
              0.025804603472352028,
              0.06325750797986984,
              0.0035094679333269596,
              0.0036993101239204407,
              -0.019464919343590736,
              0.0037480767350643873,
              -0.018015848472714424,
              -0.01592584326863289,
              0.0011303446954116225,
              -0.015633242204785347,
              -0.021485257893800735,
              -0.00836002267897129,
              0.0035652013029903173,
              0.0056499820202589035,
              0.02126232348382473,
              0.0009013149538077414,
              0.0013184451963752508,
              -0.05768415704369545,
              0.02559560351073742,
              -0.000957048439886421,
              0.003974494058638811,
              -0.014337439090013504,
              0.020816456526517868,
              0.03461049497127533,
              0.005099613685160875,
              -0.01752818003296852,
              0.01899118535220623,
              0.02509400062263012,
              0.018712516874074936,
              -0.04010024294257164,
              0.0060157328844070435,
              0.006917918566614389,
              -0.008798924274742603,
              0.00041408237302675843,
              -0.013139168731868267,
              0.022599928081035614,
              0.01708231307566166,
              0.006802968215197325,
              0.03380235657095909,
              0.0037202101666480303,
              -0.003915277309715748,
              0.012066299095749855,
              0.008645656518638134,
              -0.006245633587241173,
              -0.0037027932703495026,
              0.00836002267897129,
              -0.0035808763932436705,
              0.018503516912460327,
              0.02282286249101162,
              0.0013524078531190753,
              0.007238386198878288,
              -0.014379238709807396,
              -0.01783471554517746,
              -0.004451712127774954,
              -0.0074891867116093636,
              -0.007384686730802059,
              -0.011982698924839497,
              -0.009405025281012058,
              -0.025261200964450836,
              -0.023254796862602234,
              -0.003335300600156188,
              -0.00876409001648426,
              0.018475649878382683,
              -0.01139053050428629,
              0.009614026173949242,
              -0.005716165527701378,
              -0.003869993845000863,
              -0.017165912315249443,
              -0.013362103141844273,
              -0.0016040793852880597,
              -0.02634800411760807,
              -0.006130683235824108,
              -0.04840452969074249,
              -0.020328788086771965,
              0.023686731234192848,
              0.024397332221269608,
              0.029148612171411514,
              -0.013313336297869682,
              0.005172763951122761,
              -0.00545143149793148,
              -0.010059894062578678,
              -0.008018654771149158,
              0.016538910567760468,
              -0.005611665081232786,
              -0.016455311328172684,
              0.02609720453619957,
              -0.034136757254600525,
              -0.026111137121915817,
              -0.01950671896338463,
              0.02161065861582756,
              -0.011550764553248882,
              0.01779291406273842,
              -0.01468577329069376,
              -0.022237660363316536,
              -0.0067054349929094315,
              0.029287945479154587,
              -0.004845329560339451,
              0.02746267430484295,
              -0.0020969724282622337,
              -0.02605540305376053,
              0.020872190594673157,
              -0.03722996637225151,
              0.0004397720331326127,
              -0.03472195938229561,
              -0.012888368219137192,
              -0.003988427575677633,
              -0.0021770892199128866,
              0.012331033125519753,
              -0.014072705060243607,
              -0.003828193759545684,
              -0.0120941661298275,
              -0.01315310224890709,
              -0.01251913420855999,
              -0.0035025011748075485,
              0.046119458973407745,
              0.013055568560957909,
              0.004500478971749544,
              -0.014797240495681763,
              0.009202991612255573,
              -0.020523855462670326,
              -0.006771618500351906,
              -0.015340641140937805,
              0.02553986944258213,
              0.00311759184114635,
              -0.014866907149553299,
              0.03867903724312782,
              -0.01282566785812378,
              -0.011926965788006783,
              -0.015438174828886986,
              0.035335030406713486,
              -0.0234359297901392,
              0.045617856085300446,
              0.0035460428334772587,
              0.0027309407014399767,
              -0.017500314861536026,
              0.008276422508060932,
              -0.005280747544020414,
              -0.010895895771682262,
              -0.03148941695690155,
              0.018614983186125755,
              0.026361938565969467,
              -0.012303167022764683,
              -0.003533851122483611,
              -0.022530261427164078,
              -0.01612091064453125,
              -0.020091921091079712,
              -0.024271931499242783,
              0.04508838802576065,
              -0.0038978606462478638,
              0.003025283105671406,
              0.007168719545006752,
              0.009363225661218166,
              -0.0028371827211230993,
              -0.0002782320079859346,
              -0.037257835268974304,
              -0.020426321774721146,
              0.0015509583754464984,
              0.0397937074303627,
              0.0193952526897192,
              0.024620266631245613,
              0.04701119288802147,
              -0.011947865597903728,
              0.018127316609025,
              0.020217321813106537,
              -0.0008625627378933132,
              -0.0031837753485888243,
              -0.030346881598234177,
              -0.008234621956944466,
              -0.015995509922504425,
              0.014191138558089733,
              0.007067702244967222,
              -0.0076424540020525455,
              0.006956235505640507,
              0.01985505409538746,
              0.015006240457296371,
              0.011077029630541801,
              -0.003838643664494157,
              -0.0021370307076722383,
              0.0010876738233491778,
              -0.019172318279743195,
              -0.01256790105253458,
              0.01869858428835869,
              -0.018308449536561966,
              0.00748222041875124,
              -0.03380235657095909,
              -0.004695545881986618,
              0.0031559085473418236,
              -0.018865784630179405,
              0.011940899305045605,
              -0.00720355287194252,
              0.019074784591794014,
              -0.019269851967692375,
              0.010282827541232109,
              -0.016998711973428726,
              0.013821904547512531,
              -0.013229736126959324,
              -0.010888929478824139,
              -0.023463796824216843,
              -0.001105090486817062,
              0.01677577942609787,
              0.019966520369052887,
              -0.011620431207120419,
              0.0018583633936941624,
              -0.030514081940054893,
              0.015326708555221558,
              -0.008137088268995285,
              -0.029148612171411514,
              -0.003768976777791977,
              0.001748638111166656,
              0.007879321463406086,
              -0.016817579045891762,
              -0.04946346580982208,
              -0.034638360142707825,
              -0.013362103141844273,
              -0.009634925983846188,
              -0.001558795920573175,
              -0.016399577260017395,
              0.015298841521143913,
              -0.011042196303606033,
              -0.04135424643754959,
              -0.004496995359659195,
              -0.0026560488622635603,
              0.03023541532456875,
              0.022335194051265717,
              0.005200630519539118,
              0.012985901907086372,
              0.030904216691851616,
              0.013389969244599342,
              -0.011007362976670265,
              0.00028258617385290563,
              -0.019019052386283875,
              0.009920559823513031,
              0.011188496835529804,
              -0.023867864161729813,
              -0.0428311824798584,
              0.0299846138805151,
              0.027393007650971413,
              -0.017667515203356743,
              -0.029343679547309875,
              0.015856176614761353,
              -0.018782183527946472,
              -0.001386370393447578,
              0.00997629389166832,
              -0.011425363831222057,
              -0.021122990176081657,
              -0.009035791270434856,
              -0.012170800007879734,
              0.014295638538897038,
              0.01468577329069376,
              -0.005298164207488298,
              -0.017848648130893707,
              0.013013768941164017,
              -0.020119788125157356,
              0.005218047648668289,
              0.010881963185966015,
              -0.010847128927707672,
              0.024007197469472885,
              0.01471363939344883,
              0.022711394354701042,
              0.014267771504819393,
              0.03304995596408844,
              0.026334071531891823,
              -0.030709149315953255,
              0.018378116190433502,
              -0.013452669605612755,
              -0.003040958195924759,
              -0.007224452681839466,
              0.005360864568501711,
              0.004469128791242838,
              0.0304862167686224,
              -0.022711394354701042,
              -0.00927265826612711,
              0.004099894315004349,
              -0.009767292998731136,
              0.017653580754995346,
              0.008729256689548492,
              -0.00025994444149546325,
              0.004911513067781925,
              -0.011522897519171238,
              0.007928088307380676,
              0.034638360142707825,
              0.006123716477304697,
              -0.02176392637193203,
              -0.0007519666105508804,
              -0.0016493628500029445,
              0.0037445933558046818,
              -0.02690533921122551,
              0.009453792124986649,
              0.029510879889130592,
              -0.0418279804289341,
              -0.00914029125124216,
              0.027518408372998238,
              0.011334797367453575,
              0.011049163527786732,
              -0.029148612171411514,
              0.011090963147580624,
              0.013167035765945911,
              0.021025456488132477,
              -0.013410869985818863,
              -0.010206194594502449,
              -0.007503120228648186,
              0.015326708555221558,
              -0.03006821498274803,
              -0.017165912315249443,
              -0.019924720749258995,
              -0.011857299134135246,
              0.0158701092004776,
              0.021039390936493874,
              0.007565820589661598,
              -0.007872354239225388,
              0.010540595278143883,
              -0.029343679547309875,
              0.010241027921438217,
              0.009753359481692314,
              -0.004260128363966942,
              -0.00318725872784853,
              0.014769373461604118,
              -0.0038665104657411575,
              0.010450027883052826,
              0.0176257137209177,
              0.020133720710873604,
              -0.0065626176074147224,
              0.01000415999442339,
              -0.030597683042287827,
              -0.015897976234555244,
              -0.007955954410135746,
              0.004124277736991644,
              -0.00038055519689805806,
              -0.02800607495009899,
              0.006447667255997658,
              0.004775662906467915,
              0.00568133220076561,
              -0.0022397893480956554,
              -0.015354574657976627,
              -0.005329514387995005,
              0.002267656149342656,
              -0.011676165275275707,
              0.00816495530307293,
              0.016566777601838112,
              -0.023310529068112373,
              0.037007033824920654,
              0.0067054349929094315,
              0.04951919987797737,
              0.0018670717254281044,
              -0.01606517657637596,
              -0.03491702675819397,
              -0.026125071570277214,
              -0.017556047067046165,
              -0.026877472177147865,
              -0.03304995596408844,
              -0.025484135374426842,
              0.02790854312479496,
              0.019562453031539917,
              -0.027128273621201515,
              -0.03260408714413643,
              -0.018196983262896538,
              -0.019659986719489098,
              0.00997629389166832,
              0.01646924391388893,
              0.01779291406273842,
              0.010575428605079651,
              0.01617664471268654,
              -0.02841014415025711,
              0.03714636713266373,
              0.010087761096656322,
              -0.0006901372689753771,
              -0.013550203293561935,
              0.012581833638250828,
              0.011397497728466988,
              -0.01612091064453125,
              0.004803529474884272,
              0.02473173290491104,
              -0.04751279577612877,
              -0.0007872354472056031,
              0.03784303739666939,
              -0.006451150868088007,
              0.02403506450355053,
              0.03918064013123512,
              -0.019715720787644386,
              -0.03480556234717369,
              -0.01430957205593586,
              0.018322382122278214,
              0.033022090792655945,
              0.01692904531955719,
              0.003636609762907028,
              -0.0019855054561048746,
              -0.01342480257153511,
              0.0052772643975913525,
              0.013717403635382652,
              -0.026724206283688545,
              0.0027675158344209194,
              0.0054792980663478374,
              0.0037724601570516825,
              -0.02397933229804039,
              -0.017221646383404732,
              0.011676165275275707,
              0.012059332802891731,
              -0.014978373423218727,
              0.020523855462670326,
              -0.004420361947268248,
              -0.02967808023095131,
              0.0049393801018595695,
              0.02679387293756008,
              0.0015971126267686486,
              -0.020900055766105652,
              0.0110839968547225,
              -0.027936408296227455,
              0.0032029335852712393,
              0.02161065861582756,
              0.012958034873008728,
              -0.0052842311561107635,
              -0.002953874645754695,
              0.005691782105714083,
              -0.02573493681848049,
              -0.00224153115414083,
              -0.007691220846027136,
              0.0058694323524832726,
              -0.018461717292666435,
              0.04625879228115082,
              0.02917647920548916,
              -0.05105186998844147,
              0.017458513379096985,
              -0.013780103996396065,
              -0.008833757601678371,
              -0.019032984972000122,
              0.0032691170927137136,
              0.005033430177718401,
              -0.021136924624443054,
              0.02101152390241623,
              0.0037480767350643873,
              -0.03505636006593704,
              -0.0019611220341175795,
              -0.0012052366510033607,
              0.008680489845573902,
              -0.01047789491713047,
              0.02574886940419674,
              0.2039845585823059,
              -0.03405315801501274,
              0.01833631657063961,
              0.0029573580250144005,
              0.013780103996396065,
              0.0176257137209177,
              0.0037550434935837984,
              0.017709314823150635,
              -0.00217186426743865,
              -0.001844429993070662,
              0.00048462007544003427,
              0.008060455322265625,
              -0.013222768902778625,
              0.0024365982972085476,
              0.0224187932908535,
              0.0010824487544596195,
              -0.012066299095749855,
              -0.046175193041563034,
              -0.00687611848115921,
              -0.0056778485886752605,
              -0.005225013941526413,
              0.014699706807732582,
              -0.003213383723050356,
              -0.036505430936813354,
              0.014351372607052326,
              -0.012916235253214836,
              -0.006691501475870609,
              -0.026919273659586906,
              0.006419800687581301,
              0.03472195938229561,
              -0.007377719972282648,
              0.01858711615204811,
              -0.008241589181125164,
              0.015479975380003452,
              -0.009690659120678902,
              -0.020384522154927254,
              -0.02272532880306244,
              0.02222372591495514,
              0.011056129820644855,
              -0.009425925090909004,
              0.035084228962659836,
              0.010401261039078236,
              -0.008025621995329857,
              -0.012331033125519753,
              -0.00031154145835898817,
              -0.00023947982117533684,
              0.000322644627885893,
              -0.013814937323331833,
              -0.002095230622217059,
              0.03023541532456875,
              -0.010512728244066238,
              0.02654307149350643,
              0.039124906063079834,
              0.042691849172115326,
              0.0030461831483989954,
              -0.014267771504819393,
              0.002694365568459034,
              0.00595651613548398,
              0.005886849481612444,
              0.01949278637766838,
              -0.025762803852558136,
              0.0186428502202034,
              0.002546323463320732,
              0.03112715110182762,
              0.03179595246911049,
              0.015856176614761353,
              -0.019882921129465103,
              0.012393733486533165,
              0.015730775892734528,
              -0.004723412916064262,
              -0.015382441692054272,
              -0.016817579045891762,
              -0.005925165954977274,
              -0.004653745796531439,
              -0.03502849489450455,
              -0.02126232348382473,
              0.051414139568805695,
              0.01818304881453514,
              0.009565259329974651,
              0.034081026911735535,
              -0.018754318356513977,
              -0.005834599025547504,
              0.011369630694389343,
              -0.014894773252308369,
              -0.026779938489198685,
              -0.043054115027189255,
              0.017305247485637665,
              -0.01395427156239748,
              -0.017946181818842888,
              -0.016483178362250328,
              0.0023791231215000153,
              -0.021081190556287766,
              -0.008959157392382622,
              0.008617790415883064,
              -0.0004689450142905116,
              -0.010735662654042244,
              -0.019172318279743195,
              0.03940357267856598,
              -0.0014569080667570233,
              -0.021596726030111313,
              -0.03452689200639725,
              0.04703906178474426,
              0.03842823579907417,
              0.02644553780555725,
              -0.0008155376417562366,
              0.004946346860378981,
              -0.01019922737032175,
              -0.008157989010214806,
              0.022502394393086433,
              -0.0013027702225372195,
              0.001785213127732277,
              -0.012386767193675041,
              0.0019053885480389,
              0.007997754961252213,
              -0.0018931968370452523,
              -0.011327831074595451,
              0.005221530795097351,
              -0.014393172226846218,
              0.012637567706406116,
              -0.005221530795097351,
              0.0005899911629967391,
              -0.03388595953583717,
              -0.039821576327085495,
              0.017946181818842888,
              -0.0014830331783741713,
              -0.004685095977038145,
              -0.016413511708378792,
              -0.02619473822414875,
              -0.01430957205593586,
              -0.007405586540699005,
              0.011327831074595451,
              -0.03265982121229172,
              0.03530716150999069,
              -0.021582791581749916,
              -0.02906501293182373,
              0.00651036761701107,
              0.01950671896338463,
              -0.017542114481329918,
              -0.01779291406273842,
              0.009941460564732552,
              0.009460759349167347,
              -5.8073856052942574e-05,
              0.006141133140772581,
              -0.006984102074056864,
              0.01627417653799057,
              -0.0468439944088459,
              0.0076215541921556,
              -0.014434972777962685,
              -0.0014351372374221683,
              -0.03625463321805,
              -0.01478330697864294,
              -0.007649420760571957,
              0.0015466042095795274,
              -0.019311651587486267,
              0.023463796824216843,
              -0.002380864694714546,
              -0.018670717254281044,
              -0.015159507282078266,
              -0.013264569453895092,
              0.0021109057124704123,
              -0.028786344453692436,
              -0.015591442584991455,
              -0.0022641727700829506,
              -0.025219401344656944,
              -0.009077590890228748,
              -0.009579192847013474,
              -0.17745541036128998,
              0.010847128927707672,
              0.02076072245836258,
              -0.032074619084596634,
              -0.005552448332309723,
              0.02711433917284012,
              0.032269686460494995,
              -0.009850893169641495,
              -0.020482055842876434,
              0.003523401217535138,
              -0.0010632904013618827,
              -0.017012646421790123,
              -0.02856341004371643,
              -0.0007136498461477458,
              0.004350695293396711,
              0.01306950207799673,
              0.0018235299503430724,
              0.02045418880879879,
              0.024898933246731758,
              0.02825687639415264,
              0.03143368661403656,
              -0.044614654034376144,
              0.013536269776523113,
              -0.007321986369788647,
              0.02064925618469715,
              -0.009767292998731136,
              -0.010171361267566681,
              0.03137795254588127,
              -0.017151979729533195,
              -0.014671839773654938,
              0.012128999456763268,
              0.02725367434322834,
              0.012101132422685623,
              -0.004552728962153196,
              0.00025515485322102904,
              0.020886123180389404,
              0.01123029738664627,
              -0.01334816962480545,
              -0.0011651781387627125,
              -0.008415755815804005,
              0.05252880975604057,
              -0.00992752704769373,
              -0.0076145874336361885,
              -0.014121471904218197,
              -0.023644929751753807,
              -0.011097930371761322,
              0.007670320570468903,
              -0.0022746229078620672,
              0.016009442508220673,
              0.018364183604717255,
              -0.01748638041317463,
              -0.020384522154927254,
              0.020635321736335754,
              0.006398900877684355,
              0.007005002349615097,
              0.02679387293756008,
              0.011662231758236885,
              0.00821372214704752,
              -0.026166871190071106,
              0.015535708516836166,
              0.004977697040885687,
              -0.033022090792655945,
              0.026013603433966637,
              0.002319906372576952,
              -0.014978373423218727,
              -0.0016868087695911527,
              0.0018479133723303676,
              0.004333278629928827,
              -0.021986858919262886,
              0.011202430352568626,
              -0.03647756576538086,
              -0.023463796824216843,
              0.0003302644472569227,
              0.0047408295795321465,
              0.022599928081035614,
              -0.00012757742661051452,
              -0.019367385655641556,
              -0.0012609701370820403,
              0.001722512999549508,
              -0.01123029738664627,
              -0.033022090792655945,
              0.0317402184009552,
              -0.0096976263448596,
              0.026975005865097046,
              0.0032081587705761194,
              -0.012087199836969376,
              0.005521098151803017,
              0.018322382122278214,
              -0.019715720787644386,
              -0.026431605219841003,
              0.011432331055402756,
              0.0060226996429264545,
              0.017542114481329918,
              0.0019785386975854635,
              0.007412553299218416,
              0.01779291406273842,
              0.010220128111541271,
              -0.015284908004105091,
              0.022181926295161247,
              -0.012853534892201424,
              -0.02105332352221012,
              -0.017347047105431557,
              0.006381483748555183,
              0.009307491593062878,
              0.012783868238329887,
              0.02533086948096752,
              -0.019618187099695206,
              0.006694984622299671,
              0.029037145897746086,
              0.011481097899377346,
              -0.006043599918484688,
              -0.010833196341991425,
              0.00023664960463065654,
              0.02629227191209793,
              0.017820781096816063,
              0.009133324958384037,
              -0.03193528577685356,
              -0.0027918992564082146,
              0.005256364122033119,
              -0.02599967084825039,
              0.043499983847141266,
              -0.010603295639157295,
              0.003612226340919733,
              0.014978373423218727,
              -0.0007920250645838678,
              -0.01804371550679207,
              -0.11425364017486572,
              -0.008206755854189396,
              -0.002555032027885318,
              0.004354178439825773,
              -0.014490705914795399,
              -0.004120794590562582,
              0.02172212488949299,
              0.029483012855052948,
              -0.026027537882328033,
              0.010624195449054241,
              -0.020579589530825615,
              -0.02711433917284012,
              0.013090401887893677,
              0.004803529474884272,
              -0.005319064483046532,
              0.008185855112969875,
              0.0033701341599226,
              -0.0047582462430000305,
              -0.009251758456230164,
              0.026682404801249504,
              -0.0026473405305296183,
              -0.01959032006561756,
              0.014393172226846218,
              -0.024048998951911926,
              0.015842242166399956,
              0.00677858479321003,
              -0.029705947265028954,
              0.0015788250602781773,
              0.021889325231313705,
              0.011028262786567211,
              0.007398620247840881,
              -0.03455476090312004,
              -0.00881285686045885,
              -0.003410192672163248,
              -0.0019837636500597,
              -0.003326592268422246,
              -0.012909268029034138,
              -0.022558128461241722,
              0.005782349035143852,
              -0.03405315801501274,
              0.0029051078017801046,
              0.013292435556650162,
              -0.004169561434537172,
              -0.010157427750527859,
              -0.010136527009308338,
              -0.00012942795001436025,
              -0.014434972777962685,
              0.030151814222335815,
              0.023770330473780632,
              -0.00803258828818798,
              -0.012198666110634804,
              -0.02045418880879879,
              -0.02499646693468094,
              -0.005367831327021122,
              0.020941857248544693,
              0.033133555203676224,
              0.0186428502202034,
              0.013598970137536526,
              -0.004517895635217428,
              -0.012965002097189426,
              -0.000530774355866015,
              -0.005092646926641464,
              -0.01698477938771248,
              0.005144897382706404,
              0.018628917634487152,
              -0.001567504252307117,
              -0.05473028123378754,
              -0.011167597025632858,
              -0.008673523552715778,
              -0.005350414663553238,
              -0.014156305231153965,
              0.01793224923312664,
              0.013194902800023556,
              0.02155492454767227,
              -0.01858711615204811,
              -0.01012259442359209,
              -0.014281705021858215,
              -0.0021091641392558813,
              0.0022066975943744183,
              -0.018726451322436333,
              -0.01090286299586296,
              -0.0274487417191267,
              0.007698187604546547,
              -0.002624698681756854,
              -0.010220128111541271,
              -0.012804768048226833,
              -0.0012182991486042738,
              -0.02499646693468094,
              -0.011745831929147243,
              -0.029761681333184242,
              0.009662793017923832,
              0.008645656518638134,
              -0.029761681333184242,
              -0.008234621956944466,
              -0.004180011339485645,
              -0.0024644650984555483,
              -0.0031994504388421774,
              -0.01390550471842289,
              0.020217321813106537,
              0.005987866315990686,
              -0.015075907111167908,
              0.0043750787153840065,
              -0.04391798377037048,
              0.016162710264325142,
              0.0032952423207461834,
              -0.01698477938771248,
              0.011655264534056187,
              0.018823985010385513,
              0.03692343458533287,
              -0.017040513455867767,
              0.002717007417231798,
              0.008854657411575317,
              -0.007363786455243826,
              0.021485257893800735,
              -0.01738884672522545,
              -0.018977250903844833,
              -0.006447667255997658,
              -0.010993429459631443,
              0.005925165954977274,
              -0.004507445730268955,
              0.029204346239566803,
              0.005803248845040798,
              -0.01499230694025755,
              -0.016538910567760468,
              0.009572225622832775,
              -0.00025798508431762457,
              -0.007774821016937494,
              0.013828870840370655,
              -0.010234060697257519,
              0.026320137083530426,
              -0.026612738147377968,
              -0.011780665256083012,
              0.020077988505363464,
              -0.02403506450355053,
              -0.0008969607879407704,
              0.030040347948670387,
              -0.0010589362354949117,
              0.016664311289787292,
              0.02423013187944889,
              0.008596889674663544,
              0.03622676432132721,
              0.004639812745153904,
              -0.014755439944565296,
              -0.03600383177399635,
              0.0020307889208197594,
              -0.012540034018456936,
              -0.00260728201828897,
              0.022251592949032784,
              -0.0027222323697060347,
              0.003500759368762374,
              0.015479975380003452,
              -0.006398900877684355,
              0.008778023533523083,
              0.02091399021446705,
              0.019074784591794014,
              -0.029009278863668442,
              -0.015953710302710533,
              -0.00849238969385624,
              -0.0035286261700093746,
              0.02278106100857258,
              0.002276364481076598,
              -0.0034415426198393106,
              0.0367841012775898,
              -0.015048041008412838,
              0.022279459983110428,
              0.003981460817158222,
              0.0076424540020525455,
              -0.008750157430768013,
              0.012428566813468933,
              0.007039835676550865,
              -0.02912074513733387,
              -0.02827080897986889,
              -0.010568462312221527,
              0.009251758456230164,
              -0.011773698963224888,
              0.02565133571624756,
              0.029037145897746086,
              0.005252880975604057,
              -0.0007088602287694812,
              0.0114601980894804,
              -0.005998316220939159,
              0.01478330697864294,
              0.012484300881624222,
              -0.02459239959716797,
              -0.016845446079969406,
              -0.014448905363678932,
              0.024453066289424896,
              0.002779707545414567,
              -0.004451712127774954,
              0.012170800007879734,
              -0.005378281231969595,
              0.02195899188518524,
              0.012184732593595982,
              0.001861846656538546,
              0.009760326705873013,
              0.03555796295404434,
              0.00401629414409399,
              0.01490870676934719,
              0.007349853403866291,
              -0.019269851967692375,
              0.006796001922339201,
              -0.021749991923570633,
              0.006127200089395046,
              0.015828309580683708,
              -0.012463400140404701,
              -0.010094727389514446,
              -0.023157263174653053,
              0.021136924624443054,
              -0.013229736126959324,
              -0.00457362923771143,
              0.003953594248741865,
              0.03770370036363602,
              -0.0011442780960351229,
              0.015298841521143913,
              -0.01219169981777668,
              0.01080532930791378,
              -0.034276094287633896,
              0.02765774168074131,
              -0.007210519630461931,
              -0.036505430936813354,
              -0.026570938527584076,
              0.033022090792655945,
              0.021931126713752747,
              -0.01518737431615591,
              0.044558919966220856,
              -0.012212599627673626,
              0.018559250980615616,
              0.010819262824952602,
              0.024717800319194794,
              -0.011906065978109837,
              0.02638980560004711,
              0.02278106100857258,
              0.017597848549485207,
              -0.008680489845573902,
              0.01989685371518135,
              -0.01874038390815258,
              -0.021136924624443054,
              -0.012310133315622807,
              -0.006607901304960251,
              0.03279915452003479,
              -0.0008817211491987109,
              0.10160214453935623,
              0.021248390898108482,
              -0.0030949499923735857,
              0.012115065939724445,
              -0.015702908858656883,
              0.011871231719851494,
              0.013362103141844273,
              0.018921518698334694,
              -0.011474130675196648,
              -0.0010545820696279407,
              0.006339683663100004,
              -0.015591442584991455,
              -0.013689536601305008,
              -0.02423013187944889,
              0.0035355929285287857,
              -0.015605375170707703,
              0.003765493631362915,
              0.008617790415883064,
              -0.008610823191702366,
              -0.0030200581531971693,
              0.024299798533320427,
              -0.00043737722444348037,
              0.011223330162465572,
              0.01602337695658207,
              -0.013118268921971321,
              -0.009669759310781956,
              0.007018935866653919,
              -0.016455311328172684,
              -0.008673523552715778,
              -0.03112715110182762,
              0.004406428430229425,
              -0.016566777601838112,
              -0.04166077822446823,
              -0.013571103103458881,
              0.02815934270620346,
              0.01097949594259262,
              -0.001757346442900598,
              0.013341202400624752,
              0.014121471904218197,
              5.785614484921098e-05,
              0.009293558076024055,
              0.036449696868658066,
              -0.0029138161335140467,
              -0.00964885950088501,
              -0.013647736981511116,
              -0.007997754961252213,
              0.0014725831570103765,
              -0.013731337152421474,
              -0.02312939614057541
            ],
            "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
            "indexType": "cogsearchvs",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            },
            "question": "How are sentence pairs represented in the input to BERT?"
          },
          "output": [
            {
              "page_content": "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.",
              "metadata": {
                "id": "Bert_pdf-9",
                "source": "Bert.pdf"
              }
            },
            {
              "page_content": "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
              "metadata": {
                "id": "Bert_pdf-2",
                "source": "Bert.pdf"
              }
            },
            {
              "page_content": "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
              "metadata": {
                "id": "Bert_pdf-1",
                "source": "Bert.pdf"
              }
            }
          ],
          "start_time": 1695246512.116644,
          "end_time": 1695246512.696523,
          "error": null,
          "children": null,
          "node_name": "search_question_from_vectordb"
        },
        {
          "name": "extractPrommptTemplate",
          "type": "Tool",
          "inputs": {
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            }
          },
          "output": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
          "start_time": 1695246512.135412,
          "end_time": 1695246512.137681,
          "error": null,
          "children": null,
          "node_name": "extract_prompttemplate"
        },
        {
          "name": "executeQa",
          "type": "Tool",
          "inputs": {
            "llm": {
              "model": "gpt-3.5-turbo",
              "request_timeout": null,
              "max_tokens": 1000,
              "stream": false,
              "n": 1,
              "temperature": 0.3,
              "engine": "chat16k",
              "_type": "azure-openai-chat"
            },
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            },
            "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
            "question": "How are sentence pairs represented in the input to BERT?",
            "retrievedDocs": [
              {
                "page_content": "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.",
                "metadata": {
                  "id": "Bert_pdf-9",
                  "source": "Bert.pdf"
                }
              },
              {
                "page_content": "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "metadata": {
                  "id": "Bert_pdf-2",
                  "source": "Bert.pdf"
                }
              },
              {
                "page_content": "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
                "metadata": {
                  "id": "Bert_pdf-1",
                  "source": "Bert.pdf"
                }
              }
            ]
          },
          "output": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B.",
          "start_time": 1695246512.7152,
          "end_time": 1695246514.143959,
          "error": null,
          "children": [
            {
              "name": "openai.api_resources.chat_completion.ChatCompletion.create",
              "type": "LLM",
              "inputs": {
                "messages": [
                  {
                    "role": "user",
                    "content": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        Content: A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.\nSource: Bert.pdf\n\nContent: 2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.\nSource: Bert.pdf\n\nContent: BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\nSource: Bert.pdf\n        Question: How are sentence pairs represented in the input to BERT?\n        "
                  }
                ],
                "model": "gpt-3.5-turbo",
                "request_timeout": null,
                "max_tokens": 1000,
                "stream": false,
                "n": 1,
                "temperature": 0.3,
                "engine": "chat16k",
                "api_base": "https://dataaiapim.azure-api.net",
                "organization": "",
                "api_type": "azure",
                "api_version": "2023-07-01-preview"
              },
              "output": {
                "id": "chatcmpl-80ufJMRFP3LCpOqkQz9aICocNSAMZ",
                "object": "chat.completion",
                "created": 1695228513,
                "model": "gpt-35-turbo-16k",
                "prompt_filter_results": [
                  {
                    "prompt_index": 0,
                    "content_filter_results": {
                      "hate": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "self_harm": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "sexual": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "violence": {
                        "filtered": false,
                        "severity": "safe"
                      }
                    }
                  }
                ],
                "choices": [
                  {
                    "index": 0,
                    "finish_reason": "stop",
                    "message": {
                      "role": "assistant",
                      "content": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B."
                    },
                    "content_filter_results": {
                      "hate": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "self_harm": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "sexual": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "violence": {
                        "filtered": false,
                        "severity": "safe"
                      }
                    }
                  }
                ],
                "usage": {
                  "completion_tokens": 52,
                  "prompt_tokens": 6056,
                  "total_tokens": 6108
                }
              },
              "start_time": 1695246512.724745,
              "end_time": 1695246514.14296,
              "error": null,
              "children": null,
              "node_name": null
            }
          ],
          "node_name": "execute_langchain"
        },
        {
          "name": "generateFollowupQuestions",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "embeddedQuestion": [
              -0.011118830181658268,
              -0.01631597802042961,
              0.02559560351073742,
              -0.002060397295281291,
              -0.005500198341906071,
              0.031210750341415405,
              0.015424241311848164,
              -0.011599531397223473,
              -0.01773718185722828,
              -0.027727408334612846,
              0.028535543009638786,
              0.04366718605160713,
              -0.035139963030815125,
              0.005427048075944185,
              -0.0009474692051298916,
              0.008422723039984703,
              0.011244230903685093,
              0.009614026173949242,
              0.027797074988484383,
              -0.016956912353634834,
              -0.0016798420110717416,
              -0.006102816667407751,
              -0.016552845016121864,
              -0.0035739096347242594,
              -0.010742628946900368,
              -0.00849238969385624,
              0.029538746923208237,
              -0.020328788086771965,
              -0.003713243408128619,
              -0.009676726534962654,
              0.0021788307931274176,
              0.02357526309788227,
              -0.017918314784765244,
              -0.009948426857590675,
              -0.009258724749088287,
              0.006694984622299671,
              -0.0029294912237674,
              -0.016915112733840942,
              0.02438339963555336,
              -0.023212995380163193,
              0.0071164690889418125,
              0.021694257855415344,
              0.003977977205067873,
              -0.015270974487066269,
              -0.005953032989054918,
              0.01672004535794258,
              0.010652062483131886,
              -0.0065695843659341335,
              -0.012059332802891731,
              0.0093214251101017,
              0.00438204500824213,
              0.015856176614761353,
              -0.03945930674672127,
              -0.020997589454054832,
              -0.02251632697880268,
              -0.0034084508661180735,
              -0.002352998126298189,
              0.032966356724500656,
              -0.0272118728607893,
              -0.013543237000703812,
              0.0005050846957601607,
              -0.008130121976137161,
              0.0009370191837660968,
              0.008053488098084927,
              -0.022404860705137253,
              -0.024466998875141144,
              -0.008548122830688953,
              0.008018654771149158,
              0.019437052309513092,
              0.007398620247840881,
              0.01959032006561756,
              0.010038994252681732,
              0.0009866568725556135,
              -0.001406399649567902,
              0.0428033173084259,
              -0.01388460397720337,
              0.002535873558372259,
              -0.014755439944565296,
              0.001035423600114882,
              0.03592022880911827,
              0.012735101394355297,
              -0.002624698681756854,
              -0.0010606779251247644,
              0.026487339287996292,
              0.015897976234555244,
              -0.012742067687213421,
              0.010436095297336578,
              0.025275135412812233,
              0.002154447603970766,
              0.0040302276611328125,
              0.027379073202610016,
              0.03536289557814598,
              0.012944101355969906,
              0.0067054349929094315,
              -0.0038839271292090416,
              0.012247432954609394,
              -0.01118153054267168,
              0.038818370550870895,
              0.02634800411760807,
              -0.009523458778858185,
              -0.010157427750527859,
              0.008513289503753185,
              -0.04497692361474037,
              -0.005552448332309723,
              -0.024606334045529366,
              0.014852973632514477,
              0.010749596171081066,
              -0.01130693033337593,
              0.003262150567024946,
              0.001316703506745398,
              -0.019074784591794014,
              0.030207548290491104,
              0.006221250165253878,
              -0.024216199293732643,
              -0.0005220660241320729,
              -0.002276364481076598,
              0.015479975380003452,
              -0.039319973438978195,
              -0.009669759310781956,
              0.012602734379470348,
              0.017165912315249443,
              -0.004075510893017054,
              0.010721729137003422,
              -0.034582626074552536,
              0.03288275748491287,
              -0.003422384150326252,
              -0.010819262824952602,
              -0.03380235657095909,
              -0.006520817521959543,
              -0.04288691654801369,
              0.027420874685049057,
              0.022641727700829506,
              0.015939775854349136,
              -0.022753195837140083,
              0.005636048503220081,
              0.01854531653225422,
              -0.02760200761258602,
              -0.02050992287695408,
              -0.01959032006561756,
              -0.004643295891582966,
              0.02257206104695797,
              0.003828193759545684,
              -0.014253838919103146,
              -0.023993264883756638,
              -0.010192261077463627,
              0.018601050600409508,
              0.018782183527946472,
              0.020412389189004898,
              0.0024139564484357834,
              -0.010408228263258934,
              0.026334071531891823,
              -0.001065902877599001,
              -0.029761681333184242,
              0.013807971030473709,
              0.007726054172962904,
              0.0011695323046296835,
              0.00037707184674218297,
              0.009753359481692314,
              -0.009732459671795368,
              -0.005601215176284313,
              -0.020412389189004898,
              0.017012646421790123,
              0.017096245661377907,
              -0.009307491593062878,
              0.024118665605783463,
              0.03349582478404045,
              0.0020900056697428226,
              -0.014866907149553299,
              0.007328953128308058,
              -0.029204346239566803,
              -0.026222603395581245,
              0.018252715468406677,
              -0.01959032006561756,
              -0.003438059240579605,
              0.0040128109976649284,
              0.00180088821798563,
              0.03391382470726967,
              -0.02806180901825428,
              -0.02987314760684967,
              -0.015619308687746525,
              0.0193952526897192,
              0.0065521677024662495,
              0.020119788125157356,
              0.023547397926449776,
              -0.03098781779408455,
              -0.02559560351073742,
              -0.003594809677451849,
              -0.01974358595907688,
              0.01012259442359209,
              -0.034192491322755814,
              0.02382606454193592,
              0.007102536037564278,
              0.000681428937241435,
              -0.01843385025858879,
              -0.6148517727851868,
              -0.03820530325174332,
              -0.009300525300204754,
              -0.028535543009638786,
              0.01698477938771248,
              0.005514131393283606,
              -0.011627398431301117,
              0.03349582478404045,
              -0.024104731157422066,
              0.033579424023628235,
              -0.0008451460162177682,
              -0.023296596482396126,
              -0.0038212270010262728,
              0.007733020931482315,
              -0.010498794727027416,
              -0.012992868199944496,
              0.006503400858491659,
              0.004162594676017761,
              0.00922389142215252,
              0.022335194051265717,
              -0.03500062972307205,
              0.00739165348932147,
              -0.026069337502121925,
              -0.004152144771069288,
              -0.025372669100761414,
              0.0031245583668351173,
              0.006506884470582008,
              -0.021095123142004013,
              -0.009816059842705727,
              0.017151979729533195,
              -0.02045418880879879,
              -0.001589275081641972,
              0.021889325231313705,
              -0.030207548290491104,
              0.025010401383042336,
              0.021471325308084488,
              0.0021439974661916494,
              0.009084558114409447,
              -0.0007767854258418083,
              0.027880676090717316,
              -0.01596764288842678,
              -0.002070847200229764,
              -0.0024035065434873104,
              -0.0031228167936205864,
              0.017360979691147804,
              -0.000540788983926177,
              0.0186428502202034,
              -0.010498794727027416,
              0.006841285154223442,
              -0.004591045901179314,
              -0.008708356879651546,
              -0.003521659644320607,
              -0.007572787348181009,
              -0.002574190264567733,
              0.03550222888588905,
              0.011418397538363934,
              0.0342482253909111,
              -0.0024975568521767855,
              0.0014272996922954917,
              -0.011919998563826084,
              0.008631723001599312,
              0.017305247485637665,
              -0.012303167022764683,
              -0.01918625272810459,
              0.018823985010385513,
              0.009565259329974651,
              -0.006287433672696352,
              0.003291758941486478,
              0.00914725847542286,
              -0.04558999091386795,
              0.0003217737830709666,
              0.015229174867272377,
              0.0004941992810927331,
              -0.015284908004105091,
              0.00554199842736125,
              0.012338000349700451,
              0.0033718757331371307,
              0.011188496835529804,
              -0.002103938953951001,
              0.019757520407438278,
              -0.017444580793380737,
              -0.012247432954609394,
              -0.028842078521847725,
              0.002976516494527459,
              -0.0024052481167018414,
              0.0001037927286233753,
              -0.01738884672522545,
              0.017193779349327087,
              -0.01478330697864294,
              -0.0036505432799458504,
              0.0033718757331371307,
              0.03199101984500885,
              -0.028842078521847725,
              -0.042050912976264954,
              0.004918479826301336,
              0.011376597918570042,
              0.013125235214829445,
              -0.0074613201431930065,
              0.03112715110182762,
              -0.03722996637225151,
              0.014246871694922447,
              -0.020565655082464218,
              0.012351933866739273,
              -0.005886849481612444,
              0.02675207331776619,
              0.03600383177399635,
              -0.007475253660231829,
              0.0011259905295446515,
              -0.025707069784402847,
              0.0035303679760545492,
              -0.005911232437938452,
              -0.010401261039078236,
              -0.0035390763077884912,
              -0.0003302644472569227,
              -0.014212038367986679,
              -0.04040677472949028,
              0.011843365617096424,
              0.01065902877599001,
              0.01631597802042961,
              -0.003516434459015727,
              0.02599967084825039,
              0.01843385025858879,
              0.015521774999797344,
              -0.02886994555592537,
              0.01868464983999729,
              0.012045399285852909,
              0.0024522733874619007,
              0.006210800260305405,
              0.020844323560595512,
              0.02886994555592537,
              -0.011495031416416168,
              0.02172212488949299,
              0.010213160887360573,
              -0.010213160887360573,
              -0.002133547328412533,
              0.01571684330701828,
              0.0012836117530241609,
              -0.007726054172962904,
              0.01097949594259262,
              -0.0035094679333269596,
              -0.02130412496626377,
              -0.008144055493175983,
              -0.00489758001640439,
              -0.012553967535495758,
              -0.028591277077794075,
              -0.015647174790501595,
              -0.021931126713752747,
              -0.0011024780105799437,
              0.0001773239200701937,
              -0.02266959473490715,
              0.011599531397223473,
              -0.018475649878382683,
              -0.02528906799852848,
              -0.017347047105431557,
              -0.023394130170345306,
              -0.014616106636822224,
              -0.011655264534056187,
              -0.032213952392339706,
              -0.008617790415883064,
              -0.029092878103256226,
              0.006754201836884022,
              0.02534480206668377,
              -0.027588075026869774,
              -0.017765048891305923,
              -0.009676726534962654,
              -0.013877637684345245,
              0.0418279804289341,
              0.030792750418186188,
              0.0045875622890889645,
              -0.016190577298402786,
              0.013034668751060963,
              -0.03792663663625717,
              0.0009213441517204046,
              0.02201472595334053,
              0.0011190238874405622,
              0.01848958246409893,
              -0.02605540305376053,
              0.009906627237796783,
              -0.00020399325876496732,
              -0.032576221972703934,
              -0.009857860393822193,
              0.040573976933956146,
              -0.002218889305368066,
              0.006458117626607418,
              0.012233499437570572,
              0.0017199005233123899,
              0.003911793697625399,
              -0.0036261598579585552,
              -0.021025456488132477,
              0.015842242166399956,
              0.011237263679504395,
              0.019534585997462273,
              -0.017904382199048996,
              0.01627417653799057,
              -0.004322828259319067,
              0.02237699367105961,
              0.007948988117277622,
              0.027615942060947418,
              -0.014532506465911865,
              0.018754318356513977,
              0.025163667276501656,
              0.009850893169641495,
              -0.0022711395286023617,
              -0.0011756281601265073,
              -0.006064499728381634,
              -0.00982302613556385,
              -0.0011277322191745043,
              -0.03553009778261185,
              0.023338396102190018,
              0.0015709876315668225,
              0.02220979332923889,
              -0.02906501293182373,
              -0.033217158168554306,
              -0.01804371550679207,
              -0.008206755854189396,
              0.0012374575017020106,
              -0.009084558114409447,
              0.03265982121229172,
              -0.0279642753303051,
              0.00955132581293583,
              -0.006064499728381634,
              -0.012909268029034138,
              0.038706906139850616,
              0.012254400178790092,
              0.006893535144627094,
              0.006043599918484688,
              -0.0029660663567483425,
              -0.00960009265691042,
              0.015521774999797344,
              -0.020342722535133362,
              0.019228052347898483,
              0.02076072245836258,
              0.029956746846437454,
              -0.0038665104657411575,
              -0.015424241311848164,
              -0.010533628053963184,
              0.02630620449781418,
              0.009356258437037468,
              0.03279915452003479,
              0.003523401217535138,
              -0.012156866490840912,
              0.0030531499069184065,
              -0.004110344685614109,
              -0.0040406775660812855,
              0.053977880626916885,
              -0.01005292683839798,
              0.01995258778333664,
              0.030820617452263832,
              -0.01141143124550581,
              -0.0074404203332960606,
              0.017263446003198624,
              -0.011947865597903728,
              -0.017360979691147804,
              0.007551887072622776,
              0.017207713797688484,
              0.0009274400072172284,
              0.024801399558782578,
              0.0003629207785706967,
              -0.011195464059710503,
              0.010011127218604088,
              0.004267094656825066,
              -0.01249823346734047,
              0.02024518884718418,
              0.01787651516497135,
              0.023561330512166023,
              -0.014643972739577293,
              -0.030151814222335815,
              -0.028298676013946533,
              -0.012017532251775265,
              0.00023577875981573015,
              -0.017514247447252274,
              -0.004256644751876593,
              -0.010247994214296341,
              0.006573067978024483,
              0.003876960603520274,
              0.004364628344774246,
              -0.02363099716603756,
              0.01733311451971531,
              0.005022980272769928,
              0.02181965857744217,
              -0.006931852083653212,
              -0.020886123180389404,
              0.028131475672125816,
              -0.0040580942295491695,
              -0.011146697215735912,
              -0.015438174828886986,
              -0.019910788163542747,
              -0.010617229156196117,
              -0.028075743466615677,
              0.017597848549485207,
              0.008882524445652962,
              0.0077887545339763165,
              -0.020273054018616676,
              -0.003695826744660735,
              -0.002946907887235284,
              0.0015431208303198218,
              0.008861623704433441,
              -0.01118153054267168,
              0.013076468370854855,
              0.01125816348940134,
              -0.012045399285852909,
              0.005364347714930773,
              -0.016761844977736473,
              0.014448905363678932,
              0.049909334629774094,
              -0.006653184536844492,
              -0.02172212488949299,
              -0.01985505409538746,
              -0.018768250942230225,
              -0.018670717254281044,
              0.02332446351647377,
              0.010387328453361988,
              -0.019116584211587906,
              0.01153683103621006,
              0.024759599938988686,
              0.02372853085398674,
              -0.009425925090909004,
              0.0016868087695911527,
              0.011919998563826084,
              -0.0067054349929094315,
              -0.017402781173586845,
              -0.03753650188446045,
              -0.029455145820975304,
              0.0005564640159718692,
              0.025804603472352028,
              0.06325750797986984,
              0.0035094679333269596,
              0.0036993101239204407,
              -0.019464919343590736,
              0.0037480767350643873,
              -0.018015848472714424,
              -0.01592584326863289,
              0.0011303446954116225,
              -0.015633242204785347,
              -0.021485257893800735,
              -0.00836002267897129,
              0.0035652013029903173,
              0.0056499820202589035,
              0.02126232348382473,
              0.0009013149538077414,
              0.0013184451963752508,
              -0.05768415704369545,
              0.02559560351073742,
              -0.000957048439886421,
              0.003974494058638811,
              -0.014337439090013504,
              0.020816456526517868,
              0.03461049497127533,
              0.005099613685160875,
              -0.01752818003296852,
              0.01899118535220623,
              0.02509400062263012,
              0.018712516874074936,
              -0.04010024294257164,
              0.0060157328844070435,
              0.006917918566614389,
              -0.008798924274742603,
              0.00041408237302675843,
              -0.013139168731868267,
              0.022599928081035614,
              0.01708231307566166,
              0.006802968215197325,
              0.03380235657095909,
              0.0037202101666480303,
              -0.003915277309715748,
              0.012066299095749855,
              0.008645656518638134,
              -0.006245633587241173,
              -0.0037027932703495026,
              0.00836002267897129,
              -0.0035808763932436705,
              0.018503516912460327,
              0.02282286249101162,
              0.0013524078531190753,
              0.007238386198878288,
              -0.014379238709807396,
              -0.01783471554517746,
              -0.004451712127774954,
              -0.0074891867116093636,
              -0.007384686730802059,
              -0.011982698924839497,
              -0.009405025281012058,
              -0.025261200964450836,
              -0.023254796862602234,
              -0.003335300600156188,
              -0.00876409001648426,
              0.018475649878382683,
              -0.01139053050428629,
              0.009614026173949242,
              -0.005716165527701378,
              -0.003869993845000863,
              -0.017165912315249443,
              -0.013362103141844273,
              -0.0016040793852880597,
              -0.02634800411760807,
              -0.006130683235824108,
              -0.04840452969074249,
              -0.020328788086771965,
              0.023686731234192848,
              0.024397332221269608,
              0.029148612171411514,
              -0.013313336297869682,
              0.005172763951122761,
              -0.00545143149793148,
              -0.010059894062578678,
              -0.008018654771149158,
              0.016538910567760468,
              -0.005611665081232786,
              -0.016455311328172684,
              0.02609720453619957,
              -0.034136757254600525,
              -0.026111137121915817,
              -0.01950671896338463,
              0.02161065861582756,
              -0.011550764553248882,
              0.01779291406273842,
              -0.01468577329069376,
              -0.022237660363316536,
              -0.0067054349929094315,
              0.029287945479154587,
              -0.004845329560339451,
              0.02746267430484295,
              -0.0020969724282622337,
              -0.02605540305376053,
              0.020872190594673157,
              -0.03722996637225151,
              0.0004397720331326127,
              -0.03472195938229561,
              -0.012888368219137192,
              -0.003988427575677633,
              -0.0021770892199128866,
              0.012331033125519753,
              -0.014072705060243607,
              -0.003828193759545684,
              -0.0120941661298275,
              -0.01315310224890709,
              -0.01251913420855999,
              -0.0035025011748075485,
              0.046119458973407745,
              0.013055568560957909,
              0.004500478971749544,
              -0.014797240495681763,
              0.009202991612255573,
              -0.020523855462670326,
              -0.006771618500351906,
              -0.015340641140937805,
              0.02553986944258213,
              0.00311759184114635,
              -0.014866907149553299,
              0.03867903724312782,
              -0.01282566785812378,
              -0.011926965788006783,
              -0.015438174828886986,
              0.035335030406713486,
              -0.0234359297901392,
              0.045617856085300446,
              0.0035460428334772587,
              0.0027309407014399767,
              -0.017500314861536026,
              0.008276422508060932,
              -0.005280747544020414,
              -0.010895895771682262,
              -0.03148941695690155,
              0.018614983186125755,
              0.026361938565969467,
              -0.012303167022764683,
              -0.003533851122483611,
              -0.022530261427164078,
              -0.01612091064453125,
              -0.020091921091079712,
              -0.024271931499242783,
              0.04508838802576065,
              -0.0038978606462478638,
              0.003025283105671406,
              0.007168719545006752,
              0.009363225661218166,
              -0.0028371827211230993,
              -0.0002782320079859346,
              -0.037257835268974304,
              -0.020426321774721146,
              0.0015509583754464984,
              0.0397937074303627,
              0.0193952526897192,
              0.024620266631245613,
              0.04701119288802147,
              -0.011947865597903728,
              0.018127316609025,
              0.020217321813106537,
              -0.0008625627378933132,
              -0.0031837753485888243,
              -0.030346881598234177,
              -0.008234621956944466,
              -0.015995509922504425,
              0.014191138558089733,
              0.007067702244967222,
              -0.0076424540020525455,
              0.006956235505640507,
              0.01985505409538746,
              0.015006240457296371,
              0.011077029630541801,
              -0.003838643664494157,
              -0.0021370307076722383,
              0.0010876738233491778,
              -0.019172318279743195,
              -0.01256790105253458,
              0.01869858428835869,
              -0.018308449536561966,
              0.00748222041875124,
              -0.03380235657095909,
              -0.004695545881986618,
              0.0031559085473418236,
              -0.018865784630179405,
              0.011940899305045605,
              -0.00720355287194252,
              0.019074784591794014,
              -0.019269851967692375,
              0.010282827541232109,
              -0.016998711973428726,
              0.013821904547512531,
              -0.013229736126959324,
              -0.010888929478824139,
              -0.023463796824216843,
              -0.001105090486817062,
              0.01677577942609787,
              0.019966520369052887,
              -0.011620431207120419,
              0.0018583633936941624,
              -0.030514081940054893,
              0.015326708555221558,
              -0.008137088268995285,
              -0.029148612171411514,
              -0.003768976777791977,
              0.001748638111166656,
              0.007879321463406086,
              -0.016817579045891762,
              -0.04946346580982208,
              -0.034638360142707825,
              -0.013362103141844273,
              -0.009634925983846188,
              -0.001558795920573175,
              -0.016399577260017395,
              0.015298841521143913,
              -0.011042196303606033,
              -0.04135424643754959,
              -0.004496995359659195,
              -0.0026560488622635603,
              0.03023541532456875,
              0.022335194051265717,
              0.005200630519539118,
              0.012985901907086372,
              0.030904216691851616,
              0.013389969244599342,
              -0.011007362976670265,
              0.00028258617385290563,
              -0.019019052386283875,
              0.009920559823513031,
              0.011188496835529804,
              -0.023867864161729813,
              -0.0428311824798584,
              0.0299846138805151,
              0.027393007650971413,
              -0.017667515203356743,
              -0.029343679547309875,
              0.015856176614761353,
              -0.018782183527946472,
              -0.001386370393447578,
              0.00997629389166832,
              -0.011425363831222057,
              -0.021122990176081657,
              -0.009035791270434856,
              -0.012170800007879734,
              0.014295638538897038,
              0.01468577329069376,
              -0.005298164207488298,
              -0.017848648130893707,
              0.013013768941164017,
              -0.020119788125157356,
              0.005218047648668289,
              0.010881963185966015,
              -0.010847128927707672,
              0.024007197469472885,
              0.01471363939344883,
              0.022711394354701042,
              0.014267771504819393,
              0.03304995596408844,
              0.026334071531891823,
              -0.030709149315953255,
              0.018378116190433502,
              -0.013452669605612755,
              -0.003040958195924759,
              -0.007224452681839466,
              0.005360864568501711,
              0.004469128791242838,
              0.0304862167686224,
              -0.022711394354701042,
              -0.00927265826612711,
              0.004099894315004349,
              -0.009767292998731136,
              0.017653580754995346,
              0.008729256689548492,
              -0.00025994444149546325,
              0.004911513067781925,
              -0.011522897519171238,
              0.007928088307380676,
              0.034638360142707825,
              0.006123716477304697,
              -0.02176392637193203,
              -0.0007519666105508804,
              -0.0016493628500029445,
              0.0037445933558046818,
              -0.02690533921122551,
              0.009453792124986649,
              0.029510879889130592,
              -0.0418279804289341,
              -0.00914029125124216,
              0.027518408372998238,
              0.011334797367453575,
              0.011049163527786732,
              -0.029148612171411514,
              0.011090963147580624,
              0.013167035765945911,
              0.021025456488132477,
              -0.013410869985818863,
              -0.010206194594502449,
              -0.007503120228648186,
              0.015326708555221558,
              -0.03006821498274803,
              -0.017165912315249443,
              -0.019924720749258995,
              -0.011857299134135246,
              0.0158701092004776,
              0.021039390936493874,
              0.007565820589661598,
              -0.007872354239225388,
              0.010540595278143883,
              -0.029343679547309875,
              0.010241027921438217,
              0.009753359481692314,
              -0.004260128363966942,
              -0.00318725872784853,
              0.014769373461604118,
              -0.0038665104657411575,
              0.010450027883052826,
              0.0176257137209177,
              0.020133720710873604,
              -0.0065626176074147224,
              0.01000415999442339,
              -0.030597683042287827,
              -0.015897976234555244,
              -0.007955954410135746,
              0.004124277736991644,
              -0.00038055519689805806,
              -0.02800607495009899,
              0.006447667255997658,
              0.004775662906467915,
              0.00568133220076561,
              -0.0022397893480956554,
              -0.015354574657976627,
              -0.005329514387995005,
              0.002267656149342656,
              -0.011676165275275707,
              0.00816495530307293,
              0.016566777601838112,
              -0.023310529068112373,
              0.037007033824920654,
              0.0067054349929094315,
              0.04951919987797737,
              0.0018670717254281044,
              -0.01606517657637596,
              -0.03491702675819397,
              -0.026125071570277214,
              -0.017556047067046165,
              -0.026877472177147865,
              -0.03304995596408844,
              -0.025484135374426842,
              0.02790854312479496,
              0.019562453031539917,
              -0.027128273621201515,
              -0.03260408714413643,
              -0.018196983262896538,
              -0.019659986719489098,
              0.00997629389166832,
              0.01646924391388893,
              0.01779291406273842,
              0.010575428605079651,
              0.01617664471268654,
              -0.02841014415025711,
              0.03714636713266373,
              0.010087761096656322,
              -0.0006901372689753771,
              -0.013550203293561935,
              0.012581833638250828,
              0.011397497728466988,
              -0.01612091064453125,
              0.004803529474884272,
              0.02473173290491104,
              -0.04751279577612877,
              -0.0007872354472056031,
              0.03784303739666939,
              -0.006451150868088007,
              0.02403506450355053,
              0.03918064013123512,
              -0.019715720787644386,
              -0.03480556234717369,
              -0.01430957205593586,
              0.018322382122278214,
              0.033022090792655945,
              0.01692904531955719,
              0.003636609762907028,
              -0.0019855054561048746,
              -0.01342480257153511,
              0.0052772643975913525,
              0.013717403635382652,
              -0.026724206283688545,
              0.0027675158344209194,
              0.0054792980663478374,
              0.0037724601570516825,
              -0.02397933229804039,
              -0.017221646383404732,
              0.011676165275275707,
              0.012059332802891731,
              -0.014978373423218727,
              0.020523855462670326,
              -0.004420361947268248,
              -0.02967808023095131,
              0.0049393801018595695,
              0.02679387293756008,
              0.0015971126267686486,
              -0.020900055766105652,
              0.0110839968547225,
              -0.027936408296227455,
              0.0032029335852712393,
              0.02161065861582756,
              0.012958034873008728,
              -0.0052842311561107635,
              -0.002953874645754695,
              0.005691782105714083,
              -0.02573493681848049,
              -0.00224153115414083,
              -0.007691220846027136,
              0.0058694323524832726,
              -0.018461717292666435,
              0.04625879228115082,
              0.02917647920548916,
              -0.05105186998844147,
              0.017458513379096985,
              -0.013780103996396065,
              -0.008833757601678371,
              -0.019032984972000122,
              0.0032691170927137136,
              0.005033430177718401,
              -0.021136924624443054,
              0.02101152390241623,
              0.0037480767350643873,
              -0.03505636006593704,
              -0.0019611220341175795,
              -0.0012052366510033607,
              0.008680489845573902,
              -0.01047789491713047,
              0.02574886940419674,
              0.2039845585823059,
              -0.03405315801501274,
              0.01833631657063961,
              0.0029573580250144005,
              0.013780103996396065,
              0.0176257137209177,
              0.0037550434935837984,
              0.017709314823150635,
              -0.00217186426743865,
              -0.001844429993070662,
              0.00048462007544003427,
              0.008060455322265625,
              -0.013222768902778625,
              0.0024365982972085476,
              0.0224187932908535,
              0.0010824487544596195,
              -0.012066299095749855,
              -0.046175193041563034,
              -0.00687611848115921,
              -0.0056778485886752605,
              -0.005225013941526413,
              0.014699706807732582,
              -0.003213383723050356,
              -0.036505430936813354,
              0.014351372607052326,
              -0.012916235253214836,
              -0.006691501475870609,
              -0.026919273659586906,
              0.006419800687581301,
              0.03472195938229561,
              -0.007377719972282648,
              0.01858711615204811,
              -0.008241589181125164,
              0.015479975380003452,
              -0.009690659120678902,
              -0.020384522154927254,
              -0.02272532880306244,
              0.02222372591495514,
              0.011056129820644855,
              -0.009425925090909004,
              0.035084228962659836,
              0.010401261039078236,
              -0.008025621995329857,
              -0.012331033125519753,
              -0.00031154145835898817,
              -0.00023947982117533684,
              0.000322644627885893,
              -0.013814937323331833,
              -0.002095230622217059,
              0.03023541532456875,
              -0.010512728244066238,
              0.02654307149350643,
              0.039124906063079834,
              0.042691849172115326,
              0.0030461831483989954,
              -0.014267771504819393,
              0.002694365568459034,
              0.00595651613548398,
              0.005886849481612444,
              0.01949278637766838,
              -0.025762803852558136,
              0.0186428502202034,
              0.002546323463320732,
              0.03112715110182762,
              0.03179595246911049,
              0.015856176614761353,
              -0.019882921129465103,
              0.012393733486533165,
              0.015730775892734528,
              -0.004723412916064262,
              -0.015382441692054272,
              -0.016817579045891762,
              -0.005925165954977274,
              -0.004653745796531439,
              -0.03502849489450455,
              -0.02126232348382473,
              0.051414139568805695,
              0.01818304881453514,
              0.009565259329974651,
              0.034081026911735535,
              -0.018754318356513977,
              -0.005834599025547504,
              0.011369630694389343,
              -0.014894773252308369,
              -0.026779938489198685,
              -0.043054115027189255,
              0.017305247485637665,
              -0.01395427156239748,
              -0.017946181818842888,
              -0.016483178362250328,
              0.0023791231215000153,
              -0.021081190556287766,
              -0.008959157392382622,
              0.008617790415883064,
              -0.0004689450142905116,
              -0.010735662654042244,
              -0.019172318279743195,
              0.03940357267856598,
              -0.0014569080667570233,
              -0.021596726030111313,
              -0.03452689200639725,
              0.04703906178474426,
              0.03842823579907417,
              0.02644553780555725,
              -0.0008155376417562366,
              0.004946346860378981,
              -0.01019922737032175,
              -0.008157989010214806,
              0.022502394393086433,
              -0.0013027702225372195,
              0.001785213127732277,
              -0.012386767193675041,
              0.0019053885480389,
              0.007997754961252213,
              -0.0018931968370452523,
              -0.011327831074595451,
              0.005221530795097351,
              -0.014393172226846218,
              0.012637567706406116,
              -0.005221530795097351,
              0.0005899911629967391,
              -0.03388595953583717,
              -0.039821576327085495,
              0.017946181818842888,
              -0.0014830331783741713,
              -0.004685095977038145,
              -0.016413511708378792,
              -0.02619473822414875,
              -0.01430957205593586,
              -0.007405586540699005,
              0.011327831074595451,
              -0.03265982121229172,
              0.03530716150999069,
              -0.021582791581749916,
              -0.02906501293182373,
              0.00651036761701107,
              0.01950671896338463,
              -0.017542114481329918,
              -0.01779291406273842,
              0.009941460564732552,
              0.009460759349167347,
              -5.8073856052942574e-05,
              0.006141133140772581,
              -0.006984102074056864,
              0.01627417653799057,
              -0.0468439944088459,
              0.0076215541921556,
              -0.014434972777962685,
              -0.0014351372374221683,
              -0.03625463321805,
              -0.01478330697864294,
              -0.007649420760571957,
              0.0015466042095795274,
              -0.019311651587486267,
              0.023463796824216843,
              -0.002380864694714546,
              -0.018670717254281044,
              -0.015159507282078266,
              -0.013264569453895092,
              0.0021109057124704123,
              -0.028786344453692436,
              -0.015591442584991455,
              -0.0022641727700829506,
              -0.025219401344656944,
              -0.009077590890228748,
              -0.009579192847013474,
              -0.17745541036128998,
              0.010847128927707672,
              0.02076072245836258,
              -0.032074619084596634,
              -0.005552448332309723,
              0.02711433917284012,
              0.032269686460494995,
              -0.009850893169641495,
              -0.020482055842876434,
              0.003523401217535138,
              -0.0010632904013618827,
              -0.017012646421790123,
              -0.02856341004371643,
              -0.0007136498461477458,
              0.004350695293396711,
              0.01306950207799673,
              0.0018235299503430724,
              0.02045418880879879,
              0.024898933246731758,
              0.02825687639415264,
              0.03143368661403656,
              -0.044614654034376144,
              0.013536269776523113,
              -0.007321986369788647,
              0.02064925618469715,
              -0.009767292998731136,
              -0.010171361267566681,
              0.03137795254588127,
              -0.017151979729533195,
              -0.014671839773654938,
              0.012128999456763268,
              0.02725367434322834,
              0.012101132422685623,
              -0.004552728962153196,
              0.00025515485322102904,
              0.020886123180389404,
              0.01123029738664627,
              -0.01334816962480545,
              -0.0011651781387627125,
              -0.008415755815804005,
              0.05252880975604057,
              -0.00992752704769373,
              -0.0076145874336361885,
              -0.014121471904218197,
              -0.023644929751753807,
              -0.011097930371761322,
              0.007670320570468903,
              -0.0022746229078620672,
              0.016009442508220673,
              0.018364183604717255,
              -0.01748638041317463,
              -0.020384522154927254,
              0.020635321736335754,
              0.006398900877684355,
              0.007005002349615097,
              0.02679387293756008,
              0.011662231758236885,
              0.00821372214704752,
              -0.026166871190071106,
              0.015535708516836166,
              0.004977697040885687,
              -0.033022090792655945,
              0.026013603433966637,
              0.002319906372576952,
              -0.014978373423218727,
              -0.0016868087695911527,
              0.0018479133723303676,
              0.004333278629928827,
              -0.021986858919262886,
              0.011202430352568626,
              -0.03647756576538086,
              -0.023463796824216843,
              0.0003302644472569227,
              0.0047408295795321465,
              0.022599928081035614,
              -0.00012757742661051452,
              -0.019367385655641556,
              -0.0012609701370820403,
              0.001722512999549508,
              -0.01123029738664627,
              -0.033022090792655945,
              0.0317402184009552,
              -0.0096976263448596,
              0.026975005865097046,
              0.0032081587705761194,
              -0.012087199836969376,
              0.005521098151803017,
              0.018322382122278214,
              -0.019715720787644386,
              -0.026431605219841003,
              0.011432331055402756,
              0.0060226996429264545,
              0.017542114481329918,
              0.0019785386975854635,
              0.007412553299218416,
              0.01779291406273842,
              0.010220128111541271,
              -0.015284908004105091,
              0.022181926295161247,
              -0.012853534892201424,
              -0.02105332352221012,
              -0.017347047105431557,
              0.006381483748555183,
              0.009307491593062878,
              0.012783868238329887,
              0.02533086948096752,
              -0.019618187099695206,
              0.006694984622299671,
              0.029037145897746086,
              0.011481097899377346,
              -0.006043599918484688,
              -0.010833196341991425,
              0.00023664960463065654,
              0.02629227191209793,
              0.017820781096816063,
              0.009133324958384037,
              -0.03193528577685356,
              -0.0027918992564082146,
              0.005256364122033119,
              -0.02599967084825039,
              0.043499983847141266,
              -0.010603295639157295,
              0.003612226340919733,
              0.014978373423218727,
              -0.0007920250645838678,
              -0.01804371550679207,
              -0.11425364017486572,
              -0.008206755854189396,
              -0.002555032027885318,
              0.004354178439825773,
              -0.014490705914795399,
              -0.004120794590562582,
              0.02172212488949299,
              0.029483012855052948,
              -0.026027537882328033,
              0.010624195449054241,
              -0.020579589530825615,
              -0.02711433917284012,
              0.013090401887893677,
              0.004803529474884272,
              -0.005319064483046532,
              0.008185855112969875,
              0.0033701341599226,
              -0.0047582462430000305,
              -0.009251758456230164,
              0.026682404801249504,
              -0.0026473405305296183,
              -0.01959032006561756,
              0.014393172226846218,
              -0.024048998951911926,
              0.015842242166399956,
              0.00677858479321003,
              -0.029705947265028954,
              0.0015788250602781773,
              0.021889325231313705,
              0.011028262786567211,
              0.007398620247840881,
              -0.03455476090312004,
              -0.00881285686045885,
              -0.003410192672163248,
              -0.0019837636500597,
              -0.003326592268422246,
              -0.012909268029034138,
              -0.022558128461241722,
              0.005782349035143852,
              -0.03405315801501274,
              0.0029051078017801046,
              0.013292435556650162,
              -0.004169561434537172,
              -0.010157427750527859,
              -0.010136527009308338,
              -0.00012942795001436025,
              -0.014434972777962685,
              0.030151814222335815,
              0.023770330473780632,
              -0.00803258828818798,
              -0.012198666110634804,
              -0.02045418880879879,
              -0.02499646693468094,
              -0.005367831327021122,
              0.020941857248544693,
              0.033133555203676224,
              0.0186428502202034,
              0.013598970137536526,
              -0.004517895635217428,
              -0.012965002097189426,
              -0.000530774355866015,
              -0.005092646926641464,
              -0.01698477938771248,
              0.005144897382706404,
              0.018628917634487152,
              -0.001567504252307117,
              -0.05473028123378754,
              -0.011167597025632858,
              -0.008673523552715778,
              -0.005350414663553238,
              -0.014156305231153965,
              0.01793224923312664,
              0.013194902800023556,
              0.02155492454767227,
              -0.01858711615204811,
              -0.01012259442359209,
              -0.014281705021858215,
              -0.0021091641392558813,
              0.0022066975943744183,
              -0.018726451322436333,
              -0.01090286299586296,
              -0.0274487417191267,
              0.007698187604546547,
              -0.002624698681756854,
              -0.010220128111541271,
              -0.012804768048226833,
              -0.0012182991486042738,
              -0.02499646693468094,
              -0.011745831929147243,
              -0.029761681333184242,
              0.009662793017923832,
              0.008645656518638134,
              -0.029761681333184242,
              -0.008234621956944466,
              -0.004180011339485645,
              -0.0024644650984555483,
              -0.0031994504388421774,
              -0.01390550471842289,
              0.020217321813106537,
              0.005987866315990686,
              -0.015075907111167908,
              0.0043750787153840065,
              -0.04391798377037048,
              0.016162710264325142,
              0.0032952423207461834,
              -0.01698477938771248,
              0.011655264534056187,
              0.018823985010385513,
              0.03692343458533287,
              -0.017040513455867767,
              0.002717007417231798,
              0.008854657411575317,
              -0.007363786455243826,
              0.021485257893800735,
              -0.01738884672522545,
              -0.018977250903844833,
              -0.006447667255997658,
              -0.010993429459631443,
              0.005925165954977274,
              -0.004507445730268955,
              0.029204346239566803,
              0.005803248845040798,
              -0.01499230694025755,
              -0.016538910567760468,
              0.009572225622832775,
              -0.00025798508431762457,
              -0.007774821016937494,
              0.013828870840370655,
              -0.010234060697257519,
              0.026320137083530426,
              -0.026612738147377968,
              -0.011780665256083012,
              0.020077988505363464,
              -0.02403506450355053,
              -0.0008969607879407704,
              0.030040347948670387,
              -0.0010589362354949117,
              0.016664311289787292,
              0.02423013187944889,
              0.008596889674663544,
              0.03622676432132721,
              0.004639812745153904,
              -0.014755439944565296,
              -0.03600383177399635,
              0.0020307889208197594,
              -0.012540034018456936,
              -0.00260728201828897,
              0.022251592949032784,
              -0.0027222323697060347,
              0.003500759368762374,
              0.015479975380003452,
              -0.006398900877684355,
              0.008778023533523083,
              0.02091399021446705,
              0.019074784591794014,
              -0.029009278863668442,
              -0.015953710302710533,
              -0.00849238969385624,
              -0.0035286261700093746,
              0.02278106100857258,
              0.002276364481076598,
              -0.0034415426198393106,
              0.0367841012775898,
              -0.015048041008412838,
              0.022279459983110428,
              0.003981460817158222,
              0.0076424540020525455,
              -0.008750157430768013,
              0.012428566813468933,
              0.007039835676550865,
              -0.02912074513733387,
              -0.02827080897986889,
              -0.010568462312221527,
              0.009251758456230164,
              -0.011773698963224888,
              0.02565133571624756,
              0.029037145897746086,
              0.005252880975604057,
              -0.0007088602287694812,
              0.0114601980894804,
              -0.005998316220939159,
              0.01478330697864294,
              0.012484300881624222,
              -0.02459239959716797,
              -0.016845446079969406,
              -0.014448905363678932,
              0.024453066289424896,
              0.002779707545414567,
              -0.004451712127774954,
              0.012170800007879734,
              -0.005378281231969595,
              0.02195899188518524,
              0.012184732593595982,
              0.001861846656538546,
              0.009760326705873013,
              0.03555796295404434,
              0.00401629414409399,
              0.01490870676934719,
              0.007349853403866291,
              -0.019269851967692375,
              0.006796001922339201,
              -0.021749991923570633,
              0.006127200089395046,
              0.015828309580683708,
              -0.012463400140404701,
              -0.010094727389514446,
              -0.023157263174653053,
              0.021136924624443054,
              -0.013229736126959324,
              -0.00457362923771143,
              0.003953594248741865,
              0.03770370036363602,
              -0.0011442780960351229,
              0.015298841521143913,
              -0.01219169981777668,
              0.01080532930791378,
              -0.034276094287633896,
              0.02765774168074131,
              -0.007210519630461931,
              -0.036505430936813354,
              -0.026570938527584076,
              0.033022090792655945,
              0.021931126713752747,
              -0.01518737431615591,
              0.044558919966220856,
              -0.012212599627673626,
              0.018559250980615616,
              0.010819262824952602,
              0.024717800319194794,
              -0.011906065978109837,
              0.02638980560004711,
              0.02278106100857258,
              0.017597848549485207,
              -0.008680489845573902,
              0.01989685371518135,
              -0.01874038390815258,
              -0.021136924624443054,
              -0.012310133315622807,
              -0.006607901304960251,
              0.03279915452003479,
              -0.0008817211491987109,
              0.10160214453935623,
              0.021248390898108482,
              -0.0030949499923735857,
              0.012115065939724445,
              -0.015702908858656883,
              0.011871231719851494,
              0.013362103141844273,
              0.018921518698334694,
              -0.011474130675196648,
              -0.0010545820696279407,
              0.006339683663100004,
              -0.015591442584991455,
              -0.013689536601305008,
              -0.02423013187944889,
              0.0035355929285287857,
              -0.015605375170707703,
              0.003765493631362915,
              0.008617790415883064,
              -0.008610823191702366,
              -0.0030200581531971693,
              0.024299798533320427,
              -0.00043737722444348037,
              0.011223330162465572,
              0.01602337695658207,
              -0.013118268921971321,
              -0.009669759310781956,
              0.007018935866653919,
              -0.016455311328172684,
              -0.008673523552715778,
              -0.03112715110182762,
              0.004406428430229425,
              -0.016566777601838112,
              -0.04166077822446823,
              -0.013571103103458881,
              0.02815934270620346,
              0.01097949594259262,
              -0.001757346442900598,
              0.013341202400624752,
              0.014121471904218197,
              5.785614484921098e-05,
              0.009293558076024055,
              0.036449696868658066,
              -0.0029138161335140467,
              -0.00964885950088501,
              -0.013647736981511116,
              -0.007997754961252213,
              0.0014725831570103765,
              -0.013731337152421474,
              -0.02312939614057541
            ],
            "existingAnswer": false,
            "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
            "indexType": "cogsearchvs",
            "jsonAnswer": null,
            "llm": {
              "model": "gpt-3.5-turbo",
              "request_timeout": null,
              "max_tokens": 1000,
              "stream": false,
              "n": 1,
              "temperature": 0.3,
              "engine": "chat16k",
              "_type": "azure-openai-chat"
            },
            "modifiedAnswer": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B.",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            },
            "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
            "question": "How are sentence pairs represented in the input to BERT?",
            "retrievedDocs": [
              {
                "page_content": "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.",
                "metadata": {
                  "id": "Bert_pdf-9",
                  "source": "Bert.pdf"
                }
              },
              {
                "page_content": "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "metadata": {
                  "id": "Bert_pdf-2",
                  "source": "Bert.pdf"
                }
              },
              {
                "page_content": "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
                "metadata": {
                  "id": "Bert_pdf-1",
                  "source": "Bert.pdf"
                }
              }
            ]
          },
          "output": {
            "values": [
              {
                "recordId": 0,
                "data": {
                  "data_points": [
                    "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.",
                    "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                    "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008)."
                  ],
                  "answer": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B.",
                  "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).']<br>        Question: How are sentence pairs represented in the input to BERT?<br>        ",
                  "sources": "\nBert.pdf",
                  "nextQuestions": "<What are the two unsupervised tasks used in pre-training BERT?>\n<What is the difference between the feature-based and fine-tuning approaches for applying pre-trained language representations to downstream tasks?>\n<What is the architecture of BERT and how does it differ from other models like OpenAI GPT?>",
                  "error": ""
                }
              }
            ]
          },
          "start_time": 1695246514.150503,
          "end_time": 1695246516.348566,
          "error": null,
          "children": [
            {
              "name": "openai.api_resources.chat_completion.ChatCompletion.create",
              "type": "LLM",
              "inputs": {
                "messages": [
                  {
                    "role": "user",
                    "content": "\n    Generate three very brief questions that the user would likely ask next.\n    Use double angle brackets to reference the questions, e.g. <What is Azure?>.\n    Try not to repeat questions that have already been asked.  Don't include the context in the answer.\n\n    Return the questions in the following format:\n    <>\n    <>\n    <>\n    \n    ALWAYS return a \"NEXT QUESTIONS\" part in your answer.\n\n    ['A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).']\n    "
                  }
                ],
                "model": "gpt-3.5-turbo",
                "request_timeout": null,
                "max_tokens": 1000,
                "stream": false,
                "n": 1,
                "temperature": 0.3,
                "engine": "chat16k",
                "api_base": "https://dataaiapim.azure-api.net",
                "organization": "",
                "api_type": "azure",
                "api_version": "2023-07-01-preview"
              },
              "output": {
                "id": "chatcmpl-80ufKV6vIWPT5HZwqEvN9eYTrshLx",
                "object": "chat.completion",
                "created": 1695228514,
                "model": "gpt-35-turbo-16k",
                "prompt_filter_results": [
                  {
                    "prompt_index": 0,
                    "content_filter_results": {
                      "hate": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "self_harm": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "sexual": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "violence": {
                        "filtered": false,
                        "severity": "safe"
                      }
                    }
                  }
                ],
                "choices": [
                  {
                    "index": 0,
                    "finish_reason": "stop",
                    "message": {
                      "role": "assistant",
                      "content": "<What are the two unsupervised tasks used in pre-training BERT?>\n<What is the difference between the feature-based and fine-tuning approaches for applying pre-trained language representations to downstream tasks?>\n<What is the architecture of BERT and how does it differ from other models like OpenAI GPT?>"
                    },
                    "content_filter_results": {
                      "hate": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "self_harm": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "sexual": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "violence": {
                        "filtered": false,
                        "severity": "safe"
                      }
                    }
                  }
                ],
                "usage": {
                  "completion_tokens": 62,
                  "prompt_tokens": 6402,
                  "total_tokens": 6464
                }
              },
              "start_time": 1695246514.174635,
              "end_time": 1695246515.807909,
              "error": null,
              "children": null,
              "node_name": null
            }
          ],
          "node_name": "followup_questions"
        }
      ],
      "variant_id": "",
      "name": "",
      "description": "",
      "tags": null,
      "system_metrics": {
        "duration": 7.189956,
        "total_tokens": 12584
      },
      "result": {
        "answer": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B.",
        "context": [
          "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.",
          "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
          "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008)."
        ],
        "output": {
          "values": [
            {
              "recordId": 0,
              "data": {
                "data_points": [
                  "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.",
                  "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                  "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008)."
                ],
                "answer": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B.",
                "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).']<br>        Question: How are sentence pairs represented in the input to BERT?<br>        ",
                "sources": "\nBert.pdf",
                "nextQuestions": "<What are the two unsupervised tasks used in pre-training BERT?>\n<What is the difference between the feature-based and fine-tuning approaches for applying pre-trained language representations to downstream tasks?>\n<What is the architecture of BERT and how does it differ from other models like OpenAI GPT?>",
                "error": ""
              }
            }
          ]
        }
      },
      "upload_metrics": false
    }
  ],
  "node_runs": [
    {
      "node": "parse_postBody",
      "flow_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7",
      "run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_parse_postBody_0",
      "status": "Completed",
      "inputs": {
        "postBody": {
          "values": [
            {
              "data": {
                "approach": "rtr",
                "overrides": {
                  "chainType": "stuff",
                  "deploymentType": "gpt3516k",
                  "embeddingModelType": "azureopenai",
                  "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
                  "searchType": "hybridrerank",
                  "semantic_captions": false,
                  "semantic_ranker": true,
                  "temperature": 0,
                  "tokenLength": 1000,
                  "top": 3
                },
                "text": ""
              },
              "recordId": 0
            }
          ]
        }
      },
      "output": {
        "chainType": "stuff",
        "deploymentType": "gpt3516k",
        "embeddingModelType": "azureopenai",
        "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
        "searchType": "hybridrerank",
        "semantic_captions": false,
        "semantic_ranker": true,
        "temperature": 0,
        "tokenLength": 1000,
        "top": 3
      },
      "metrics": null,
      "error": null,
      "parent_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_0",
      "start_time": "2023-09-20T16:48:29.193627Z",
      "end_time": "2023-09-20T16:48:29.195653Z",
      "index": 0,
      "api_calls": [
        {
          "name": "parseBody",
          "type": "Tool",
          "inputs": {
            "postBody": {
              "values": [
                {
                  "data": {
                    "approach": "rtr",
                    "overrides": {
                      "chainType": "stuff",
                      "deploymentType": "gpt3516k",
                      "embeddingModelType": "azureopenai",
                      "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
                      "searchType": "hybridrerank",
                      "semantic_captions": false,
                      "semantic_ranker": true,
                      "temperature": 0,
                      "tokenLength": 1000,
                      "top": 3
                    },
                    "text": ""
                  },
                  "recordId": 0
                }
              ]
            }
          },
          "output": {
            "chainType": "stuff",
            "deploymentType": "gpt3516k",
            "embeddingModelType": "azureopenai",
            "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
            "searchType": "hybridrerank",
            "semantic_captions": false,
            "semantic_ranker": true,
            "temperature": 0,
            "tokenLength": 1000,
            "top": 3
          },
          "start_time": 1695246509.193627,
          "end_time": 1695246509.195653,
          "error": null,
          "children": null,
          "node_name": "parse_postBody"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "",
        "stderr": ""
      },
      "system_metrics": {
        "duration": 0.002026
      },
      "result": {
        "chainType": "stuff",
        "deploymentType": "gpt3516k",
        "embeddingModelType": "azureopenai",
        "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
        "searchType": "hybridrerank",
        "semantic_captions": false,
        "semantic_ranker": true,
        "temperature": 0,
        "tokenLength": 1000,
        "top": 3
      }
    },
    {
      "node": "create_llm",
      "flow_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7",
      "run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_create_llm_0",
      "status": "Completed",
      "inputs": {
        "conn": "entaoai",
        "overrides": {
          "chainType": "stuff",
          "deploymentType": "gpt3516k",
          "embeddingModelType": "azureopenai",
          "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
          "searchType": "hybridrerank",
          "semantic_captions": false,
          "semantic_ranker": true,
          "temperature": 0,
          "tokenLength": 1000,
          "top": 3
        }
      },
      "output": "AzureChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.3, model_kwargs={}, openai_api_key='1361532858cb428c8da412a73105de78', openai_api_base='https://dataaiapim.azure-api.net', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=1000, tiktoken_model_name=None, deployment_name='chat16k', model_version='', openai_api_type='azure', openai_api_version='2023-07-01-preview')",
      "metrics": null,
      "error": null,
      "parent_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_0",
      "start_time": "2023-09-20T16:48:29.201186Z",
      "end_time": "2023-09-20T16:48:29.206186Z",
      "index": 0,
      "api_calls": [
        {
          "name": "createLlm",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            }
          },
          "output": {
            "model": "gpt-3.5-turbo",
            "request_timeout": null,
            "max_tokens": 1000,
            "stream": false,
            "n": 1,
            "temperature": 0.3,
            "engine": "chat16k",
            "_type": "azure-openai-chat"
          },
          "start_time": 1695246509.202186,
          "end_time": 1695246509.203201,
          "error": null,
          "children": null,
          "node_name": "create_llm"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "",
        "stderr": ""
      },
      "system_metrics": {
        "duration": 0.005
      },
      "result": "AzureChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.3, model_kwargs={}, openai_api_key='1361532858cb428c8da412a73105de78', openai_api_base='https://dataaiapim.azure-api.net', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=1000, tiktoken_model_name=None, deployment_name='chat16k', model_version='', openai_api_type='azure', openai_api_version='2023-07-01-preview')"
    },
    {
      "node": "embed_the_question",
      "flow_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7",
      "run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_embed_the_question_0",
      "status": "Completed",
      "inputs": {
        "conn": "entaoai",
        "overrides": {
          "chainType": "stuff",
          "deploymentType": "gpt3516k",
          "embeddingModelType": "azureopenai",
          "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
          "searchType": "hybridrerank",
          "semantic_captions": false,
          "semantic_ranker": true,
          "temperature": 0,
          "tokenLength": 1000,
          "top": 3
        },
        "question": "How are sentence pairs represented in the input to BERT?"
      },
      "output": [
        -0.011118830181658268,
        -0.01631597802042961,
        0.02559560351073742,
        -0.002060397295281291,
        -0.005500198341906071,
        0.031210750341415405,
        0.015424241311848164,
        -0.011599531397223473,
        -0.01773718185722828,
        -0.027727408334612846,
        0.028535543009638786,
        0.04366718605160713,
        -0.035139963030815125,
        0.005427048075944185,
        -0.0009474692051298916,
        0.008422723039984703,
        0.011244230903685093,
        0.009614026173949242,
        0.027797074988484383,
        -0.016956912353634834,
        -0.0016798420110717416,
        -0.006102816667407751,
        -0.016552845016121864,
        -0.0035739096347242594,
        -0.010742628946900368,
        -0.00849238969385624,
        0.029538746923208237,
        -0.020328788086771965,
        -0.003713243408128619,
        -0.009676726534962654,
        0.0021788307931274176,
        0.02357526309788227,
        -0.017918314784765244,
        -0.009948426857590675,
        -0.009258724749088287,
        0.006694984622299671,
        -0.0029294912237674,
        -0.016915112733840942,
        0.02438339963555336,
        -0.023212995380163193,
        0.0071164690889418125,
        0.021694257855415344,
        0.003977977205067873,
        -0.015270974487066269,
        -0.005953032989054918,
        0.01672004535794258,
        0.010652062483131886,
        -0.0065695843659341335,
        -0.012059332802891731,
        0.0093214251101017,
        0.00438204500824213,
        0.015856176614761353,
        -0.03945930674672127,
        -0.020997589454054832,
        -0.02251632697880268,
        -0.0034084508661180735,
        -0.002352998126298189,
        0.032966356724500656,
        -0.0272118728607893,
        -0.013543237000703812,
        0.0005050846957601607,
        -0.008130121976137161,
        0.0009370191837660968,
        0.008053488098084927,
        -0.022404860705137253,
        -0.024466998875141144,
        -0.008548122830688953,
        0.008018654771149158,
        0.019437052309513092,
        0.007398620247840881,
        0.01959032006561756,
        0.010038994252681732,
        0.0009866568725556135,
        -0.001406399649567902,
        0.0428033173084259,
        -0.01388460397720337,
        0.002535873558372259,
        -0.014755439944565296,
        0.001035423600114882,
        0.03592022880911827,
        0.012735101394355297,
        -0.002624698681756854,
        -0.0010606779251247644,
        0.026487339287996292,
        0.015897976234555244,
        -0.012742067687213421,
        0.010436095297336578,
        0.025275135412812233,
        0.002154447603970766,
        0.0040302276611328125,
        0.027379073202610016,
        0.03536289557814598,
        0.012944101355969906,
        0.0067054349929094315,
        -0.0038839271292090416,
        0.012247432954609394,
        -0.01118153054267168,
        0.038818370550870895,
        0.02634800411760807,
        -0.009523458778858185,
        -0.010157427750527859,
        0.008513289503753185,
        -0.04497692361474037,
        -0.005552448332309723,
        -0.024606334045529366,
        0.014852973632514477,
        0.010749596171081066,
        -0.01130693033337593,
        0.003262150567024946,
        0.001316703506745398,
        -0.019074784591794014,
        0.030207548290491104,
        0.006221250165253878,
        -0.024216199293732643,
        -0.0005220660241320729,
        -0.002276364481076598,
        0.015479975380003452,
        -0.039319973438978195,
        -0.009669759310781956,
        0.012602734379470348,
        0.017165912315249443,
        -0.004075510893017054,
        0.010721729137003422,
        -0.034582626074552536,
        0.03288275748491287,
        -0.003422384150326252,
        -0.010819262824952602,
        -0.03380235657095909,
        -0.006520817521959543,
        -0.04288691654801369,
        0.027420874685049057,
        0.022641727700829506,
        0.015939775854349136,
        -0.022753195837140083,
        0.005636048503220081,
        0.01854531653225422,
        -0.02760200761258602,
        -0.02050992287695408,
        -0.01959032006561756,
        -0.004643295891582966,
        0.02257206104695797,
        0.003828193759545684,
        -0.014253838919103146,
        -0.023993264883756638,
        -0.010192261077463627,
        0.018601050600409508,
        0.018782183527946472,
        0.020412389189004898,
        0.0024139564484357834,
        -0.010408228263258934,
        0.026334071531891823,
        -0.001065902877599001,
        -0.029761681333184242,
        0.013807971030473709,
        0.007726054172962904,
        0.0011695323046296835,
        0.00037707184674218297,
        0.009753359481692314,
        -0.009732459671795368,
        -0.005601215176284313,
        -0.020412389189004898,
        0.017012646421790123,
        0.017096245661377907,
        -0.009307491593062878,
        0.024118665605783463,
        0.03349582478404045,
        0.0020900056697428226,
        -0.014866907149553299,
        0.007328953128308058,
        -0.029204346239566803,
        -0.026222603395581245,
        0.018252715468406677,
        -0.01959032006561756,
        -0.003438059240579605,
        0.0040128109976649284,
        0.00180088821798563,
        0.03391382470726967,
        -0.02806180901825428,
        -0.02987314760684967,
        -0.015619308687746525,
        0.0193952526897192,
        0.0065521677024662495,
        0.020119788125157356,
        0.023547397926449776,
        -0.03098781779408455,
        -0.02559560351073742,
        -0.003594809677451849,
        -0.01974358595907688,
        0.01012259442359209,
        -0.034192491322755814,
        0.02382606454193592,
        0.007102536037564278,
        0.000681428937241435,
        -0.01843385025858879,
        -0.6148517727851868,
        -0.03820530325174332,
        -0.009300525300204754,
        -0.028535543009638786,
        0.01698477938771248,
        0.005514131393283606,
        -0.011627398431301117,
        0.03349582478404045,
        -0.024104731157422066,
        0.033579424023628235,
        -0.0008451460162177682,
        -0.023296596482396126,
        -0.0038212270010262728,
        0.007733020931482315,
        -0.010498794727027416,
        -0.012992868199944496,
        0.006503400858491659,
        0.004162594676017761,
        0.00922389142215252,
        0.022335194051265717,
        -0.03500062972307205,
        0.00739165348932147,
        -0.026069337502121925,
        -0.004152144771069288,
        -0.025372669100761414,
        0.0031245583668351173,
        0.006506884470582008,
        -0.021095123142004013,
        -0.009816059842705727,
        0.017151979729533195,
        -0.02045418880879879,
        -0.001589275081641972,
        0.021889325231313705,
        -0.030207548290491104,
        0.025010401383042336,
        0.021471325308084488,
        0.0021439974661916494,
        0.009084558114409447,
        -0.0007767854258418083,
        0.027880676090717316,
        -0.01596764288842678,
        -0.002070847200229764,
        -0.0024035065434873104,
        -0.0031228167936205864,
        0.017360979691147804,
        -0.000540788983926177,
        0.0186428502202034,
        -0.010498794727027416,
        0.006841285154223442,
        -0.004591045901179314,
        -0.008708356879651546,
        -0.003521659644320607,
        -0.007572787348181009,
        -0.002574190264567733,
        0.03550222888588905,
        0.011418397538363934,
        0.0342482253909111,
        -0.0024975568521767855,
        0.0014272996922954917,
        -0.011919998563826084,
        0.008631723001599312,
        0.017305247485637665,
        -0.012303167022764683,
        -0.01918625272810459,
        0.018823985010385513,
        0.009565259329974651,
        -0.006287433672696352,
        0.003291758941486478,
        0.00914725847542286,
        -0.04558999091386795,
        0.0003217737830709666,
        0.015229174867272377,
        0.0004941992810927331,
        -0.015284908004105091,
        0.00554199842736125,
        0.012338000349700451,
        0.0033718757331371307,
        0.011188496835529804,
        -0.002103938953951001,
        0.019757520407438278,
        -0.017444580793380737,
        -0.012247432954609394,
        -0.028842078521847725,
        0.002976516494527459,
        -0.0024052481167018414,
        0.0001037927286233753,
        -0.01738884672522545,
        0.017193779349327087,
        -0.01478330697864294,
        -0.0036505432799458504,
        0.0033718757331371307,
        0.03199101984500885,
        -0.028842078521847725,
        -0.042050912976264954,
        0.004918479826301336,
        0.011376597918570042,
        0.013125235214829445,
        -0.0074613201431930065,
        0.03112715110182762,
        -0.03722996637225151,
        0.014246871694922447,
        -0.020565655082464218,
        0.012351933866739273,
        -0.005886849481612444,
        0.02675207331776619,
        0.03600383177399635,
        -0.007475253660231829,
        0.0011259905295446515,
        -0.025707069784402847,
        0.0035303679760545492,
        -0.005911232437938452,
        -0.010401261039078236,
        -0.0035390763077884912,
        -0.0003302644472569227,
        -0.014212038367986679,
        -0.04040677472949028,
        0.011843365617096424,
        0.01065902877599001,
        0.01631597802042961,
        -0.003516434459015727,
        0.02599967084825039,
        0.01843385025858879,
        0.015521774999797344,
        -0.02886994555592537,
        0.01868464983999729,
        0.012045399285852909,
        0.0024522733874619007,
        0.006210800260305405,
        0.020844323560595512,
        0.02886994555592537,
        -0.011495031416416168,
        0.02172212488949299,
        0.010213160887360573,
        -0.010213160887360573,
        -0.002133547328412533,
        0.01571684330701828,
        0.0012836117530241609,
        -0.007726054172962904,
        0.01097949594259262,
        -0.0035094679333269596,
        -0.02130412496626377,
        -0.008144055493175983,
        -0.00489758001640439,
        -0.012553967535495758,
        -0.028591277077794075,
        -0.015647174790501595,
        -0.021931126713752747,
        -0.0011024780105799437,
        0.0001773239200701937,
        -0.02266959473490715,
        0.011599531397223473,
        -0.018475649878382683,
        -0.02528906799852848,
        -0.017347047105431557,
        -0.023394130170345306,
        -0.014616106636822224,
        -0.011655264534056187,
        -0.032213952392339706,
        -0.008617790415883064,
        -0.029092878103256226,
        0.006754201836884022,
        0.02534480206668377,
        -0.027588075026869774,
        -0.017765048891305923,
        -0.009676726534962654,
        -0.013877637684345245,
        0.0418279804289341,
        0.030792750418186188,
        0.0045875622890889645,
        -0.016190577298402786,
        0.013034668751060963,
        -0.03792663663625717,
        0.0009213441517204046,
        0.02201472595334053,
        0.0011190238874405622,
        0.01848958246409893,
        -0.02605540305376053,
        0.009906627237796783,
        -0.00020399325876496732,
        -0.032576221972703934,
        -0.009857860393822193,
        0.040573976933956146,
        -0.002218889305368066,
        0.006458117626607418,
        0.012233499437570572,
        0.0017199005233123899,
        0.003911793697625399,
        -0.0036261598579585552,
        -0.021025456488132477,
        0.015842242166399956,
        0.011237263679504395,
        0.019534585997462273,
        -0.017904382199048996,
        0.01627417653799057,
        -0.004322828259319067,
        0.02237699367105961,
        0.007948988117277622,
        0.027615942060947418,
        -0.014532506465911865,
        0.018754318356513977,
        0.025163667276501656,
        0.009850893169641495,
        -0.0022711395286023617,
        -0.0011756281601265073,
        -0.006064499728381634,
        -0.00982302613556385,
        -0.0011277322191745043,
        -0.03553009778261185,
        0.023338396102190018,
        0.0015709876315668225,
        0.02220979332923889,
        -0.02906501293182373,
        -0.033217158168554306,
        -0.01804371550679207,
        -0.008206755854189396,
        0.0012374575017020106,
        -0.009084558114409447,
        0.03265982121229172,
        -0.0279642753303051,
        0.00955132581293583,
        -0.006064499728381634,
        -0.012909268029034138,
        0.038706906139850616,
        0.012254400178790092,
        0.006893535144627094,
        0.006043599918484688,
        -0.0029660663567483425,
        -0.00960009265691042,
        0.015521774999797344,
        -0.020342722535133362,
        0.019228052347898483,
        0.02076072245836258,
        0.029956746846437454,
        -0.0038665104657411575,
        -0.015424241311848164,
        -0.010533628053963184,
        0.02630620449781418,
        0.009356258437037468,
        0.03279915452003479,
        0.003523401217535138,
        -0.012156866490840912,
        0.0030531499069184065,
        -0.004110344685614109,
        -0.0040406775660812855,
        0.053977880626916885,
        -0.01005292683839798,
        0.01995258778333664,
        0.030820617452263832,
        -0.01141143124550581,
        -0.0074404203332960606,
        0.017263446003198624,
        -0.011947865597903728,
        -0.017360979691147804,
        0.007551887072622776,
        0.017207713797688484,
        0.0009274400072172284,
        0.024801399558782578,
        0.0003629207785706967,
        -0.011195464059710503,
        0.010011127218604088,
        0.004267094656825066,
        -0.01249823346734047,
        0.02024518884718418,
        0.01787651516497135,
        0.023561330512166023,
        -0.014643972739577293,
        -0.030151814222335815,
        -0.028298676013946533,
        -0.012017532251775265,
        0.00023577875981573015,
        -0.017514247447252274,
        -0.004256644751876593,
        -0.010247994214296341,
        0.006573067978024483,
        0.003876960603520274,
        0.004364628344774246,
        -0.02363099716603756,
        0.01733311451971531,
        0.005022980272769928,
        0.02181965857744217,
        -0.006931852083653212,
        -0.020886123180389404,
        0.028131475672125816,
        -0.0040580942295491695,
        -0.011146697215735912,
        -0.015438174828886986,
        -0.019910788163542747,
        -0.010617229156196117,
        -0.028075743466615677,
        0.017597848549485207,
        0.008882524445652962,
        0.0077887545339763165,
        -0.020273054018616676,
        -0.003695826744660735,
        -0.002946907887235284,
        0.0015431208303198218,
        0.008861623704433441,
        -0.01118153054267168,
        0.013076468370854855,
        0.01125816348940134,
        -0.012045399285852909,
        0.005364347714930773,
        -0.016761844977736473,
        0.014448905363678932,
        0.049909334629774094,
        -0.006653184536844492,
        -0.02172212488949299,
        -0.01985505409538746,
        -0.018768250942230225,
        -0.018670717254281044,
        0.02332446351647377,
        0.010387328453361988,
        -0.019116584211587906,
        0.01153683103621006,
        0.024759599938988686,
        0.02372853085398674,
        -0.009425925090909004,
        0.0016868087695911527,
        0.011919998563826084,
        -0.0067054349929094315,
        -0.017402781173586845,
        -0.03753650188446045,
        -0.029455145820975304,
        0.0005564640159718692,
        0.025804603472352028,
        0.06325750797986984,
        0.0035094679333269596,
        0.0036993101239204407,
        -0.019464919343590736,
        0.0037480767350643873,
        -0.018015848472714424,
        -0.01592584326863289,
        0.0011303446954116225,
        -0.015633242204785347,
        -0.021485257893800735,
        -0.00836002267897129,
        0.0035652013029903173,
        0.0056499820202589035,
        0.02126232348382473,
        0.0009013149538077414,
        0.0013184451963752508,
        -0.05768415704369545,
        0.02559560351073742,
        -0.000957048439886421,
        0.003974494058638811,
        -0.014337439090013504,
        0.020816456526517868,
        0.03461049497127533,
        0.005099613685160875,
        -0.01752818003296852,
        0.01899118535220623,
        0.02509400062263012,
        0.018712516874074936,
        -0.04010024294257164,
        0.0060157328844070435,
        0.006917918566614389,
        -0.008798924274742603,
        0.00041408237302675843,
        -0.013139168731868267,
        0.022599928081035614,
        0.01708231307566166,
        0.006802968215197325,
        0.03380235657095909,
        0.0037202101666480303,
        -0.003915277309715748,
        0.012066299095749855,
        0.008645656518638134,
        -0.006245633587241173,
        -0.0037027932703495026,
        0.00836002267897129,
        -0.0035808763932436705,
        0.018503516912460327,
        0.02282286249101162,
        0.0013524078531190753,
        0.007238386198878288,
        -0.014379238709807396,
        -0.01783471554517746,
        -0.004451712127774954,
        -0.0074891867116093636,
        -0.007384686730802059,
        -0.011982698924839497,
        -0.009405025281012058,
        -0.025261200964450836,
        -0.023254796862602234,
        -0.003335300600156188,
        -0.00876409001648426,
        0.018475649878382683,
        -0.01139053050428629,
        0.009614026173949242,
        -0.005716165527701378,
        -0.003869993845000863,
        -0.017165912315249443,
        -0.013362103141844273,
        -0.0016040793852880597,
        -0.02634800411760807,
        -0.006130683235824108,
        -0.04840452969074249,
        -0.020328788086771965,
        0.023686731234192848,
        0.024397332221269608,
        0.029148612171411514,
        -0.013313336297869682,
        0.005172763951122761,
        -0.00545143149793148,
        -0.010059894062578678,
        -0.008018654771149158,
        0.016538910567760468,
        -0.005611665081232786,
        -0.016455311328172684,
        0.02609720453619957,
        -0.034136757254600525,
        -0.026111137121915817,
        -0.01950671896338463,
        0.02161065861582756,
        -0.011550764553248882,
        0.01779291406273842,
        -0.01468577329069376,
        -0.022237660363316536,
        -0.0067054349929094315,
        0.029287945479154587,
        -0.004845329560339451,
        0.02746267430484295,
        -0.0020969724282622337,
        -0.02605540305376053,
        0.020872190594673157,
        -0.03722996637225151,
        0.0004397720331326127,
        -0.03472195938229561,
        -0.012888368219137192,
        -0.003988427575677633,
        -0.0021770892199128866,
        0.012331033125519753,
        -0.014072705060243607,
        -0.003828193759545684,
        -0.0120941661298275,
        -0.01315310224890709,
        -0.01251913420855999,
        -0.0035025011748075485,
        0.046119458973407745,
        0.013055568560957909,
        0.004500478971749544,
        -0.014797240495681763,
        0.009202991612255573,
        -0.020523855462670326,
        -0.006771618500351906,
        -0.015340641140937805,
        0.02553986944258213,
        0.00311759184114635,
        -0.014866907149553299,
        0.03867903724312782,
        -0.01282566785812378,
        -0.011926965788006783,
        -0.015438174828886986,
        0.035335030406713486,
        -0.0234359297901392,
        0.045617856085300446,
        0.0035460428334772587,
        0.0027309407014399767,
        -0.017500314861536026,
        0.008276422508060932,
        -0.005280747544020414,
        -0.010895895771682262,
        -0.03148941695690155,
        0.018614983186125755,
        0.026361938565969467,
        -0.012303167022764683,
        -0.003533851122483611,
        -0.022530261427164078,
        -0.01612091064453125,
        -0.020091921091079712,
        -0.024271931499242783,
        0.04508838802576065,
        -0.0038978606462478638,
        0.003025283105671406,
        0.007168719545006752,
        0.009363225661218166,
        -0.0028371827211230993,
        -0.0002782320079859346,
        -0.037257835268974304,
        -0.020426321774721146,
        0.0015509583754464984,
        0.0397937074303627,
        0.0193952526897192,
        0.024620266631245613,
        0.04701119288802147,
        -0.011947865597903728,
        0.018127316609025,
        0.020217321813106537,
        -0.0008625627378933132,
        -0.0031837753485888243,
        -0.030346881598234177,
        -0.008234621956944466,
        -0.015995509922504425,
        0.014191138558089733,
        0.007067702244967222,
        -0.0076424540020525455,
        0.006956235505640507,
        0.01985505409538746,
        0.015006240457296371,
        0.011077029630541801,
        -0.003838643664494157,
        -0.0021370307076722383,
        0.0010876738233491778,
        -0.019172318279743195,
        -0.01256790105253458,
        0.01869858428835869,
        -0.018308449536561966,
        0.00748222041875124,
        -0.03380235657095909,
        -0.004695545881986618,
        0.0031559085473418236,
        -0.018865784630179405,
        0.011940899305045605,
        -0.00720355287194252,
        0.019074784591794014,
        -0.019269851967692375,
        0.010282827541232109,
        -0.016998711973428726,
        0.013821904547512531,
        -0.013229736126959324,
        -0.010888929478824139,
        -0.023463796824216843,
        -0.001105090486817062,
        0.01677577942609787,
        0.019966520369052887,
        -0.011620431207120419,
        0.0018583633936941624,
        -0.030514081940054893,
        0.015326708555221558,
        -0.008137088268995285,
        -0.029148612171411514,
        -0.003768976777791977,
        0.001748638111166656,
        0.007879321463406086,
        -0.016817579045891762,
        -0.04946346580982208,
        -0.034638360142707825,
        -0.013362103141844273,
        -0.009634925983846188,
        -0.001558795920573175,
        -0.016399577260017395,
        0.015298841521143913,
        -0.011042196303606033,
        -0.04135424643754959,
        -0.004496995359659195,
        -0.0026560488622635603,
        0.03023541532456875,
        0.022335194051265717,
        0.005200630519539118,
        0.012985901907086372,
        0.030904216691851616,
        0.013389969244599342,
        -0.011007362976670265,
        0.00028258617385290563,
        -0.019019052386283875,
        0.009920559823513031,
        0.011188496835529804,
        -0.023867864161729813,
        -0.0428311824798584,
        0.0299846138805151,
        0.027393007650971413,
        -0.017667515203356743,
        -0.029343679547309875,
        0.015856176614761353,
        -0.018782183527946472,
        -0.001386370393447578,
        0.00997629389166832,
        -0.011425363831222057,
        -0.021122990176081657,
        -0.009035791270434856,
        -0.012170800007879734,
        0.014295638538897038,
        0.01468577329069376,
        -0.005298164207488298,
        -0.017848648130893707,
        0.013013768941164017,
        -0.020119788125157356,
        0.005218047648668289,
        0.010881963185966015,
        -0.010847128927707672,
        0.024007197469472885,
        0.01471363939344883,
        0.022711394354701042,
        0.014267771504819393,
        0.03304995596408844,
        0.026334071531891823,
        -0.030709149315953255,
        0.018378116190433502,
        -0.013452669605612755,
        -0.003040958195924759,
        -0.007224452681839466,
        0.005360864568501711,
        0.004469128791242838,
        0.0304862167686224,
        -0.022711394354701042,
        -0.00927265826612711,
        0.004099894315004349,
        -0.009767292998731136,
        0.017653580754995346,
        0.008729256689548492,
        -0.00025994444149546325,
        0.004911513067781925,
        -0.011522897519171238,
        0.007928088307380676,
        0.034638360142707825,
        0.006123716477304697,
        -0.02176392637193203,
        -0.0007519666105508804,
        -0.0016493628500029445,
        0.0037445933558046818,
        -0.02690533921122551,
        0.009453792124986649,
        0.029510879889130592,
        -0.0418279804289341,
        -0.00914029125124216,
        0.027518408372998238,
        0.011334797367453575,
        0.011049163527786732,
        -0.029148612171411514,
        0.011090963147580624,
        0.013167035765945911,
        0.021025456488132477,
        -0.013410869985818863,
        -0.010206194594502449,
        -0.007503120228648186,
        0.015326708555221558,
        -0.03006821498274803,
        -0.017165912315249443,
        -0.019924720749258995,
        -0.011857299134135246,
        0.0158701092004776,
        0.021039390936493874,
        0.007565820589661598,
        -0.007872354239225388,
        0.010540595278143883,
        -0.029343679547309875,
        0.010241027921438217,
        0.009753359481692314,
        -0.004260128363966942,
        -0.00318725872784853,
        0.014769373461604118,
        -0.0038665104657411575,
        0.010450027883052826,
        0.0176257137209177,
        0.020133720710873604,
        -0.0065626176074147224,
        0.01000415999442339,
        -0.030597683042287827,
        -0.015897976234555244,
        -0.007955954410135746,
        0.004124277736991644,
        -0.00038055519689805806,
        -0.02800607495009899,
        0.006447667255997658,
        0.004775662906467915,
        0.00568133220076561,
        -0.0022397893480956554,
        -0.015354574657976627,
        -0.005329514387995005,
        0.002267656149342656,
        -0.011676165275275707,
        0.00816495530307293,
        0.016566777601838112,
        -0.023310529068112373,
        0.037007033824920654,
        0.0067054349929094315,
        0.04951919987797737,
        0.0018670717254281044,
        -0.01606517657637596,
        -0.03491702675819397,
        -0.026125071570277214,
        -0.017556047067046165,
        -0.026877472177147865,
        -0.03304995596408844,
        -0.025484135374426842,
        0.02790854312479496,
        0.019562453031539917,
        -0.027128273621201515,
        -0.03260408714413643,
        -0.018196983262896538,
        -0.019659986719489098,
        0.00997629389166832,
        0.01646924391388893,
        0.01779291406273842,
        0.010575428605079651,
        0.01617664471268654,
        -0.02841014415025711,
        0.03714636713266373,
        0.010087761096656322,
        -0.0006901372689753771,
        -0.013550203293561935,
        0.012581833638250828,
        0.011397497728466988,
        -0.01612091064453125,
        0.004803529474884272,
        0.02473173290491104,
        -0.04751279577612877,
        -0.0007872354472056031,
        0.03784303739666939,
        -0.006451150868088007,
        0.02403506450355053,
        0.03918064013123512,
        -0.019715720787644386,
        -0.03480556234717369,
        -0.01430957205593586,
        0.018322382122278214,
        0.033022090792655945,
        0.01692904531955719,
        0.003636609762907028,
        -0.0019855054561048746,
        -0.01342480257153511,
        0.0052772643975913525,
        0.013717403635382652,
        -0.026724206283688545,
        0.0027675158344209194,
        0.0054792980663478374,
        0.0037724601570516825,
        -0.02397933229804039,
        -0.017221646383404732,
        0.011676165275275707,
        0.012059332802891731,
        -0.014978373423218727,
        0.020523855462670326,
        -0.004420361947268248,
        -0.02967808023095131,
        0.0049393801018595695,
        0.02679387293756008,
        0.0015971126267686486,
        -0.020900055766105652,
        0.0110839968547225,
        -0.027936408296227455,
        0.0032029335852712393,
        0.02161065861582756,
        0.012958034873008728,
        -0.0052842311561107635,
        -0.002953874645754695,
        0.005691782105714083,
        -0.02573493681848049,
        -0.00224153115414083,
        -0.007691220846027136,
        0.0058694323524832726,
        -0.018461717292666435,
        0.04625879228115082,
        0.02917647920548916,
        -0.05105186998844147,
        0.017458513379096985,
        -0.013780103996396065,
        -0.008833757601678371,
        -0.019032984972000122,
        0.0032691170927137136,
        0.005033430177718401,
        -0.021136924624443054,
        0.02101152390241623,
        0.0037480767350643873,
        -0.03505636006593704,
        -0.0019611220341175795,
        -0.0012052366510033607,
        0.008680489845573902,
        -0.01047789491713047,
        0.02574886940419674,
        0.2039845585823059,
        -0.03405315801501274,
        0.01833631657063961,
        0.0029573580250144005,
        0.013780103996396065,
        0.0176257137209177,
        0.0037550434935837984,
        0.017709314823150635,
        -0.00217186426743865,
        -0.001844429993070662,
        0.00048462007544003427,
        0.008060455322265625,
        -0.013222768902778625,
        0.0024365982972085476,
        0.0224187932908535,
        0.0010824487544596195,
        -0.012066299095749855,
        -0.046175193041563034,
        -0.00687611848115921,
        -0.0056778485886752605,
        -0.005225013941526413,
        0.014699706807732582,
        -0.003213383723050356,
        -0.036505430936813354,
        0.014351372607052326,
        -0.012916235253214836,
        -0.006691501475870609,
        -0.026919273659586906,
        0.006419800687581301,
        0.03472195938229561,
        -0.007377719972282648,
        0.01858711615204811,
        -0.008241589181125164,
        0.015479975380003452,
        -0.009690659120678902,
        -0.020384522154927254,
        -0.02272532880306244,
        0.02222372591495514,
        0.011056129820644855,
        -0.009425925090909004,
        0.035084228962659836,
        0.010401261039078236,
        -0.008025621995329857,
        -0.012331033125519753,
        -0.00031154145835898817,
        -0.00023947982117533684,
        0.000322644627885893,
        -0.013814937323331833,
        -0.002095230622217059,
        0.03023541532456875,
        -0.010512728244066238,
        0.02654307149350643,
        0.039124906063079834,
        0.042691849172115326,
        0.0030461831483989954,
        -0.014267771504819393,
        0.002694365568459034,
        0.00595651613548398,
        0.005886849481612444,
        0.01949278637766838,
        -0.025762803852558136,
        0.0186428502202034,
        0.002546323463320732,
        0.03112715110182762,
        0.03179595246911049,
        0.015856176614761353,
        -0.019882921129465103,
        0.012393733486533165,
        0.015730775892734528,
        -0.004723412916064262,
        -0.015382441692054272,
        -0.016817579045891762,
        -0.005925165954977274,
        -0.004653745796531439,
        -0.03502849489450455,
        -0.02126232348382473,
        0.051414139568805695,
        0.01818304881453514,
        0.009565259329974651,
        0.034081026911735535,
        -0.018754318356513977,
        -0.005834599025547504,
        0.011369630694389343,
        -0.014894773252308369,
        -0.026779938489198685,
        -0.043054115027189255,
        0.017305247485637665,
        -0.01395427156239748,
        -0.017946181818842888,
        -0.016483178362250328,
        0.0023791231215000153,
        -0.021081190556287766,
        -0.008959157392382622,
        0.008617790415883064,
        -0.0004689450142905116,
        -0.010735662654042244,
        -0.019172318279743195,
        0.03940357267856598,
        -0.0014569080667570233,
        -0.021596726030111313,
        -0.03452689200639725,
        0.04703906178474426,
        0.03842823579907417,
        0.02644553780555725,
        -0.0008155376417562366,
        0.004946346860378981,
        -0.01019922737032175,
        -0.008157989010214806,
        0.022502394393086433,
        -0.0013027702225372195,
        0.001785213127732277,
        -0.012386767193675041,
        0.0019053885480389,
        0.007997754961252213,
        -0.0018931968370452523,
        -0.011327831074595451,
        0.005221530795097351,
        -0.014393172226846218,
        0.012637567706406116,
        -0.005221530795097351,
        0.0005899911629967391,
        -0.03388595953583717,
        -0.039821576327085495,
        0.017946181818842888,
        -0.0014830331783741713,
        -0.004685095977038145,
        -0.016413511708378792,
        -0.02619473822414875,
        -0.01430957205593586,
        -0.007405586540699005,
        0.011327831074595451,
        -0.03265982121229172,
        0.03530716150999069,
        -0.021582791581749916,
        -0.02906501293182373,
        0.00651036761701107,
        0.01950671896338463,
        -0.017542114481329918,
        -0.01779291406273842,
        0.009941460564732552,
        0.009460759349167347,
        -5.8073856052942574e-05,
        0.006141133140772581,
        -0.006984102074056864,
        0.01627417653799057,
        -0.0468439944088459,
        0.0076215541921556,
        -0.014434972777962685,
        -0.0014351372374221683,
        -0.03625463321805,
        -0.01478330697864294,
        -0.007649420760571957,
        0.0015466042095795274,
        -0.019311651587486267,
        0.023463796824216843,
        -0.002380864694714546,
        -0.018670717254281044,
        -0.015159507282078266,
        -0.013264569453895092,
        0.0021109057124704123,
        -0.028786344453692436,
        -0.015591442584991455,
        -0.0022641727700829506,
        -0.025219401344656944,
        -0.009077590890228748,
        -0.009579192847013474,
        -0.17745541036128998,
        0.010847128927707672,
        0.02076072245836258,
        -0.032074619084596634,
        -0.005552448332309723,
        0.02711433917284012,
        0.032269686460494995,
        -0.009850893169641495,
        -0.020482055842876434,
        0.003523401217535138,
        -0.0010632904013618827,
        -0.017012646421790123,
        -0.02856341004371643,
        -0.0007136498461477458,
        0.004350695293396711,
        0.01306950207799673,
        0.0018235299503430724,
        0.02045418880879879,
        0.024898933246731758,
        0.02825687639415264,
        0.03143368661403656,
        -0.044614654034376144,
        0.013536269776523113,
        -0.007321986369788647,
        0.02064925618469715,
        -0.009767292998731136,
        -0.010171361267566681,
        0.03137795254588127,
        -0.017151979729533195,
        -0.014671839773654938,
        0.012128999456763268,
        0.02725367434322834,
        0.012101132422685623,
        -0.004552728962153196,
        0.00025515485322102904,
        0.020886123180389404,
        0.01123029738664627,
        -0.01334816962480545,
        -0.0011651781387627125,
        -0.008415755815804005,
        0.05252880975604057,
        -0.00992752704769373,
        -0.0076145874336361885,
        -0.014121471904218197,
        -0.023644929751753807,
        -0.011097930371761322,
        0.007670320570468903,
        -0.0022746229078620672,
        0.016009442508220673,
        0.018364183604717255,
        -0.01748638041317463,
        -0.020384522154927254,
        0.020635321736335754,
        0.006398900877684355,
        0.007005002349615097,
        0.02679387293756008,
        0.011662231758236885,
        0.00821372214704752,
        -0.026166871190071106,
        0.015535708516836166,
        0.004977697040885687,
        -0.033022090792655945,
        0.026013603433966637,
        0.002319906372576952,
        -0.014978373423218727,
        -0.0016868087695911527,
        0.0018479133723303676,
        0.004333278629928827,
        -0.021986858919262886,
        0.011202430352568626,
        -0.03647756576538086,
        -0.023463796824216843,
        0.0003302644472569227,
        0.0047408295795321465,
        0.022599928081035614,
        -0.00012757742661051452,
        -0.019367385655641556,
        -0.0012609701370820403,
        0.001722512999549508,
        -0.01123029738664627,
        -0.033022090792655945,
        0.0317402184009552,
        -0.0096976263448596,
        0.026975005865097046,
        0.0032081587705761194,
        -0.012087199836969376,
        0.005521098151803017,
        0.018322382122278214,
        -0.019715720787644386,
        -0.026431605219841003,
        0.011432331055402756,
        0.0060226996429264545,
        0.017542114481329918,
        0.0019785386975854635,
        0.007412553299218416,
        0.01779291406273842,
        0.010220128111541271,
        -0.015284908004105091,
        0.022181926295161247,
        -0.012853534892201424,
        -0.02105332352221012,
        -0.017347047105431557,
        0.006381483748555183,
        0.009307491593062878,
        0.012783868238329887,
        0.02533086948096752,
        -0.019618187099695206,
        0.006694984622299671,
        0.029037145897746086,
        0.011481097899377346,
        -0.006043599918484688,
        -0.010833196341991425,
        0.00023664960463065654,
        0.02629227191209793,
        0.017820781096816063,
        0.009133324958384037,
        -0.03193528577685356,
        -0.0027918992564082146,
        0.005256364122033119,
        -0.02599967084825039,
        0.043499983847141266,
        -0.010603295639157295,
        0.003612226340919733,
        0.014978373423218727,
        -0.0007920250645838678,
        -0.01804371550679207,
        -0.11425364017486572,
        -0.008206755854189396,
        -0.002555032027885318,
        0.004354178439825773,
        -0.014490705914795399,
        -0.004120794590562582,
        0.02172212488949299,
        0.029483012855052948,
        -0.026027537882328033,
        0.010624195449054241,
        -0.020579589530825615,
        -0.02711433917284012,
        0.013090401887893677,
        0.004803529474884272,
        -0.005319064483046532,
        0.008185855112969875,
        0.0033701341599226,
        -0.0047582462430000305,
        -0.009251758456230164,
        0.026682404801249504,
        -0.0026473405305296183,
        -0.01959032006561756,
        0.014393172226846218,
        -0.024048998951911926,
        0.015842242166399956,
        0.00677858479321003,
        -0.029705947265028954,
        0.0015788250602781773,
        0.021889325231313705,
        0.011028262786567211,
        0.007398620247840881,
        -0.03455476090312004,
        -0.00881285686045885,
        -0.003410192672163248,
        -0.0019837636500597,
        -0.003326592268422246,
        -0.012909268029034138,
        -0.022558128461241722,
        0.005782349035143852,
        -0.03405315801501274,
        0.0029051078017801046,
        0.013292435556650162,
        -0.004169561434537172,
        -0.010157427750527859,
        -0.010136527009308338,
        -0.00012942795001436025,
        -0.014434972777962685,
        0.030151814222335815,
        0.023770330473780632,
        -0.00803258828818798,
        -0.012198666110634804,
        -0.02045418880879879,
        -0.02499646693468094,
        -0.005367831327021122,
        0.020941857248544693,
        0.033133555203676224,
        0.0186428502202034,
        0.013598970137536526,
        -0.004517895635217428,
        -0.012965002097189426,
        -0.000530774355866015,
        -0.005092646926641464,
        -0.01698477938771248,
        0.005144897382706404,
        0.018628917634487152,
        -0.001567504252307117,
        -0.05473028123378754,
        -0.011167597025632858,
        -0.008673523552715778,
        -0.005350414663553238,
        -0.014156305231153965,
        0.01793224923312664,
        0.013194902800023556,
        0.02155492454767227,
        -0.01858711615204811,
        -0.01012259442359209,
        -0.014281705021858215,
        -0.0021091641392558813,
        0.0022066975943744183,
        -0.018726451322436333,
        -0.01090286299586296,
        -0.0274487417191267,
        0.007698187604546547,
        -0.002624698681756854,
        -0.010220128111541271,
        -0.012804768048226833,
        -0.0012182991486042738,
        -0.02499646693468094,
        -0.011745831929147243,
        -0.029761681333184242,
        0.009662793017923832,
        0.008645656518638134,
        -0.029761681333184242,
        -0.008234621956944466,
        -0.004180011339485645,
        -0.0024644650984555483,
        -0.0031994504388421774,
        -0.01390550471842289,
        0.020217321813106537,
        0.005987866315990686,
        -0.015075907111167908,
        0.0043750787153840065,
        -0.04391798377037048,
        0.016162710264325142,
        0.0032952423207461834,
        -0.01698477938771248,
        0.011655264534056187,
        0.018823985010385513,
        0.03692343458533287,
        -0.017040513455867767,
        0.002717007417231798,
        0.008854657411575317,
        -0.007363786455243826,
        0.021485257893800735,
        -0.01738884672522545,
        -0.018977250903844833,
        -0.006447667255997658,
        -0.010993429459631443,
        0.005925165954977274,
        -0.004507445730268955,
        0.029204346239566803,
        0.005803248845040798,
        -0.01499230694025755,
        -0.016538910567760468,
        0.009572225622832775,
        -0.00025798508431762457,
        -0.007774821016937494,
        0.013828870840370655,
        -0.010234060697257519,
        0.026320137083530426,
        -0.026612738147377968,
        -0.011780665256083012,
        0.020077988505363464,
        -0.02403506450355053,
        -0.0008969607879407704,
        0.030040347948670387,
        -0.0010589362354949117,
        0.016664311289787292,
        0.02423013187944889,
        0.008596889674663544,
        0.03622676432132721,
        0.004639812745153904,
        -0.014755439944565296,
        -0.03600383177399635,
        0.0020307889208197594,
        -0.012540034018456936,
        -0.00260728201828897,
        0.022251592949032784,
        -0.0027222323697060347,
        0.003500759368762374,
        0.015479975380003452,
        -0.006398900877684355,
        0.008778023533523083,
        0.02091399021446705,
        0.019074784591794014,
        -0.029009278863668442,
        -0.015953710302710533,
        -0.00849238969385624,
        -0.0035286261700093746,
        0.02278106100857258,
        0.002276364481076598,
        -0.0034415426198393106,
        0.0367841012775898,
        -0.015048041008412838,
        0.022279459983110428,
        0.003981460817158222,
        0.0076424540020525455,
        -0.008750157430768013,
        0.012428566813468933,
        0.007039835676550865,
        -0.02912074513733387,
        -0.02827080897986889,
        -0.010568462312221527,
        0.009251758456230164,
        -0.011773698963224888,
        0.02565133571624756,
        0.029037145897746086,
        0.005252880975604057,
        -0.0007088602287694812,
        0.0114601980894804,
        -0.005998316220939159,
        0.01478330697864294,
        0.012484300881624222,
        -0.02459239959716797,
        -0.016845446079969406,
        -0.014448905363678932,
        0.024453066289424896,
        0.002779707545414567,
        -0.004451712127774954,
        0.012170800007879734,
        -0.005378281231969595,
        0.02195899188518524,
        0.012184732593595982,
        0.001861846656538546,
        0.009760326705873013,
        0.03555796295404434,
        0.00401629414409399,
        0.01490870676934719,
        0.007349853403866291,
        -0.019269851967692375,
        0.006796001922339201,
        -0.021749991923570633,
        0.006127200089395046,
        0.015828309580683708,
        -0.012463400140404701,
        -0.010094727389514446,
        -0.023157263174653053,
        0.021136924624443054,
        -0.013229736126959324,
        -0.00457362923771143,
        0.003953594248741865,
        0.03770370036363602,
        -0.0011442780960351229,
        0.015298841521143913,
        -0.01219169981777668,
        0.01080532930791378,
        -0.034276094287633896,
        0.02765774168074131,
        -0.007210519630461931,
        -0.036505430936813354,
        -0.026570938527584076,
        0.033022090792655945,
        0.021931126713752747,
        -0.01518737431615591,
        0.044558919966220856,
        -0.012212599627673626,
        0.018559250980615616,
        0.010819262824952602,
        0.024717800319194794,
        -0.011906065978109837,
        0.02638980560004711,
        0.02278106100857258,
        0.017597848549485207,
        -0.008680489845573902,
        0.01989685371518135,
        -0.01874038390815258,
        -0.021136924624443054,
        -0.012310133315622807,
        -0.006607901304960251,
        0.03279915452003479,
        -0.0008817211491987109,
        0.10160214453935623,
        0.021248390898108482,
        -0.0030949499923735857,
        0.012115065939724445,
        -0.015702908858656883,
        0.011871231719851494,
        0.013362103141844273,
        0.018921518698334694,
        -0.011474130675196648,
        -0.0010545820696279407,
        0.006339683663100004,
        -0.015591442584991455,
        -0.013689536601305008,
        -0.02423013187944889,
        0.0035355929285287857,
        -0.015605375170707703,
        0.003765493631362915,
        0.008617790415883064,
        -0.008610823191702366,
        -0.0030200581531971693,
        0.024299798533320427,
        -0.00043737722444348037,
        0.011223330162465572,
        0.01602337695658207,
        -0.013118268921971321,
        -0.009669759310781956,
        0.007018935866653919,
        -0.016455311328172684,
        -0.008673523552715778,
        -0.03112715110182762,
        0.004406428430229425,
        -0.016566777601838112,
        -0.04166077822446823,
        -0.013571103103458881,
        0.02815934270620346,
        0.01097949594259262,
        -0.001757346442900598,
        0.013341202400624752,
        0.014121471904218197,
        5.785614484921098e-05,
        0.009293558076024055,
        0.036449696868658066,
        -0.0029138161335140467,
        -0.00964885950088501,
        -0.013647736981511116,
        -0.007997754961252213,
        0.0014725831570103765,
        -0.013731337152421474,
        -0.02312939614057541
      ],
      "metrics": null,
      "error": null,
      "parent_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_0",
      "start_time": "2023-09-20T16:48:29.204190Z",
      "end_time": "2023-09-20T16:48:30.000195Z",
      "index": 0,
      "api_calls": [
        {
          "name": "embedQuestion",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            },
            "question": "How are sentence pairs represented in the input to BERT?"
          },
          "output": [
            -0.011118830181658268,
            -0.01631597802042961,
            0.02559560351073742,
            -0.002060397295281291,
            -0.005500198341906071,
            0.031210750341415405,
            0.015424241311848164,
            -0.011599531397223473,
            -0.01773718185722828,
            -0.027727408334612846,
            0.028535543009638786,
            0.04366718605160713,
            -0.035139963030815125,
            0.005427048075944185,
            -0.0009474692051298916,
            0.008422723039984703,
            0.011244230903685093,
            0.009614026173949242,
            0.027797074988484383,
            -0.016956912353634834,
            -0.0016798420110717416,
            -0.006102816667407751,
            -0.016552845016121864,
            -0.0035739096347242594,
            -0.010742628946900368,
            -0.00849238969385624,
            0.029538746923208237,
            -0.020328788086771965,
            -0.003713243408128619,
            -0.009676726534962654,
            0.0021788307931274176,
            0.02357526309788227,
            -0.017918314784765244,
            -0.009948426857590675,
            -0.009258724749088287,
            0.006694984622299671,
            -0.0029294912237674,
            -0.016915112733840942,
            0.02438339963555336,
            -0.023212995380163193,
            0.0071164690889418125,
            0.021694257855415344,
            0.003977977205067873,
            -0.015270974487066269,
            -0.005953032989054918,
            0.01672004535794258,
            0.010652062483131886,
            -0.0065695843659341335,
            -0.012059332802891731,
            0.0093214251101017,
            0.00438204500824213,
            0.015856176614761353,
            -0.03945930674672127,
            -0.020997589454054832,
            -0.02251632697880268,
            -0.0034084508661180735,
            -0.002352998126298189,
            0.032966356724500656,
            -0.0272118728607893,
            -0.013543237000703812,
            0.0005050846957601607,
            -0.008130121976137161,
            0.0009370191837660968,
            0.008053488098084927,
            -0.022404860705137253,
            -0.024466998875141144,
            -0.008548122830688953,
            0.008018654771149158,
            0.019437052309513092,
            0.007398620247840881,
            0.01959032006561756,
            0.010038994252681732,
            0.0009866568725556135,
            -0.001406399649567902,
            0.0428033173084259,
            -0.01388460397720337,
            0.002535873558372259,
            -0.014755439944565296,
            0.001035423600114882,
            0.03592022880911827,
            0.012735101394355297,
            -0.002624698681756854,
            -0.0010606779251247644,
            0.026487339287996292,
            0.015897976234555244,
            -0.012742067687213421,
            0.010436095297336578,
            0.025275135412812233,
            0.002154447603970766,
            0.0040302276611328125,
            0.027379073202610016,
            0.03536289557814598,
            0.012944101355969906,
            0.0067054349929094315,
            -0.0038839271292090416,
            0.012247432954609394,
            -0.01118153054267168,
            0.038818370550870895,
            0.02634800411760807,
            -0.009523458778858185,
            -0.010157427750527859,
            0.008513289503753185,
            -0.04497692361474037,
            -0.005552448332309723,
            -0.024606334045529366,
            0.014852973632514477,
            0.010749596171081066,
            -0.01130693033337593,
            0.003262150567024946,
            0.001316703506745398,
            -0.019074784591794014,
            0.030207548290491104,
            0.006221250165253878,
            -0.024216199293732643,
            -0.0005220660241320729,
            -0.002276364481076598,
            0.015479975380003452,
            -0.039319973438978195,
            -0.009669759310781956,
            0.012602734379470348,
            0.017165912315249443,
            -0.004075510893017054,
            0.010721729137003422,
            -0.034582626074552536,
            0.03288275748491287,
            -0.003422384150326252,
            -0.010819262824952602,
            -0.03380235657095909,
            -0.006520817521959543,
            -0.04288691654801369,
            0.027420874685049057,
            0.022641727700829506,
            0.015939775854349136,
            -0.022753195837140083,
            0.005636048503220081,
            0.01854531653225422,
            -0.02760200761258602,
            -0.02050992287695408,
            -0.01959032006561756,
            -0.004643295891582966,
            0.02257206104695797,
            0.003828193759545684,
            -0.014253838919103146,
            -0.023993264883756638,
            -0.010192261077463627,
            0.018601050600409508,
            0.018782183527946472,
            0.020412389189004898,
            0.0024139564484357834,
            -0.010408228263258934,
            0.026334071531891823,
            -0.001065902877599001,
            -0.029761681333184242,
            0.013807971030473709,
            0.007726054172962904,
            0.0011695323046296835,
            0.00037707184674218297,
            0.009753359481692314,
            -0.009732459671795368,
            -0.005601215176284313,
            -0.020412389189004898,
            0.017012646421790123,
            0.017096245661377907,
            -0.009307491593062878,
            0.024118665605783463,
            0.03349582478404045,
            0.0020900056697428226,
            -0.014866907149553299,
            0.007328953128308058,
            -0.029204346239566803,
            -0.026222603395581245,
            0.018252715468406677,
            -0.01959032006561756,
            -0.003438059240579605,
            0.0040128109976649284,
            0.00180088821798563,
            0.03391382470726967,
            -0.02806180901825428,
            -0.02987314760684967,
            -0.015619308687746525,
            0.0193952526897192,
            0.0065521677024662495,
            0.020119788125157356,
            0.023547397926449776,
            -0.03098781779408455,
            -0.02559560351073742,
            -0.003594809677451849,
            -0.01974358595907688,
            0.01012259442359209,
            -0.034192491322755814,
            0.02382606454193592,
            0.007102536037564278,
            0.000681428937241435,
            -0.01843385025858879,
            -0.6148517727851868,
            -0.03820530325174332,
            -0.009300525300204754,
            -0.028535543009638786,
            0.01698477938771248,
            0.005514131393283606,
            -0.011627398431301117,
            0.03349582478404045,
            -0.024104731157422066,
            0.033579424023628235,
            -0.0008451460162177682,
            -0.023296596482396126,
            -0.0038212270010262728,
            0.007733020931482315,
            -0.010498794727027416,
            -0.012992868199944496,
            0.006503400858491659,
            0.004162594676017761,
            0.00922389142215252,
            0.022335194051265717,
            -0.03500062972307205,
            0.00739165348932147,
            -0.026069337502121925,
            -0.004152144771069288,
            -0.025372669100761414,
            0.0031245583668351173,
            0.006506884470582008,
            -0.021095123142004013,
            -0.009816059842705727,
            0.017151979729533195,
            -0.02045418880879879,
            -0.001589275081641972,
            0.021889325231313705,
            -0.030207548290491104,
            0.025010401383042336,
            0.021471325308084488,
            0.0021439974661916494,
            0.009084558114409447,
            -0.0007767854258418083,
            0.027880676090717316,
            -0.01596764288842678,
            -0.002070847200229764,
            -0.0024035065434873104,
            -0.0031228167936205864,
            0.017360979691147804,
            -0.000540788983926177,
            0.0186428502202034,
            -0.010498794727027416,
            0.006841285154223442,
            -0.004591045901179314,
            -0.008708356879651546,
            -0.003521659644320607,
            -0.007572787348181009,
            -0.002574190264567733,
            0.03550222888588905,
            0.011418397538363934,
            0.0342482253909111,
            -0.0024975568521767855,
            0.0014272996922954917,
            -0.011919998563826084,
            0.008631723001599312,
            0.017305247485637665,
            -0.012303167022764683,
            -0.01918625272810459,
            0.018823985010385513,
            0.009565259329974651,
            -0.006287433672696352,
            0.003291758941486478,
            0.00914725847542286,
            -0.04558999091386795,
            0.0003217737830709666,
            0.015229174867272377,
            0.0004941992810927331,
            -0.015284908004105091,
            0.00554199842736125,
            0.012338000349700451,
            0.0033718757331371307,
            0.011188496835529804,
            -0.002103938953951001,
            0.019757520407438278,
            -0.017444580793380737,
            -0.012247432954609394,
            -0.028842078521847725,
            0.002976516494527459,
            -0.0024052481167018414,
            0.0001037927286233753,
            -0.01738884672522545,
            0.017193779349327087,
            -0.01478330697864294,
            -0.0036505432799458504,
            0.0033718757331371307,
            0.03199101984500885,
            -0.028842078521847725,
            -0.042050912976264954,
            0.004918479826301336,
            0.011376597918570042,
            0.013125235214829445,
            -0.0074613201431930065,
            0.03112715110182762,
            -0.03722996637225151,
            0.014246871694922447,
            -0.020565655082464218,
            0.012351933866739273,
            -0.005886849481612444,
            0.02675207331776619,
            0.03600383177399635,
            -0.007475253660231829,
            0.0011259905295446515,
            -0.025707069784402847,
            0.0035303679760545492,
            -0.005911232437938452,
            -0.010401261039078236,
            -0.0035390763077884912,
            -0.0003302644472569227,
            -0.014212038367986679,
            -0.04040677472949028,
            0.011843365617096424,
            0.01065902877599001,
            0.01631597802042961,
            -0.003516434459015727,
            0.02599967084825039,
            0.01843385025858879,
            0.015521774999797344,
            -0.02886994555592537,
            0.01868464983999729,
            0.012045399285852909,
            0.0024522733874619007,
            0.006210800260305405,
            0.020844323560595512,
            0.02886994555592537,
            -0.011495031416416168,
            0.02172212488949299,
            0.010213160887360573,
            -0.010213160887360573,
            -0.002133547328412533,
            0.01571684330701828,
            0.0012836117530241609,
            -0.007726054172962904,
            0.01097949594259262,
            -0.0035094679333269596,
            -0.02130412496626377,
            -0.008144055493175983,
            -0.00489758001640439,
            -0.012553967535495758,
            -0.028591277077794075,
            -0.015647174790501595,
            -0.021931126713752747,
            -0.0011024780105799437,
            0.0001773239200701937,
            -0.02266959473490715,
            0.011599531397223473,
            -0.018475649878382683,
            -0.02528906799852848,
            -0.017347047105431557,
            -0.023394130170345306,
            -0.014616106636822224,
            -0.011655264534056187,
            -0.032213952392339706,
            -0.008617790415883064,
            -0.029092878103256226,
            0.006754201836884022,
            0.02534480206668377,
            -0.027588075026869774,
            -0.017765048891305923,
            -0.009676726534962654,
            -0.013877637684345245,
            0.0418279804289341,
            0.030792750418186188,
            0.0045875622890889645,
            -0.016190577298402786,
            0.013034668751060963,
            -0.03792663663625717,
            0.0009213441517204046,
            0.02201472595334053,
            0.0011190238874405622,
            0.01848958246409893,
            -0.02605540305376053,
            0.009906627237796783,
            -0.00020399325876496732,
            -0.032576221972703934,
            -0.009857860393822193,
            0.040573976933956146,
            -0.002218889305368066,
            0.006458117626607418,
            0.012233499437570572,
            0.0017199005233123899,
            0.003911793697625399,
            -0.0036261598579585552,
            -0.021025456488132477,
            0.015842242166399956,
            0.011237263679504395,
            0.019534585997462273,
            -0.017904382199048996,
            0.01627417653799057,
            -0.004322828259319067,
            0.02237699367105961,
            0.007948988117277622,
            0.027615942060947418,
            -0.014532506465911865,
            0.018754318356513977,
            0.025163667276501656,
            0.009850893169641495,
            -0.0022711395286023617,
            -0.0011756281601265073,
            -0.006064499728381634,
            -0.00982302613556385,
            -0.0011277322191745043,
            -0.03553009778261185,
            0.023338396102190018,
            0.0015709876315668225,
            0.02220979332923889,
            -0.02906501293182373,
            -0.033217158168554306,
            -0.01804371550679207,
            -0.008206755854189396,
            0.0012374575017020106,
            -0.009084558114409447,
            0.03265982121229172,
            -0.0279642753303051,
            0.00955132581293583,
            -0.006064499728381634,
            -0.012909268029034138,
            0.038706906139850616,
            0.012254400178790092,
            0.006893535144627094,
            0.006043599918484688,
            -0.0029660663567483425,
            -0.00960009265691042,
            0.015521774999797344,
            -0.020342722535133362,
            0.019228052347898483,
            0.02076072245836258,
            0.029956746846437454,
            -0.0038665104657411575,
            -0.015424241311848164,
            -0.010533628053963184,
            0.02630620449781418,
            0.009356258437037468,
            0.03279915452003479,
            0.003523401217535138,
            -0.012156866490840912,
            0.0030531499069184065,
            -0.004110344685614109,
            -0.0040406775660812855,
            0.053977880626916885,
            -0.01005292683839798,
            0.01995258778333664,
            0.030820617452263832,
            -0.01141143124550581,
            -0.0074404203332960606,
            0.017263446003198624,
            -0.011947865597903728,
            -0.017360979691147804,
            0.007551887072622776,
            0.017207713797688484,
            0.0009274400072172284,
            0.024801399558782578,
            0.0003629207785706967,
            -0.011195464059710503,
            0.010011127218604088,
            0.004267094656825066,
            -0.01249823346734047,
            0.02024518884718418,
            0.01787651516497135,
            0.023561330512166023,
            -0.014643972739577293,
            -0.030151814222335815,
            -0.028298676013946533,
            -0.012017532251775265,
            0.00023577875981573015,
            -0.017514247447252274,
            -0.004256644751876593,
            -0.010247994214296341,
            0.006573067978024483,
            0.003876960603520274,
            0.004364628344774246,
            -0.02363099716603756,
            0.01733311451971531,
            0.005022980272769928,
            0.02181965857744217,
            -0.006931852083653212,
            -0.020886123180389404,
            0.028131475672125816,
            -0.0040580942295491695,
            -0.011146697215735912,
            -0.015438174828886986,
            -0.019910788163542747,
            -0.010617229156196117,
            -0.028075743466615677,
            0.017597848549485207,
            0.008882524445652962,
            0.0077887545339763165,
            -0.020273054018616676,
            -0.003695826744660735,
            -0.002946907887235284,
            0.0015431208303198218,
            0.008861623704433441,
            -0.01118153054267168,
            0.013076468370854855,
            0.01125816348940134,
            -0.012045399285852909,
            0.005364347714930773,
            -0.016761844977736473,
            0.014448905363678932,
            0.049909334629774094,
            -0.006653184536844492,
            -0.02172212488949299,
            -0.01985505409538746,
            -0.018768250942230225,
            -0.018670717254281044,
            0.02332446351647377,
            0.010387328453361988,
            -0.019116584211587906,
            0.01153683103621006,
            0.024759599938988686,
            0.02372853085398674,
            -0.009425925090909004,
            0.0016868087695911527,
            0.011919998563826084,
            -0.0067054349929094315,
            -0.017402781173586845,
            -0.03753650188446045,
            -0.029455145820975304,
            0.0005564640159718692,
            0.025804603472352028,
            0.06325750797986984,
            0.0035094679333269596,
            0.0036993101239204407,
            -0.019464919343590736,
            0.0037480767350643873,
            -0.018015848472714424,
            -0.01592584326863289,
            0.0011303446954116225,
            -0.015633242204785347,
            -0.021485257893800735,
            -0.00836002267897129,
            0.0035652013029903173,
            0.0056499820202589035,
            0.02126232348382473,
            0.0009013149538077414,
            0.0013184451963752508,
            -0.05768415704369545,
            0.02559560351073742,
            -0.000957048439886421,
            0.003974494058638811,
            -0.014337439090013504,
            0.020816456526517868,
            0.03461049497127533,
            0.005099613685160875,
            -0.01752818003296852,
            0.01899118535220623,
            0.02509400062263012,
            0.018712516874074936,
            -0.04010024294257164,
            0.0060157328844070435,
            0.006917918566614389,
            -0.008798924274742603,
            0.00041408237302675843,
            -0.013139168731868267,
            0.022599928081035614,
            0.01708231307566166,
            0.006802968215197325,
            0.03380235657095909,
            0.0037202101666480303,
            -0.003915277309715748,
            0.012066299095749855,
            0.008645656518638134,
            -0.006245633587241173,
            -0.0037027932703495026,
            0.00836002267897129,
            -0.0035808763932436705,
            0.018503516912460327,
            0.02282286249101162,
            0.0013524078531190753,
            0.007238386198878288,
            -0.014379238709807396,
            -0.01783471554517746,
            -0.004451712127774954,
            -0.0074891867116093636,
            -0.007384686730802059,
            -0.011982698924839497,
            -0.009405025281012058,
            -0.025261200964450836,
            -0.023254796862602234,
            -0.003335300600156188,
            -0.00876409001648426,
            0.018475649878382683,
            -0.01139053050428629,
            0.009614026173949242,
            -0.005716165527701378,
            -0.003869993845000863,
            -0.017165912315249443,
            -0.013362103141844273,
            -0.0016040793852880597,
            -0.02634800411760807,
            -0.006130683235824108,
            -0.04840452969074249,
            -0.020328788086771965,
            0.023686731234192848,
            0.024397332221269608,
            0.029148612171411514,
            -0.013313336297869682,
            0.005172763951122761,
            -0.00545143149793148,
            -0.010059894062578678,
            -0.008018654771149158,
            0.016538910567760468,
            -0.005611665081232786,
            -0.016455311328172684,
            0.02609720453619957,
            -0.034136757254600525,
            -0.026111137121915817,
            -0.01950671896338463,
            0.02161065861582756,
            -0.011550764553248882,
            0.01779291406273842,
            -0.01468577329069376,
            -0.022237660363316536,
            -0.0067054349929094315,
            0.029287945479154587,
            -0.004845329560339451,
            0.02746267430484295,
            -0.0020969724282622337,
            -0.02605540305376053,
            0.020872190594673157,
            -0.03722996637225151,
            0.0004397720331326127,
            -0.03472195938229561,
            -0.012888368219137192,
            -0.003988427575677633,
            -0.0021770892199128866,
            0.012331033125519753,
            -0.014072705060243607,
            -0.003828193759545684,
            -0.0120941661298275,
            -0.01315310224890709,
            -0.01251913420855999,
            -0.0035025011748075485,
            0.046119458973407745,
            0.013055568560957909,
            0.004500478971749544,
            -0.014797240495681763,
            0.009202991612255573,
            -0.020523855462670326,
            -0.006771618500351906,
            -0.015340641140937805,
            0.02553986944258213,
            0.00311759184114635,
            -0.014866907149553299,
            0.03867903724312782,
            -0.01282566785812378,
            -0.011926965788006783,
            -0.015438174828886986,
            0.035335030406713486,
            -0.0234359297901392,
            0.045617856085300446,
            0.0035460428334772587,
            0.0027309407014399767,
            -0.017500314861536026,
            0.008276422508060932,
            -0.005280747544020414,
            -0.010895895771682262,
            -0.03148941695690155,
            0.018614983186125755,
            0.026361938565969467,
            -0.012303167022764683,
            -0.003533851122483611,
            -0.022530261427164078,
            -0.01612091064453125,
            -0.020091921091079712,
            -0.024271931499242783,
            0.04508838802576065,
            -0.0038978606462478638,
            0.003025283105671406,
            0.007168719545006752,
            0.009363225661218166,
            -0.0028371827211230993,
            -0.0002782320079859346,
            -0.037257835268974304,
            -0.020426321774721146,
            0.0015509583754464984,
            0.0397937074303627,
            0.0193952526897192,
            0.024620266631245613,
            0.04701119288802147,
            -0.011947865597903728,
            0.018127316609025,
            0.020217321813106537,
            -0.0008625627378933132,
            -0.0031837753485888243,
            -0.030346881598234177,
            -0.008234621956944466,
            -0.015995509922504425,
            0.014191138558089733,
            0.007067702244967222,
            -0.0076424540020525455,
            0.006956235505640507,
            0.01985505409538746,
            0.015006240457296371,
            0.011077029630541801,
            -0.003838643664494157,
            -0.0021370307076722383,
            0.0010876738233491778,
            -0.019172318279743195,
            -0.01256790105253458,
            0.01869858428835869,
            -0.018308449536561966,
            0.00748222041875124,
            -0.03380235657095909,
            -0.004695545881986618,
            0.0031559085473418236,
            -0.018865784630179405,
            0.011940899305045605,
            -0.00720355287194252,
            0.019074784591794014,
            -0.019269851967692375,
            0.010282827541232109,
            -0.016998711973428726,
            0.013821904547512531,
            -0.013229736126959324,
            -0.010888929478824139,
            -0.023463796824216843,
            -0.001105090486817062,
            0.01677577942609787,
            0.019966520369052887,
            -0.011620431207120419,
            0.0018583633936941624,
            -0.030514081940054893,
            0.015326708555221558,
            -0.008137088268995285,
            -0.029148612171411514,
            -0.003768976777791977,
            0.001748638111166656,
            0.007879321463406086,
            -0.016817579045891762,
            -0.04946346580982208,
            -0.034638360142707825,
            -0.013362103141844273,
            -0.009634925983846188,
            -0.001558795920573175,
            -0.016399577260017395,
            0.015298841521143913,
            -0.011042196303606033,
            -0.04135424643754959,
            -0.004496995359659195,
            -0.0026560488622635603,
            0.03023541532456875,
            0.022335194051265717,
            0.005200630519539118,
            0.012985901907086372,
            0.030904216691851616,
            0.013389969244599342,
            -0.011007362976670265,
            0.00028258617385290563,
            -0.019019052386283875,
            0.009920559823513031,
            0.011188496835529804,
            -0.023867864161729813,
            -0.0428311824798584,
            0.0299846138805151,
            0.027393007650971413,
            -0.017667515203356743,
            -0.029343679547309875,
            0.015856176614761353,
            -0.018782183527946472,
            -0.001386370393447578,
            0.00997629389166832,
            -0.011425363831222057,
            -0.021122990176081657,
            -0.009035791270434856,
            -0.012170800007879734,
            0.014295638538897038,
            0.01468577329069376,
            -0.005298164207488298,
            -0.017848648130893707,
            0.013013768941164017,
            -0.020119788125157356,
            0.005218047648668289,
            0.010881963185966015,
            -0.010847128927707672,
            0.024007197469472885,
            0.01471363939344883,
            0.022711394354701042,
            0.014267771504819393,
            0.03304995596408844,
            0.026334071531891823,
            -0.030709149315953255,
            0.018378116190433502,
            -0.013452669605612755,
            -0.003040958195924759,
            -0.007224452681839466,
            0.005360864568501711,
            0.004469128791242838,
            0.0304862167686224,
            -0.022711394354701042,
            -0.00927265826612711,
            0.004099894315004349,
            -0.009767292998731136,
            0.017653580754995346,
            0.008729256689548492,
            -0.00025994444149546325,
            0.004911513067781925,
            -0.011522897519171238,
            0.007928088307380676,
            0.034638360142707825,
            0.006123716477304697,
            -0.02176392637193203,
            -0.0007519666105508804,
            -0.0016493628500029445,
            0.0037445933558046818,
            -0.02690533921122551,
            0.009453792124986649,
            0.029510879889130592,
            -0.0418279804289341,
            -0.00914029125124216,
            0.027518408372998238,
            0.011334797367453575,
            0.011049163527786732,
            -0.029148612171411514,
            0.011090963147580624,
            0.013167035765945911,
            0.021025456488132477,
            -0.013410869985818863,
            -0.010206194594502449,
            -0.007503120228648186,
            0.015326708555221558,
            -0.03006821498274803,
            -0.017165912315249443,
            -0.019924720749258995,
            -0.011857299134135246,
            0.0158701092004776,
            0.021039390936493874,
            0.007565820589661598,
            -0.007872354239225388,
            0.010540595278143883,
            -0.029343679547309875,
            0.010241027921438217,
            0.009753359481692314,
            -0.004260128363966942,
            -0.00318725872784853,
            0.014769373461604118,
            -0.0038665104657411575,
            0.010450027883052826,
            0.0176257137209177,
            0.020133720710873604,
            -0.0065626176074147224,
            0.01000415999442339,
            -0.030597683042287827,
            -0.015897976234555244,
            -0.007955954410135746,
            0.004124277736991644,
            -0.00038055519689805806,
            -0.02800607495009899,
            0.006447667255997658,
            0.004775662906467915,
            0.00568133220076561,
            -0.0022397893480956554,
            -0.015354574657976627,
            -0.005329514387995005,
            0.002267656149342656,
            -0.011676165275275707,
            0.00816495530307293,
            0.016566777601838112,
            -0.023310529068112373,
            0.037007033824920654,
            0.0067054349929094315,
            0.04951919987797737,
            0.0018670717254281044,
            -0.01606517657637596,
            -0.03491702675819397,
            -0.026125071570277214,
            -0.017556047067046165,
            -0.026877472177147865,
            -0.03304995596408844,
            -0.025484135374426842,
            0.02790854312479496,
            0.019562453031539917,
            -0.027128273621201515,
            -0.03260408714413643,
            -0.018196983262896538,
            -0.019659986719489098,
            0.00997629389166832,
            0.01646924391388893,
            0.01779291406273842,
            0.010575428605079651,
            0.01617664471268654,
            -0.02841014415025711,
            0.03714636713266373,
            0.010087761096656322,
            -0.0006901372689753771,
            -0.013550203293561935,
            0.012581833638250828,
            0.011397497728466988,
            -0.01612091064453125,
            0.004803529474884272,
            0.02473173290491104,
            -0.04751279577612877,
            -0.0007872354472056031,
            0.03784303739666939,
            -0.006451150868088007,
            0.02403506450355053,
            0.03918064013123512,
            -0.019715720787644386,
            -0.03480556234717369,
            -0.01430957205593586,
            0.018322382122278214,
            0.033022090792655945,
            0.01692904531955719,
            0.003636609762907028,
            -0.0019855054561048746,
            -0.01342480257153511,
            0.0052772643975913525,
            0.013717403635382652,
            -0.026724206283688545,
            0.0027675158344209194,
            0.0054792980663478374,
            0.0037724601570516825,
            -0.02397933229804039,
            -0.017221646383404732,
            0.011676165275275707,
            0.012059332802891731,
            -0.014978373423218727,
            0.020523855462670326,
            -0.004420361947268248,
            -0.02967808023095131,
            0.0049393801018595695,
            0.02679387293756008,
            0.0015971126267686486,
            -0.020900055766105652,
            0.0110839968547225,
            -0.027936408296227455,
            0.0032029335852712393,
            0.02161065861582756,
            0.012958034873008728,
            -0.0052842311561107635,
            -0.002953874645754695,
            0.005691782105714083,
            -0.02573493681848049,
            -0.00224153115414083,
            -0.007691220846027136,
            0.0058694323524832726,
            -0.018461717292666435,
            0.04625879228115082,
            0.02917647920548916,
            -0.05105186998844147,
            0.017458513379096985,
            -0.013780103996396065,
            -0.008833757601678371,
            -0.019032984972000122,
            0.0032691170927137136,
            0.005033430177718401,
            -0.021136924624443054,
            0.02101152390241623,
            0.0037480767350643873,
            -0.03505636006593704,
            -0.0019611220341175795,
            -0.0012052366510033607,
            0.008680489845573902,
            -0.01047789491713047,
            0.02574886940419674,
            0.2039845585823059,
            -0.03405315801501274,
            0.01833631657063961,
            0.0029573580250144005,
            0.013780103996396065,
            0.0176257137209177,
            0.0037550434935837984,
            0.017709314823150635,
            -0.00217186426743865,
            -0.001844429993070662,
            0.00048462007544003427,
            0.008060455322265625,
            -0.013222768902778625,
            0.0024365982972085476,
            0.0224187932908535,
            0.0010824487544596195,
            -0.012066299095749855,
            -0.046175193041563034,
            -0.00687611848115921,
            -0.0056778485886752605,
            -0.005225013941526413,
            0.014699706807732582,
            -0.003213383723050356,
            -0.036505430936813354,
            0.014351372607052326,
            -0.012916235253214836,
            -0.006691501475870609,
            -0.026919273659586906,
            0.006419800687581301,
            0.03472195938229561,
            -0.007377719972282648,
            0.01858711615204811,
            -0.008241589181125164,
            0.015479975380003452,
            -0.009690659120678902,
            -0.020384522154927254,
            -0.02272532880306244,
            0.02222372591495514,
            0.011056129820644855,
            -0.009425925090909004,
            0.035084228962659836,
            0.010401261039078236,
            -0.008025621995329857,
            -0.012331033125519753,
            -0.00031154145835898817,
            -0.00023947982117533684,
            0.000322644627885893,
            -0.013814937323331833,
            -0.002095230622217059,
            0.03023541532456875,
            -0.010512728244066238,
            0.02654307149350643,
            0.039124906063079834,
            0.042691849172115326,
            0.0030461831483989954,
            -0.014267771504819393,
            0.002694365568459034,
            0.00595651613548398,
            0.005886849481612444,
            0.01949278637766838,
            -0.025762803852558136,
            0.0186428502202034,
            0.002546323463320732,
            0.03112715110182762,
            0.03179595246911049,
            0.015856176614761353,
            -0.019882921129465103,
            0.012393733486533165,
            0.015730775892734528,
            -0.004723412916064262,
            -0.015382441692054272,
            -0.016817579045891762,
            -0.005925165954977274,
            -0.004653745796531439,
            -0.03502849489450455,
            -0.02126232348382473,
            0.051414139568805695,
            0.01818304881453514,
            0.009565259329974651,
            0.034081026911735535,
            -0.018754318356513977,
            -0.005834599025547504,
            0.011369630694389343,
            -0.014894773252308369,
            -0.026779938489198685,
            -0.043054115027189255,
            0.017305247485637665,
            -0.01395427156239748,
            -0.017946181818842888,
            -0.016483178362250328,
            0.0023791231215000153,
            -0.021081190556287766,
            -0.008959157392382622,
            0.008617790415883064,
            -0.0004689450142905116,
            -0.010735662654042244,
            -0.019172318279743195,
            0.03940357267856598,
            -0.0014569080667570233,
            -0.021596726030111313,
            -0.03452689200639725,
            0.04703906178474426,
            0.03842823579907417,
            0.02644553780555725,
            -0.0008155376417562366,
            0.004946346860378981,
            -0.01019922737032175,
            -0.008157989010214806,
            0.022502394393086433,
            -0.0013027702225372195,
            0.001785213127732277,
            -0.012386767193675041,
            0.0019053885480389,
            0.007997754961252213,
            -0.0018931968370452523,
            -0.011327831074595451,
            0.005221530795097351,
            -0.014393172226846218,
            0.012637567706406116,
            -0.005221530795097351,
            0.0005899911629967391,
            -0.03388595953583717,
            -0.039821576327085495,
            0.017946181818842888,
            -0.0014830331783741713,
            -0.004685095977038145,
            -0.016413511708378792,
            -0.02619473822414875,
            -0.01430957205593586,
            -0.007405586540699005,
            0.011327831074595451,
            -0.03265982121229172,
            0.03530716150999069,
            -0.021582791581749916,
            -0.02906501293182373,
            0.00651036761701107,
            0.01950671896338463,
            -0.017542114481329918,
            -0.01779291406273842,
            0.009941460564732552,
            0.009460759349167347,
            -5.8073856052942574e-05,
            0.006141133140772581,
            -0.006984102074056864,
            0.01627417653799057,
            -0.0468439944088459,
            0.0076215541921556,
            -0.014434972777962685,
            -0.0014351372374221683,
            -0.03625463321805,
            -0.01478330697864294,
            -0.007649420760571957,
            0.0015466042095795274,
            -0.019311651587486267,
            0.023463796824216843,
            -0.002380864694714546,
            -0.018670717254281044,
            -0.015159507282078266,
            -0.013264569453895092,
            0.0021109057124704123,
            -0.028786344453692436,
            -0.015591442584991455,
            -0.0022641727700829506,
            -0.025219401344656944,
            -0.009077590890228748,
            -0.009579192847013474,
            -0.17745541036128998,
            0.010847128927707672,
            0.02076072245836258,
            -0.032074619084596634,
            -0.005552448332309723,
            0.02711433917284012,
            0.032269686460494995,
            -0.009850893169641495,
            -0.020482055842876434,
            0.003523401217535138,
            -0.0010632904013618827,
            -0.017012646421790123,
            -0.02856341004371643,
            -0.0007136498461477458,
            0.004350695293396711,
            0.01306950207799673,
            0.0018235299503430724,
            0.02045418880879879,
            0.024898933246731758,
            0.02825687639415264,
            0.03143368661403656,
            -0.044614654034376144,
            0.013536269776523113,
            -0.007321986369788647,
            0.02064925618469715,
            -0.009767292998731136,
            -0.010171361267566681,
            0.03137795254588127,
            -0.017151979729533195,
            -0.014671839773654938,
            0.012128999456763268,
            0.02725367434322834,
            0.012101132422685623,
            -0.004552728962153196,
            0.00025515485322102904,
            0.020886123180389404,
            0.01123029738664627,
            -0.01334816962480545,
            -0.0011651781387627125,
            -0.008415755815804005,
            0.05252880975604057,
            -0.00992752704769373,
            -0.0076145874336361885,
            -0.014121471904218197,
            -0.023644929751753807,
            -0.011097930371761322,
            0.007670320570468903,
            -0.0022746229078620672,
            0.016009442508220673,
            0.018364183604717255,
            -0.01748638041317463,
            -0.020384522154927254,
            0.020635321736335754,
            0.006398900877684355,
            0.007005002349615097,
            0.02679387293756008,
            0.011662231758236885,
            0.00821372214704752,
            -0.026166871190071106,
            0.015535708516836166,
            0.004977697040885687,
            -0.033022090792655945,
            0.026013603433966637,
            0.002319906372576952,
            -0.014978373423218727,
            -0.0016868087695911527,
            0.0018479133723303676,
            0.004333278629928827,
            -0.021986858919262886,
            0.011202430352568626,
            -0.03647756576538086,
            -0.023463796824216843,
            0.0003302644472569227,
            0.0047408295795321465,
            0.022599928081035614,
            -0.00012757742661051452,
            -0.019367385655641556,
            -0.0012609701370820403,
            0.001722512999549508,
            -0.01123029738664627,
            -0.033022090792655945,
            0.0317402184009552,
            -0.0096976263448596,
            0.026975005865097046,
            0.0032081587705761194,
            -0.012087199836969376,
            0.005521098151803017,
            0.018322382122278214,
            -0.019715720787644386,
            -0.026431605219841003,
            0.011432331055402756,
            0.0060226996429264545,
            0.017542114481329918,
            0.0019785386975854635,
            0.007412553299218416,
            0.01779291406273842,
            0.010220128111541271,
            -0.015284908004105091,
            0.022181926295161247,
            -0.012853534892201424,
            -0.02105332352221012,
            -0.017347047105431557,
            0.006381483748555183,
            0.009307491593062878,
            0.012783868238329887,
            0.02533086948096752,
            -0.019618187099695206,
            0.006694984622299671,
            0.029037145897746086,
            0.011481097899377346,
            -0.006043599918484688,
            -0.010833196341991425,
            0.00023664960463065654,
            0.02629227191209793,
            0.017820781096816063,
            0.009133324958384037,
            -0.03193528577685356,
            -0.0027918992564082146,
            0.005256364122033119,
            -0.02599967084825039,
            0.043499983847141266,
            -0.010603295639157295,
            0.003612226340919733,
            0.014978373423218727,
            -0.0007920250645838678,
            -0.01804371550679207,
            -0.11425364017486572,
            -0.008206755854189396,
            -0.002555032027885318,
            0.004354178439825773,
            -0.014490705914795399,
            -0.004120794590562582,
            0.02172212488949299,
            0.029483012855052948,
            -0.026027537882328033,
            0.010624195449054241,
            -0.020579589530825615,
            -0.02711433917284012,
            0.013090401887893677,
            0.004803529474884272,
            -0.005319064483046532,
            0.008185855112969875,
            0.0033701341599226,
            -0.0047582462430000305,
            -0.009251758456230164,
            0.026682404801249504,
            -0.0026473405305296183,
            -0.01959032006561756,
            0.014393172226846218,
            -0.024048998951911926,
            0.015842242166399956,
            0.00677858479321003,
            -0.029705947265028954,
            0.0015788250602781773,
            0.021889325231313705,
            0.011028262786567211,
            0.007398620247840881,
            -0.03455476090312004,
            -0.00881285686045885,
            -0.003410192672163248,
            -0.0019837636500597,
            -0.003326592268422246,
            -0.012909268029034138,
            -0.022558128461241722,
            0.005782349035143852,
            -0.03405315801501274,
            0.0029051078017801046,
            0.013292435556650162,
            -0.004169561434537172,
            -0.010157427750527859,
            -0.010136527009308338,
            -0.00012942795001436025,
            -0.014434972777962685,
            0.030151814222335815,
            0.023770330473780632,
            -0.00803258828818798,
            -0.012198666110634804,
            -0.02045418880879879,
            -0.02499646693468094,
            -0.005367831327021122,
            0.020941857248544693,
            0.033133555203676224,
            0.0186428502202034,
            0.013598970137536526,
            -0.004517895635217428,
            -0.012965002097189426,
            -0.000530774355866015,
            -0.005092646926641464,
            -0.01698477938771248,
            0.005144897382706404,
            0.018628917634487152,
            -0.001567504252307117,
            -0.05473028123378754,
            -0.011167597025632858,
            -0.008673523552715778,
            -0.005350414663553238,
            -0.014156305231153965,
            0.01793224923312664,
            0.013194902800023556,
            0.02155492454767227,
            -0.01858711615204811,
            -0.01012259442359209,
            -0.014281705021858215,
            -0.0021091641392558813,
            0.0022066975943744183,
            -0.018726451322436333,
            -0.01090286299586296,
            -0.0274487417191267,
            0.007698187604546547,
            -0.002624698681756854,
            -0.010220128111541271,
            -0.012804768048226833,
            -0.0012182991486042738,
            -0.02499646693468094,
            -0.011745831929147243,
            -0.029761681333184242,
            0.009662793017923832,
            0.008645656518638134,
            -0.029761681333184242,
            -0.008234621956944466,
            -0.004180011339485645,
            -0.0024644650984555483,
            -0.0031994504388421774,
            -0.01390550471842289,
            0.020217321813106537,
            0.005987866315990686,
            -0.015075907111167908,
            0.0043750787153840065,
            -0.04391798377037048,
            0.016162710264325142,
            0.0032952423207461834,
            -0.01698477938771248,
            0.011655264534056187,
            0.018823985010385513,
            0.03692343458533287,
            -0.017040513455867767,
            0.002717007417231798,
            0.008854657411575317,
            -0.007363786455243826,
            0.021485257893800735,
            -0.01738884672522545,
            -0.018977250903844833,
            -0.006447667255997658,
            -0.010993429459631443,
            0.005925165954977274,
            -0.004507445730268955,
            0.029204346239566803,
            0.005803248845040798,
            -0.01499230694025755,
            -0.016538910567760468,
            0.009572225622832775,
            -0.00025798508431762457,
            -0.007774821016937494,
            0.013828870840370655,
            -0.010234060697257519,
            0.026320137083530426,
            -0.026612738147377968,
            -0.011780665256083012,
            0.020077988505363464,
            -0.02403506450355053,
            -0.0008969607879407704,
            0.030040347948670387,
            -0.0010589362354949117,
            0.016664311289787292,
            0.02423013187944889,
            0.008596889674663544,
            0.03622676432132721,
            0.004639812745153904,
            -0.014755439944565296,
            -0.03600383177399635,
            0.0020307889208197594,
            -0.012540034018456936,
            -0.00260728201828897,
            0.022251592949032784,
            -0.0027222323697060347,
            0.003500759368762374,
            0.015479975380003452,
            -0.006398900877684355,
            0.008778023533523083,
            0.02091399021446705,
            0.019074784591794014,
            -0.029009278863668442,
            -0.015953710302710533,
            -0.00849238969385624,
            -0.0035286261700093746,
            0.02278106100857258,
            0.002276364481076598,
            -0.0034415426198393106,
            0.0367841012775898,
            -0.015048041008412838,
            0.022279459983110428,
            0.003981460817158222,
            0.0076424540020525455,
            -0.008750157430768013,
            0.012428566813468933,
            0.007039835676550865,
            -0.02912074513733387,
            -0.02827080897986889,
            -0.010568462312221527,
            0.009251758456230164,
            -0.011773698963224888,
            0.02565133571624756,
            0.029037145897746086,
            0.005252880975604057,
            -0.0007088602287694812,
            0.0114601980894804,
            -0.005998316220939159,
            0.01478330697864294,
            0.012484300881624222,
            -0.02459239959716797,
            -0.016845446079969406,
            -0.014448905363678932,
            0.024453066289424896,
            0.002779707545414567,
            -0.004451712127774954,
            0.012170800007879734,
            -0.005378281231969595,
            0.02195899188518524,
            0.012184732593595982,
            0.001861846656538546,
            0.009760326705873013,
            0.03555796295404434,
            0.00401629414409399,
            0.01490870676934719,
            0.007349853403866291,
            -0.019269851967692375,
            0.006796001922339201,
            -0.021749991923570633,
            0.006127200089395046,
            0.015828309580683708,
            -0.012463400140404701,
            -0.010094727389514446,
            -0.023157263174653053,
            0.021136924624443054,
            -0.013229736126959324,
            -0.00457362923771143,
            0.003953594248741865,
            0.03770370036363602,
            -0.0011442780960351229,
            0.015298841521143913,
            -0.01219169981777668,
            0.01080532930791378,
            -0.034276094287633896,
            0.02765774168074131,
            -0.007210519630461931,
            -0.036505430936813354,
            -0.026570938527584076,
            0.033022090792655945,
            0.021931126713752747,
            -0.01518737431615591,
            0.044558919966220856,
            -0.012212599627673626,
            0.018559250980615616,
            0.010819262824952602,
            0.024717800319194794,
            -0.011906065978109837,
            0.02638980560004711,
            0.02278106100857258,
            0.017597848549485207,
            -0.008680489845573902,
            0.01989685371518135,
            -0.01874038390815258,
            -0.021136924624443054,
            -0.012310133315622807,
            -0.006607901304960251,
            0.03279915452003479,
            -0.0008817211491987109,
            0.10160214453935623,
            0.021248390898108482,
            -0.0030949499923735857,
            0.012115065939724445,
            -0.015702908858656883,
            0.011871231719851494,
            0.013362103141844273,
            0.018921518698334694,
            -0.011474130675196648,
            -0.0010545820696279407,
            0.006339683663100004,
            -0.015591442584991455,
            -0.013689536601305008,
            -0.02423013187944889,
            0.0035355929285287857,
            -0.015605375170707703,
            0.003765493631362915,
            0.008617790415883064,
            -0.008610823191702366,
            -0.0030200581531971693,
            0.024299798533320427,
            -0.00043737722444348037,
            0.011223330162465572,
            0.01602337695658207,
            -0.013118268921971321,
            -0.009669759310781956,
            0.007018935866653919,
            -0.016455311328172684,
            -0.008673523552715778,
            -0.03112715110182762,
            0.004406428430229425,
            -0.016566777601838112,
            -0.04166077822446823,
            -0.013571103103458881,
            0.02815934270620346,
            0.01097949594259262,
            -0.001757346442900598,
            0.013341202400624752,
            0.014121471904218197,
            5.785614484921098e-05,
            0.009293558076024055,
            0.036449696868658066,
            -0.0029138161335140467,
            -0.00964885950088501,
            -0.013647736981511116,
            -0.007997754961252213,
            0.0014725831570103765,
            -0.013731337152421474,
            -0.02312939614057541
          ],
          "start_time": 1695246509.205183,
          "end_time": 1695246509.980058,
          "error": null,
          "children": [
            {
              "name": "openai.api_resources.embedding.Embedding.create",
              "type": "LLM",
              "inputs": {
                "input": "How are sentence pairs represented in the input to BERT?",
                "engine": "embedding"
              },
              "output": {
                "object": "list",
                "data": [
                  {
                    "object": "embedding",
                    "index": 0,
                    "embedding": [
                      -0.011118830181658268,
                      -0.01631597802042961,
                      0.02559560351073742,
                      -0.002060397295281291,
                      -0.005500198341906071,
                      0.031210750341415405,
                      0.015424241311848164,
                      -0.011599531397223473,
                      -0.01773718185722828,
                      -0.027727408334612846,
                      0.028535543009638786,
                      0.04366718605160713,
                      -0.035139963030815125,
                      0.005427048075944185,
                      -0.0009474692051298916,
                      0.008422723039984703,
                      0.011244230903685093,
                      0.009614026173949242,
                      0.027797074988484383,
                      -0.016956912353634834,
                      -0.0016798420110717416,
                      -0.006102816667407751,
                      -0.016552845016121864,
                      -0.0035739096347242594,
                      -0.010742628946900368,
                      -0.00849238969385624,
                      0.029538746923208237,
                      -0.020328788086771965,
                      -0.003713243408128619,
                      -0.009676726534962654,
                      0.0021788307931274176,
                      0.02357526309788227,
                      -0.017918314784765244,
                      -0.009948426857590675,
                      -0.009258724749088287,
                      0.006694984622299671,
                      -0.0029294912237674,
                      -0.016915112733840942,
                      0.02438339963555336,
                      -0.023212995380163193,
                      0.0071164690889418125,
                      0.021694257855415344,
                      0.003977977205067873,
                      -0.015270974487066269,
                      -0.005953032989054918,
                      0.01672004535794258,
                      0.010652062483131886,
                      -0.0065695843659341335,
                      -0.012059332802891731,
                      0.0093214251101017,
                      0.00438204500824213,
                      0.015856176614761353,
                      -0.03945930674672127,
                      -0.020997589454054832,
                      -0.02251632697880268,
                      -0.0034084508661180735,
                      -0.002352998126298189,
                      0.032966356724500656,
                      -0.0272118728607893,
                      -0.013543237000703812,
                      0.0005050846957601607,
                      -0.008130121976137161,
                      0.0009370191837660968,
                      0.008053488098084927,
                      -0.022404860705137253,
                      -0.024466998875141144,
                      -0.008548122830688953,
                      0.008018654771149158,
                      0.019437052309513092,
                      0.007398620247840881,
                      0.01959032006561756,
                      0.010038994252681732,
                      0.0009866568725556135,
                      -0.001406399649567902,
                      0.0428033173084259,
                      -0.01388460397720337,
                      0.002535873558372259,
                      -0.014755439944565296,
                      0.001035423600114882,
                      0.03592022880911827,
                      0.012735101394355297,
                      -0.002624698681756854,
                      -0.0010606779251247644,
                      0.026487339287996292,
                      0.015897976234555244,
                      -0.012742067687213421,
                      0.010436095297336578,
                      0.025275135412812233,
                      0.002154447603970766,
                      0.0040302276611328125,
                      0.027379073202610016,
                      0.03536289557814598,
                      0.012944101355969906,
                      0.0067054349929094315,
                      -0.0038839271292090416,
                      0.012247432954609394,
                      -0.01118153054267168,
                      0.038818370550870895,
                      0.02634800411760807,
                      -0.009523458778858185,
                      -0.010157427750527859,
                      0.008513289503753185,
                      -0.04497692361474037,
                      -0.005552448332309723,
                      -0.024606334045529366,
                      0.014852973632514477,
                      0.010749596171081066,
                      -0.01130693033337593,
                      0.003262150567024946,
                      0.001316703506745398,
                      -0.019074784591794014,
                      0.030207548290491104,
                      0.006221250165253878,
                      -0.024216199293732643,
                      -0.0005220660241320729,
                      -0.002276364481076598,
                      0.015479975380003452,
                      -0.039319973438978195,
                      -0.009669759310781956,
                      0.012602734379470348,
                      0.017165912315249443,
                      -0.004075510893017054,
                      0.010721729137003422,
                      -0.034582626074552536,
                      0.03288275748491287,
                      -0.003422384150326252,
                      -0.010819262824952602,
                      -0.03380235657095909,
                      -0.006520817521959543,
                      -0.04288691654801369,
                      0.027420874685049057,
                      0.022641727700829506,
                      0.015939775854349136,
                      -0.022753195837140083,
                      0.005636048503220081,
                      0.01854531653225422,
                      -0.02760200761258602,
                      -0.02050992287695408,
                      -0.01959032006561756,
                      -0.004643295891582966,
                      0.02257206104695797,
                      0.003828193759545684,
                      -0.014253838919103146,
                      -0.023993264883756638,
                      -0.010192261077463627,
                      0.018601050600409508,
                      0.018782183527946472,
                      0.020412389189004898,
                      0.0024139564484357834,
                      -0.010408228263258934,
                      0.026334071531891823,
                      -0.001065902877599001,
                      -0.029761681333184242,
                      0.013807971030473709,
                      0.007726054172962904,
                      0.0011695323046296835,
                      0.00037707184674218297,
                      0.009753359481692314,
                      -0.009732459671795368,
                      -0.005601215176284313,
                      -0.020412389189004898,
                      0.017012646421790123,
                      0.017096245661377907,
                      -0.009307491593062878,
                      0.024118665605783463,
                      0.03349582478404045,
                      0.0020900056697428226,
                      -0.014866907149553299,
                      0.007328953128308058,
                      -0.029204346239566803,
                      -0.026222603395581245,
                      0.018252715468406677,
                      -0.01959032006561756,
                      -0.003438059240579605,
                      0.0040128109976649284,
                      0.00180088821798563,
                      0.03391382470726967,
                      -0.02806180901825428,
                      -0.02987314760684967,
                      -0.015619308687746525,
                      0.0193952526897192,
                      0.0065521677024662495,
                      0.020119788125157356,
                      0.023547397926449776,
                      -0.03098781779408455,
                      -0.02559560351073742,
                      -0.003594809677451849,
                      -0.01974358595907688,
                      0.01012259442359209,
                      -0.034192491322755814,
                      0.02382606454193592,
                      0.007102536037564278,
                      0.000681428937241435,
                      -0.01843385025858879,
                      -0.6148517727851868,
                      -0.03820530325174332,
                      -0.009300525300204754,
                      -0.028535543009638786,
                      0.01698477938771248,
                      0.005514131393283606,
                      -0.011627398431301117,
                      0.03349582478404045,
                      -0.024104731157422066,
                      0.033579424023628235,
                      -0.0008451460162177682,
                      -0.023296596482396126,
                      -0.0038212270010262728,
                      0.007733020931482315,
                      -0.010498794727027416,
                      -0.012992868199944496,
                      0.006503400858491659,
                      0.004162594676017761,
                      0.00922389142215252,
                      0.022335194051265717,
                      -0.03500062972307205,
                      0.00739165348932147,
                      -0.026069337502121925,
                      -0.004152144771069288,
                      -0.025372669100761414,
                      0.0031245583668351173,
                      0.006506884470582008,
                      -0.021095123142004013,
                      -0.009816059842705727,
                      0.017151979729533195,
                      -0.02045418880879879,
                      -0.001589275081641972,
                      0.021889325231313705,
                      -0.030207548290491104,
                      0.025010401383042336,
                      0.021471325308084488,
                      0.0021439974661916494,
                      0.009084558114409447,
                      -0.0007767854258418083,
                      0.027880676090717316,
                      -0.01596764288842678,
                      -0.002070847200229764,
                      -0.0024035065434873104,
                      -0.0031228167936205864,
                      0.017360979691147804,
                      -0.000540788983926177,
                      0.0186428502202034,
                      -0.010498794727027416,
                      0.006841285154223442,
                      -0.004591045901179314,
                      -0.008708356879651546,
                      -0.003521659644320607,
                      -0.007572787348181009,
                      -0.002574190264567733,
                      0.03550222888588905,
                      0.011418397538363934,
                      0.0342482253909111,
                      -0.0024975568521767855,
                      0.0014272996922954917,
                      -0.011919998563826084,
                      0.008631723001599312,
                      0.017305247485637665,
                      -0.012303167022764683,
                      -0.01918625272810459,
                      0.018823985010385513,
                      0.009565259329974651,
                      -0.006287433672696352,
                      0.003291758941486478,
                      0.00914725847542286,
                      -0.04558999091386795,
                      0.0003217737830709666,
                      0.015229174867272377,
                      0.0004941992810927331,
                      -0.015284908004105091,
                      0.00554199842736125,
                      0.012338000349700451,
                      0.0033718757331371307,
                      0.011188496835529804,
                      -0.002103938953951001,
                      0.019757520407438278,
                      -0.017444580793380737,
                      -0.012247432954609394,
                      -0.028842078521847725,
                      0.002976516494527459,
                      -0.0024052481167018414,
                      0.0001037927286233753,
                      -0.01738884672522545,
                      0.017193779349327087,
                      -0.01478330697864294,
                      -0.0036505432799458504,
                      0.0033718757331371307,
                      0.03199101984500885,
                      -0.028842078521847725,
                      -0.042050912976264954,
                      0.004918479826301336,
                      0.011376597918570042,
                      0.013125235214829445,
                      -0.0074613201431930065,
                      0.03112715110182762,
                      -0.03722996637225151,
                      0.014246871694922447,
                      -0.020565655082464218,
                      0.012351933866739273,
                      -0.005886849481612444,
                      0.02675207331776619,
                      0.03600383177399635,
                      -0.007475253660231829,
                      0.0011259905295446515,
                      -0.025707069784402847,
                      0.0035303679760545492,
                      -0.005911232437938452,
                      -0.010401261039078236,
                      -0.0035390763077884912,
                      -0.0003302644472569227,
                      -0.014212038367986679,
                      -0.04040677472949028,
                      0.011843365617096424,
                      0.01065902877599001,
                      0.01631597802042961,
                      -0.003516434459015727,
                      0.02599967084825039,
                      0.01843385025858879,
                      0.015521774999797344,
                      -0.02886994555592537,
                      0.01868464983999729,
                      0.012045399285852909,
                      0.0024522733874619007,
                      0.006210800260305405,
                      0.020844323560595512,
                      0.02886994555592537,
                      -0.011495031416416168,
                      0.02172212488949299,
                      0.010213160887360573,
                      -0.010213160887360573,
                      -0.002133547328412533,
                      0.01571684330701828,
                      0.0012836117530241609,
                      -0.007726054172962904,
                      0.01097949594259262,
                      -0.0035094679333269596,
                      -0.02130412496626377,
                      -0.008144055493175983,
                      -0.00489758001640439,
                      -0.012553967535495758,
                      -0.028591277077794075,
                      -0.015647174790501595,
                      -0.021931126713752747,
                      -0.0011024780105799437,
                      0.0001773239200701937,
                      -0.02266959473490715,
                      0.011599531397223473,
                      -0.018475649878382683,
                      -0.02528906799852848,
                      -0.017347047105431557,
                      -0.023394130170345306,
                      -0.014616106636822224,
                      -0.011655264534056187,
                      -0.032213952392339706,
                      -0.008617790415883064,
                      -0.029092878103256226,
                      0.006754201836884022,
                      0.02534480206668377,
                      -0.027588075026869774,
                      -0.017765048891305923,
                      -0.009676726534962654,
                      -0.013877637684345245,
                      0.0418279804289341,
                      0.030792750418186188,
                      0.0045875622890889645,
                      -0.016190577298402786,
                      0.013034668751060963,
                      -0.03792663663625717,
                      0.0009213441517204046,
                      0.02201472595334053,
                      0.0011190238874405622,
                      0.01848958246409893,
                      -0.02605540305376053,
                      0.009906627237796783,
                      -0.00020399325876496732,
                      -0.032576221972703934,
                      -0.009857860393822193,
                      0.040573976933956146,
                      -0.002218889305368066,
                      0.006458117626607418,
                      0.012233499437570572,
                      0.0017199005233123899,
                      0.003911793697625399,
                      -0.0036261598579585552,
                      -0.021025456488132477,
                      0.015842242166399956,
                      0.011237263679504395,
                      0.019534585997462273,
                      -0.017904382199048996,
                      0.01627417653799057,
                      -0.004322828259319067,
                      0.02237699367105961,
                      0.007948988117277622,
                      0.027615942060947418,
                      -0.014532506465911865,
                      0.018754318356513977,
                      0.025163667276501656,
                      0.009850893169641495,
                      -0.0022711395286023617,
                      -0.0011756281601265073,
                      -0.006064499728381634,
                      -0.00982302613556385,
                      -0.0011277322191745043,
                      -0.03553009778261185,
                      0.023338396102190018,
                      0.0015709876315668225,
                      0.02220979332923889,
                      -0.02906501293182373,
                      -0.033217158168554306,
                      -0.01804371550679207,
                      -0.008206755854189396,
                      0.0012374575017020106,
                      -0.009084558114409447,
                      0.03265982121229172,
                      -0.0279642753303051,
                      0.00955132581293583,
                      -0.006064499728381634,
                      -0.012909268029034138,
                      0.038706906139850616,
                      0.012254400178790092,
                      0.006893535144627094,
                      0.006043599918484688,
                      -0.0029660663567483425,
                      -0.00960009265691042,
                      0.015521774999797344,
                      -0.020342722535133362,
                      0.019228052347898483,
                      0.02076072245836258,
                      0.029956746846437454,
                      -0.0038665104657411575,
                      -0.015424241311848164,
                      -0.010533628053963184,
                      0.02630620449781418,
                      0.009356258437037468,
                      0.03279915452003479,
                      0.003523401217535138,
                      -0.012156866490840912,
                      0.0030531499069184065,
                      -0.004110344685614109,
                      -0.0040406775660812855,
                      0.053977880626916885,
                      -0.01005292683839798,
                      0.01995258778333664,
                      0.030820617452263832,
                      -0.01141143124550581,
                      -0.0074404203332960606,
                      0.017263446003198624,
                      -0.011947865597903728,
                      -0.017360979691147804,
                      0.007551887072622776,
                      0.017207713797688484,
                      0.0009274400072172284,
                      0.024801399558782578,
                      0.0003629207785706967,
                      -0.011195464059710503,
                      0.010011127218604088,
                      0.004267094656825066,
                      -0.01249823346734047,
                      0.02024518884718418,
                      0.01787651516497135,
                      0.023561330512166023,
                      -0.014643972739577293,
                      -0.030151814222335815,
                      -0.028298676013946533,
                      -0.012017532251775265,
                      0.00023577875981573015,
                      -0.017514247447252274,
                      -0.004256644751876593,
                      -0.010247994214296341,
                      0.006573067978024483,
                      0.003876960603520274,
                      0.004364628344774246,
                      -0.02363099716603756,
                      0.01733311451971531,
                      0.005022980272769928,
                      0.02181965857744217,
                      -0.006931852083653212,
                      -0.020886123180389404,
                      0.028131475672125816,
                      -0.0040580942295491695,
                      -0.011146697215735912,
                      -0.015438174828886986,
                      -0.019910788163542747,
                      -0.010617229156196117,
                      -0.028075743466615677,
                      0.017597848549485207,
                      0.008882524445652962,
                      0.0077887545339763165,
                      -0.020273054018616676,
                      -0.003695826744660735,
                      -0.002946907887235284,
                      0.0015431208303198218,
                      0.008861623704433441,
                      -0.01118153054267168,
                      0.013076468370854855,
                      0.01125816348940134,
                      -0.012045399285852909,
                      0.005364347714930773,
                      -0.016761844977736473,
                      0.014448905363678932,
                      0.049909334629774094,
                      -0.006653184536844492,
                      -0.02172212488949299,
                      -0.01985505409538746,
                      -0.018768250942230225,
                      -0.018670717254281044,
                      0.02332446351647377,
                      0.010387328453361988,
                      -0.019116584211587906,
                      0.01153683103621006,
                      0.024759599938988686,
                      0.02372853085398674,
                      -0.009425925090909004,
                      0.0016868087695911527,
                      0.011919998563826084,
                      -0.0067054349929094315,
                      -0.017402781173586845,
                      -0.03753650188446045,
                      -0.029455145820975304,
                      0.0005564640159718692,
                      0.025804603472352028,
                      0.06325750797986984,
                      0.0035094679333269596,
                      0.0036993101239204407,
                      -0.019464919343590736,
                      0.0037480767350643873,
                      -0.018015848472714424,
                      -0.01592584326863289,
                      0.0011303446954116225,
                      -0.015633242204785347,
                      -0.021485257893800735,
                      -0.00836002267897129,
                      0.0035652013029903173,
                      0.0056499820202589035,
                      0.02126232348382473,
                      0.0009013149538077414,
                      0.0013184451963752508,
                      -0.05768415704369545,
                      0.02559560351073742,
                      -0.000957048439886421,
                      0.003974494058638811,
                      -0.014337439090013504,
                      0.020816456526517868,
                      0.03461049497127533,
                      0.005099613685160875,
                      -0.01752818003296852,
                      0.01899118535220623,
                      0.02509400062263012,
                      0.018712516874074936,
                      -0.04010024294257164,
                      0.0060157328844070435,
                      0.006917918566614389,
                      -0.008798924274742603,
                      0.00041408237302675843,
                      -0.013139168731868267,
                      0.022599928081035614,
                      0.01708231307566166,
                      0.006802968215197325,
                      0.03380235657095909,
                      0.0037202101666480303,
                      -0.003915277309715748,
                      0.012066299095749855,
                      0.008645656518638134,
                      -0.006245633587241173,
                      -0.0037027932703495026,
                      0.00836002267897129,
                      -0.0035808763932436705,
                      0.018503516912460327,
                      0.02282286249101162,
                      0.0013524078531190753,
                      0.007238386198878288,
                      -0.014379238709807396,
                      -0.01783471554517746,
                      -0.004451712127774954,
                      -0.0074891867116093636,
                      -0.007384686730802059,
                      -0.011982698924839497,
                      -0.009405025281012058,
                      -0.025261200964450836,
                      -0.023254796862602234,
                      -0.003335300600156188,
                      -0.00876409001648426,
                      0.018475649878382683,
                      -0.01139053050428629,
                      0.009614026173949242,
                      -0.005716165527701378,
                      -0.003869993845000863,
                      -0.017165912315249443,
                      -0.013362103141844273,
                      -0.0016040793852880597,
                      -0.02634800411760807,
                      -0.006130683235824108,
                      -0.04840452969074249,
                      -0.020328788086771965,
                      0.023686731234192848,
                      0.024397332221269608,
                      0.029148612171411514,
                      -0.013313336297869682,
                      0.005172763951122761,
                      -0.00545143149793148,
                      -0.010059894062578678,
                      -0.008018654771149158,
                      0.016538910567760468,
                      -0.005611665081232786,
                      -0.016455311328172684,
                      0.02609720453619957,
                      -0.034136757254600525,
                      -0.026111137121915817,
                      -0.01950671896338463,
                      0.02161065861582756,
                      -0.011550764553248882,
                      0.01779291406273842,
                      -0.01468577329069376,
                      -0.022237660363316536,
                      -0.0067054349929094315,
                      0.029287945479154587,
                      -0.004845329560339451,
                      0.02746267430484295,
                      -0.0020969724282622337,
                      -0.02605540305376053,
                      0.020872190594673157,
                      -0.03722996637225151,
                      0.0004397720331326127,
                      -0.03472195938229561,
                      -0.012888368219137192,
                      -0.003988427575677633,
                      -0.0021770892199128866,
                      0.012331033125519753,
                      -0.014072705060243607,
                      -0.003828193759545684,
                      -0.0120941661298275,
                      -0.01315310224890709,
                      -0.01251913420855999,
                      -0.0035025011748075485,
                      0.046119458973407745,
                      0.013055568560957909,
                      0.004500478971749544,
                      -0.014797240495681763,
                      0.009202991612255573,
                      -0.020523855462670326,
                      -0.006771618500351906,
                      -0.015340641140937805,
                      0.02553986944258213,
                      0.00311759184114635,
                      -0.014866907149553299,
                      0.03867903724312782,
                      -0.01282566785812378,
                      -0.011926965788006783,
                      -0.015438174828886986,
                      0.035335030406713486,
                      -0.0234359297901392,
                      0.045617856085300446,
                      0.0035460428334772587,
                      0.0027309407014399767,
                      -0.017500314861536026,
                      0.008276422508060932,
                      -0.005280747544020414,
                      -0.010895895771682262,
                      -0.03148941695690155,
                      0.018614983186125755,
                      0.026361938565969467,
                      -0.012303167022764683,
                      -0.003533851122483611,
                      -0.022530261427164078,
                      -0.01612091064453125,
                      -0.020091921091079712,
                      -0.024271931499242783,
                      0.04508838802576065,
                      -0.0038978606462478638,
                      0.003025283105671406,
                      0.007168719545006752,
                      0.009363225661218166,
                      -0.0028371827211230993,
                      -0.0002782320079859346,
                      -0.037257835268974304,
                      -0.020426321774721146,
                      0.0015509583754464984,
                      0.0397937074303627,
                      0.0193952526897192,
                      0.024620266631245613,
                      0.04701119288802147,
                      -0.011947865597903728,
                      0.018127316609025,
                      0.020217321813106537,
                      -0.0008625627378933132,
                      -0.0031837753485888243,
                      -0.030346881598234177,
                      -0.008234621956944466,
                      -0.015995509922504425,
                      0.014191138558089733,
                      0.007067702244967222,
                      -0.0076424540020525455,
                      0.006956235505640507,
                      0.01985505409538746,
                      0.015006240457296371,
                      0.011077029630541801,
                      -0.003838643664494157,
                      -0.0021370307076722383,
                      0.0010876738233491778,
                      -0.019172318279743195,
                      -0.01256790105253458,
                      0.01869858428835869,
                      -0.018308449536561966,
                      0.00748222041875124,
                      -0.03380235657095909,
                      -0.004695545881986618,
                      0.0031559085473418236,
                      -0.018865784630179405,
                      0.011940899305045605,
                      -0.00720355287194252,
                      0.019074784591794014,
                      -0.019269851967692375,
                      0.010282827541232109,
                      -0.016998711973428726,
                      0.013821904547512531,
                      -0.013229736126959324,
                      -0.010888929478824139,
                      -0.023463796824216843,
                      -0.001105090486817062,
                      0.01677577942609787,
                      0.019966520369052887,
                      -0.011620431207120419,
                      0.0018583633936941624,
                      -0.030514081940054893,
                      0.015326708555221558,
                      -0.008137088268995285,
                      -0.029148612171411514,
                      -0.003768976777791977,
                      0.001748638111166656,
                      0.007879321463406086,
                      -0.016817579045891762,
                      -0.04946346580982208,
                      -0.034638360142707825,
                      -0.013362103141844273,
                      -0.009634925983846188,
                      -0.001558795920573175,
                      -0.016399577260017395,
                      0.015298841521143913,
                      -0.011042196303606033,
                      -0.04135424643754959,
                      -0.004496995359659195,
                      -0.0026560488622635603,
                      0.03023541532456875,
                      0.022335194051265717,
                      0.005200630519539118,
                      0.012985901907086372,
                      0.030904216691851616,
                      0.013389969244599342,
                      -0.011007362976670265,
                      0.00028258617385290563,
                      -0.019019052386283875,
                      0.009920559823513031,
                      0.011188496835529804,
                      -0.023867864161729813,
                      -0.0428311824798584,
                      0.0299846138805151,
                      0.027393007650971413,
                      -0.017667515203356743,
                      -0.029343679547309875,
                      0.015856176614761353,
                      -0.018782183527946472,
                      -0.001386370393447578,
                      0.00997629389166832,
                      -0.011425363831222057,
                      -0.021122990176081657,
                      -0.009035791270434856,
                      -0.012170800007879734,
                      0.014295638538897038,
                      0.01468577329069376,
                      -0.005298164207488298,
                      -0.017848648130893707,
                      0.013013768941164017,
                      -0.020119788125157356,
                      0.005218047648668289,
                      0.010881963185966015,
                      -0.010847128927707672,
                      0.024007197469472885,
                      0.01471363939344883,
                      0.022711394354701042,
                      0.014267771504819393,
                      0.03304995596408844,
                      0.026334071531891823,
                      -0.030709149315953255,
                      0.018378116190433502,
                      -0.013452669605612755,
                      -0.003040958195924759,
                      -0.007224452681839466,
                      0.005360864568501711,
                      0.004469128791242838,
                      0.0304862167686224,
                      -0.022711394354701042,
                      -0.00927265826612711,
                      0.004099894315004349,
                      -0.009767292998731136,
                      0.017653580754995346,
                      0.008729256689548492,
                      -0.00025994444149546325,
                      0.004911513067781925,
                      -0.011522897519171238,
                      0.007928088307380676,
                      0.034638360142707825,
                      0.006123716477304697,
                      -0.02176392637193203,
                      -0.0007519666105508804,
                      -0.0016493628500029445,
                      0.0037445933558046818,
                      -0.02690533921122551,
                      0.009453792124986649,
                      0.029510879889130592,
                      -0.0418279804289341,
                      -0.00914029125124216,
                      0.027518408372998238,
                      0.011334797367453575,
                      0.011049163527786732,
                      -0.029148612171411514,
                      0.011090963147580624,
                      0.013167035765945911,
                      0.021025456488132477,
                      -0.013410869985818863,
                      -0.010206194594502449,
                      -0.007503120228648186,
                      0.015326708555221558,
                      -0.03006821498274803,
                      -0.017165912315249443,
                      -0.019924720749258995,
                      -0.011857299134135246,
                      0.0158701092004776,
                      0.021039390936493874,
                      0.007565820589661598,
                      -0.007872354239225388,
                      0.010540595278143883,
                      -0.029343679547309875,
                      0.010241027921438217,
                      0.009753359481692314,
                      -0.004260128363966942,
                      -0.00318725872784853,
                      0.014769373461604118,
                      -0.0038665104657411575,
                      0.010450027883052826,
                      0.0176257137209177,
                      0.020133720710873604,
                      -0.0065626176074147224,
                      0.01000415999442339,
                      -0.030597683042287827,
                      -0.015897976234555244,
                      -0.007955954410135746,
                      0.004124277736991644,
                      -0.00038055519689805806,
                      -0.02800607495009899,
                      0.006447667255997658,
                      0.004775662906467915,
                      0.00568133220076561,
                      -0.0022397893480956554,
                      -0.015354574657976627,
                      -0.005329514387995005,
                      0.002267656149342656,
                      -0.011676165275275707,
                      0.00816495530307293,
                      0.016566777601838112,
                      -0.023310529068112373,
                      0.037007033824920654,
                      0.0067054349929094315,
                      0.04951919987797737,
                      0.0018670717254281044,
                      -0.01606517657637596,
                      -0.03491702675819397,
                      -0.026125071570277214,
                      -0.017556047067046165,
                      -0.026877472177147865,
                      -0.03304995596408844,
                      -0.025484135374426842,
                      0.02790854312479496,
                      0.019562453031539917,
                      -0.027128273621201515,
                      -0.03260408714413643,
                      -0.018196983262896538,
                      -0.019659986719489098,
                      0.00997629389166832,
                      0.01646924391388893,
                      0.01779291406273842,
                      0.010575428605079651,
                      0.01617664471268654,
                      -0.02841014415025711,
                      0.03714636713266373,
                      0.010087761096656322,
                      -0.0006901372689753771,
                      -0.013550203293561935,
                      0.012581833638250828,
                      0.011397497728466988,
                      -0.01612091064453125,
                      0.004803529474884272,
                      0.02473173290491104,
                      -0.04751279577612877,
                      -0.0007872354472056031,
                      0.03784303739666939,
                      -0.006451150868088007,
                      0.02403506450355053,
                      0.03918064013123512,
                      -0.019715720787644386,
                      -0.03480556234717369,
                      -0.01430957205593586,
                      0.018322382122278214,
                      0.033022090792655945,
                      0.01692904531955719,
                      0.003636609762907028,
                      -0.0019855054561048746,
                      -0.01342480257153511,
                      0.0052772643975913525,
                      0.013717403635382652,
                      -0.026724206283688545,
                      0.0027675158344209194,
                      0.0054792980663478374,
                      0.0037724601570516825,
                      -0.02397933229804039,
                      -0.017221646383404732,
                      0.011676165275275707,
                      0.012059332802891731,
                      -0.014978373423218727,
                      0.020523855462670326,
                      -0.004420361947268248,
                      -0.02967808023095131,
                      0.0049393801018595695,
                      0.02679387293756008,
                      0.0015971126267686486,
                      -0.020900055766105652,
                      0.0110839968547225,
                      -0.027936408296227455,
                      0.0032029335852712393,
                      0.02161065861582756,
                      0.012958034873008728,
                      -0.0052842311561107635,
                      -0.002953874645754695,
                      0.005691782105714083,
                      -0.02573493681848049,
                      -0.00224153115414083,
                      -0.007691220846027136,
                      0.0058694323524832726,
                      -0.018461717292666435,
                      0.04625879228115082,
                      0.02917647920548916,
                      -0.05105186998844147,
                      0.017458513379096985,
                      -0.013780103996396065,
                      -0.008833757601678371,
                      -0.019032984972000122,
                      0.0032691170927137136,
                      0.005033430177718401,
                      -0.021136924624443054,
                      0.02101152390241623,
                      0.0037480767350643873,
                      -0.03505636006593704,
                      -0.0019611220341175795,
                      -0.0012052366510033607,
                      0.008680489845573902,
                      -0.01047789491713047,
                      0.02574886940419674,
                      0.2039845585823059,
                      -0.03405315801501274,
                      0.01833631657063961,
                      0.0029573580250144005,
                      0.013780103996396065,
                      0.0176257137209177,
                      0.0037550434935837984,
                      0.017709314823150635,
                      -0.00217186426743865,
                      -0.001844429993070662,
                      0.00048462007544003427,
                      0.008060455322265625,
                      -0.013222768902778625,
                      0.0024365982972085476,
                      0.0224187932908535,
                      0.0010824487544596195,
                      -0.012066299095749855,
                      -0.046175193041563034,
                      -0.00687611848115921,
                      -0.0056778485886752605,
                      -0.005225013941526413,
                      0.014699706807732582,
                      -0.003213383723050356,
                      -0.036505430936813354,
                      0.014351372607052326,
                      -0.012916235253214836,
                      -0.006691501475870609,
                      -0.026919273659586906,
                      0.006419800687581301,
                      0.03472195938229561,
                      -0.007377719972282648,
                      0.01858711615204811,
                      -0.008241589181125164,
                      0.015479975380003452,
                      -0.009690659120678902,
                      -0.020384522154927254,
                      -0.02272532880306244,
                      0.02222372591495514,
                      0.011056129820644855,
                      -0.009425925090909004,
                      0.035084228962659836,
                      0.010401261039078236,
                      -0.008025621995329857,
                      -0.012331033125519753,
                      -0.00031154145835898817,
                      -0.00023947982117533684,
                      0.000322644627885893,
                      -0.013814937323331833,
                      -0.002095230622217059,
                      0.03023541532456875,
                      -0.010512728244066238,
                      0.02654307149350643,
                      0.039124906063079834,
                      0.042691849172115326,
                      0.0030461831483989954,
                      -0.014267771504819393,
                      0.002694365568459034,
                      0.00595651613548398,
                      0.005886849481612444,
                      0.01949278637766838,
                      -0.025762803852558136,
                      0.0186428502202034,
                      0.002546323463320732,
                      0.03112715110182762,
                      0.03179595246911049,
                      0.015856176614761353,
                      -0.019882921129465103,
                      0.012393733486533165,
                      0.015730775892734528,
                      -0.004723412916064262,
                      -0.015382441692054272,
                      -0.016817579045891762,
                      -0.005925165954977274,
                      -0.004653745796531439,
                      -0.03502849489450455,
                      -0.02126232348382473,
                      0.051414139568805695,
                      0.01818304881453514,
                      0.009565259329974651,
                      0.034081026911735535,
                      -0.018754318356513977,
                      -0.005834599025547504,
                      0.011369630694389343,
                      -0.014894773252308369,
                      -0.026779938489198685,
                      -0.043054115027189255,
                      0.017305247485637665,
                      -0.01395427156239748,
                      -0.017946181818842888,
                      -0.016483178362250328,
                      0.0023791231215000153,
                      -0.021081190556287766,
                      -0.008959157392382622,
                      0.008617790415883064,
                      -0.0004689450142905116,
                      -0.010735662654042244,
                      -0.019172318279743195,
                      0.03940357267856598,
                      -0.0014569080667570233,
                      -0.021596726030111313,
                      -0.03452689200639725,
                      0.04703906178474426,
                      0.03842823579907417,
                      0.02644553780555725,
                      -0.0008155376417562366,
                      0.004946346860378981,
                      -0.01019922737032175,
                      -0.008157989010214806,
                      0.022502394393086433,
                      -0.0013027702225372195,
                      0.001785213127732277,
                      -0.012386767193675041,
                      0.0019053885480389,
                      0.007997754961252213,
                      -0.0018931968370452523,
                      -0.011327831074595451,
                      0.005221530795097351,
                      -0.014393172226846218,
                      0.012637567706406116,
                      -0.005221530795097351,
                      0.0005899911629967391,
                      -0.03388595953583717,
                      -0.039821576327085495,
                      0.017946181818842888,
                      -0.0014830331783741713,
                      -0.004685095977038145,
                      -0.016413511708378792,
                      -0.02619473822414875,
                      -0.01430957205593586,
                      -0.007405586540699005,
                      0.011327831074595451,
                      -0.03265982121229172,
                      0.03530716150999069,
                      -0.021582791581749916,
                      -0.02906501293182373,
                      0.00651036761701107,
                      0.01950671896338463,
                      -0.017542114481329918,
                      -0.01779291406273842,
                      0.009941460564732552,
                      0.009460759349167347,
                      -5.8073856052942574e-05,
                      0.006141133140772581,
                      -0.006984102074056864,
                      0.01627417653799057,
                      -0.0468439944088459,
                      0.0076215541921556,
                      -0.014434972777962685,
                      -0.0014351372374221683,
                      -0.03625463321805,
                      -0.01478330697864294,
                      -0.007649420760571957,
                      0.0015466042095795274,
                      -0.019311651587486267,
                      0.023463796824216843,
                      -0.002380864694714546,
                      -0.018670717254281044,
                      -0.015159507282078266,
                      -0.013264569453895092,
                      0.0021109057124704123,
                      -0.028786344453692436,
                      -0.015591442584991455,
                      -0.0022641727700829506,
                      -0.025219401344656944,
                      -0.009077590890228748,
                      -0.009579192847013474,
                      -0.17745541036128998,
                      0.010847128927707672,
                      0.02076072245836258,
                      -0.032074619084596634,
                      -0.005552448332309723,
                      0.02711433917284012,
                      0.032269686460494995,
                      -0.009850893169641495,
                      -0.020482055842876434,
                      0.003523401217535138,
                      -0.0010632904013618827,
                      -0.017012646421790123,
                      -0.02856341004371643,
                      -0.0007136498461477458,
                      0.004350695293396711,
                      0.01306950207799673,
                      0.0018235299503430724,
                      0.02045418880879879,
                      0.024898933246731758,
                      0.02825687639415264,
                      0.03143368661403656,
                      -0.044614654034376144,
                      0.013536269776523113,
                      -0.007321986369788647,
                      0.02064925618469715,
                      -0.009767292998731136,
                      -0.010171361267566681,
                      0.03137795254588127,
                      -0.017151979729533195,
                      -0.014671839773654938,
                      0.012128999456763268,
                      0.02725367434322834,
                      0.012101132422685623,
                      -0.004552728962153196,
                      0.00025515485322102904,
                      0.020886123180389404,
                      0.01123029738664627,
                      -0.01334816962480545,
                      -0.0011651781387627125,
                      -0.008415755815804005,
                      0.05252880975604057,
                      -0.00992752704769373,
                      -0.0076145874336361885,
                      -0.014121471904218197,
                      -0.023644929751753807,
                      -0.011097930371761322,
                      0.007670320570468903,
                      -0.0022746229078620672,
                      0.016009442508220673,
                      0.018364183604717255,
                      -0.01748638041317463,
                      -0.020384522154927254,
                      0.020635321736335754,
                      0.006398900877684355,
                      0.007005002349615097,
                      0.02679387293756008,
                      0.011662231758236885,
                      0.00821372214704752,
                      -0.026166871190071106,
                      0.015535708516836166,
                      0.004977697040885687,
                      -0.033022090792655945,
                      0.026013603433966637,
                      0.002319906372576952,
                      -0.014978373423218727,
                      -0.0016868087695911527,
                      0.0018479133723303676,
                      0.004333278629928827,
                      -0.021986858919262886,
                      0.011202430352568626,
                      -0.03647756576538086,
                      -0.023463796824216843,
                      0.0003302644472569227,
                      0.0047408295795321465,
                      0.022599928081035614,
                      -0.00012757742661051452,
                      -0.019367385655641556,
                      -0.0012609701370820403,
                      0.001722512999549508,
                      -0.01123029738664627,
                      -0.033022090792655945,
                      0.0317402184009552,
                      -0.0096976263448596,
                      0.026975005865097046,
                      0.0032081587705761194,
                      -0.012087199836969376,
                      0.005521098151803017,
                      0.018322382122278214,
                      -0.019715720787644386,
                      -0.026431605219841003,
                      0.011432331055402756,
                      0.0060226996429264545,
                      0.017542114481329918,
                      0.0019785386975854635,
                      0.007412553299218416,
                      0.01779291406273842,
                      0.010220128111541271,
                      -0.015284908004105091,
                      0.022181926295161247,
                      -0.012853534892201424,
                      -0.02105332352221012,
                      -0.017347047105431557,
                      0.006381483748555183,
                      0.009307491593062878,
                      0.012783868238329887,
                      0.02533086948096752,
                      -0.019618187099695206,
                      0.006694984622299671,
                      0.029037145897746086,
                      0.011481097899377346,
                      -0.006043599918484688,
                      -0.010833196341991425,
                      0.00023664960463065654,
                      0.02629227191209793,
                      0.017820781096816063,
                      0.009133324958384037,
                      -0.03193528577685356,
                      -0.0027918992564082146,
                      0.005256364122033119,
                      -0.02599967084825039,
                      0.043499983847141266,
                      -0.010603295639157295,
                      0.003612226340919733,
                      0.014978373423218727,
                      -0.0007920250645838678,
                      -0.01804371550679207,
                      -0.11425364017486572,
                      -0.008206755854189396,
                      -0.002555032027885318,
                      0.004354178439825773,
                      -0.014490705914795399,
                      -0.004120794590562582,
                      0.02172212488949299,
                      0.029483012855052948,
                      -0.026027537882328033,
                      0.010624195449054241,
                      -0.020579589530825615,
                      -0.02711433917284012,
                      0.013090401887893677,
                      0.004803529474884272,
                      -0.005319064483046532,
                      0.008185855112969875,
                      0.0033701341599226,
                      -0.0047582462430000305,
                      -0.009251758456230164,
                      0.026682404801249504,
                      -0.0026473405305296183,
                      -0.01959032006561756,
                      0.014393172226846218,
                      -0.024048998951911926,
                      0.015842242166399956,
                      0.00677858479321003,
                      -0.029705947265028954,
                      0.0015788250602781773,
                      0.021889325231313705,
                      0.011028262786567211,
                      0.007398620247840881,
                      -0.03455476090312004,
                      -0.00881285686045885,
                      -0.003410192672163248,
                      -0.0019837636500597,
                      -0.003326592268422246,
                      -0.012909268029034138,
                      -0.022558128461241722,
                      0.005782349035143852,
                      -0.03405315801501274,
                      0.0029051078017801046,
                      0.013292435556650162,
                      -0.004169561434537172,
                      -0.010157427750527859,
                      -0.010136527009308338,
                      -0.00012942795001436025,
                      -0.014434972777962685,
                      0.030151814222335815,
                      0.023770330473780632,
                      -0.00803258828818798,
                      -0.012198666110634804,
                      -0.02045418880879879,
                      -0.02499646693468094,
                      -0.005367831327021122,
                      0.020941857248544693,
                      0.033133555203676224,
                      0.0186428502202034,
                      0.013598970137536526,
                      -0.004517895635217428,
                      -0.012965002097189426,
                      -0.000530774355866015,
                      -0.005092646926641464,
                      -0.01698477938771248,
                      0.005144897382706404,
                      0.018628917634487152,
                      -0.001567504252307117,
                      -0.05473028123378754,
                      -0.011167597025632858,
                      -0.008673523552715778,
                      -0.005350414663553238,
                      -0.014156305231153965,
                      0.01793224923312664,
                      0.013194902800023556,
                      0.02155492454767227,
                      -0.01858711615204811,
                      -0.01012259442359209,
                      -0.014281705021858215,
                      -0.0021091641392558813,
                      0.0022066975943744183,
                      -0.018726451322436333,
                      -0.01090286299586296,
                      -0.0274487417191267,
                      0.007698187604546547,
                      -0.002624698681756854,
                      -0.010220128111541271,
                      -0.012804768048226833,
                      -0.0012182991486042738,
                      -0.02499646693468094,
                      -0.011745831929147243,
                      -0.029761681333184242,
                      0.009662793017923832,
                      0.008645656518638134,
                      -0.029761681333184242,
                      -0.008234621956944466,
                      -0.004180011339485645,
                      -0.0024644650984555483,
                      -0.0031994504388421774,
                      -0.01390550471842289,
                      0.020217321813106537,
                      0.005987866315990686,
                      -0.015075907111167908,
                      0.0043750787153840065,
                      -0.04391798377037048,
                      0.016162710264325142,
                      0.0032952423207461834,
                      -0.01698477938771248,
                      0.011655264534056187,
                      0.018823985010385513,
                      0.03692343458533287,
                      -0.017040513455867767,
                      0.002717007417231798,
                      0.008854657411575317,
                      -0.007363786455243826,
                      0.021485257893800735,
                      -0.01738884672522545,
                      -0.018977250903844833,
                      -0.006447667255997658,
                      -0.010993429459631443,
                      0.005925165954977274,
                      -0.004507445730268955,
                      0.029204346239566803,
                      0.005803248845040798,
                      -0.01499230694025755,
                      -0.016538910567760468,
                      0.009572225622832775,
                      -0.00025798508431762457,
                      -0.007774821016937494,
                      0.013828870840370655,
                      -0.010234060697257519,
                      0.026320137083530426,
                      -0.026612738147377968,
                      -0.011780665256083012,
                      0.020077988505363464,
                      -0.02403506450355053,
                      -0.0008969607879407704,
                      0.030040347948670387,
                      -0.0010589362354949117,
                      0.016664311289787292,
                      0.02423013187944889,
                      0.008596889674663544,
                      0.03622676432132721,
                      0.004639812745153904,
                      -0.014755439944565296,
                      -0.03600383177399635,
                      0.0020307889208197594,
                      -0.012540034018456936,
                      -0.00260728201828897,
                      0.022251592949032784,
                      -0.0027222323697060347,
                      0.003500759368762374,
                      0.015479975380003452,
                      -0.006398900877684355,
                      0.008778023533523083,
                      0.02091399021446705,
                      0.019074784591794014,
                      -0.029009278863668442,
                      -0.015953710302710533,
                      -0.00849238969385624,
                      -0.0035286261700093746,
                      0.02278106100857258,
                      0.002276364481076598,
                      -0.0034415426198393106,
                      0.0367841012775898,
                      -0.015048041008412838,
                      0.022279459983110428,
                      0.003981460817158222,
                      0.0076424540020525455,
                      -0.008750157430768013,
                      0.012428566813468933,
                      0.007039835676550865,
                      -0.02912074513733387,
                      -0.02827080897986889,
                      -0.010568462312221527,
                      0.009251758456230164,
                      -0.011773698963224888,
                      0.02565133571624756,
                      0.029037145897746086,
                      0.005252880975604057,
                      -0.0007088602287694812,
                      0.0114601980894804,
                      -0.005998316220939159,
                      0.01478330697864294,
                      0.012484300881624222,
                      -0.02459239959716797,
                      -0.016845446079969406,
                      -0.014448905363678932,
                      0.024453066289424896,
                      0.002779707545414567,
                      -0.004451712127774954,
                      0.012170800007879734,
                      -0.005378281231969595,
                      0.02195899188518524,
                      0.012184732593595982,
                      0.001861846656538546,
                      0.009760326705873013,
                      0.03555796295404434,
                      0.00401629414409399,
                      0.01490870676934719,
                      0.007349853403866291,
                      -0.019269851967692375,
                      0.006796001922339201,
                      -0.021749991923570633,
                      0.006127200089395046,
                      0.015828309580683708,
                      -0.012463400140404701,
                      -0.010094727389514446,
                      -0.023157263174653053,
                      0.021136924624443054,
                      -0.013229736126959324,
                      -0.00457362923771143,
                      0.003953594248741865,
                      0.03770370036363602,
                      -0.0011442780960351229,
                      0.015298841521143913,
                      -0.01219169981777668,
                      0.01080532930791378,
                      -0.034276094287633896,
                      0.02765774168074131,
                      -0.007210519630461931,
                      -0.036505430936813354,
                      -0.026570938527584076,
                      0.033022090792655945,
                      0.021931126713752747,
                      -0.01518737431615591,
                      0.044558919966220856,
                      -0.012212599627673626,
                      0.018559250980615616,
                      0.010819262824952602,
                      0.024717800319194794,
                      -0.011906065978109837,
                      0.02638980560004711,
                      0.02278106100857258,
                      0.017597848549485207,
                      -0.008680489845573902,
                      0.01989685371518135,
                      -0.01874038390815258,
                      -0.021136924624443054,
                      -0.012310133315622807,
                      -0.006607901304960251,
                      0.03279915452003479,
                      -0.0008817211491987109,
                      0.10160214453935623,
                      0.021248390898108482,
                      -0.0030949499923735857,
                      0.012115065939724445,
                      -0.015702908858656883,
                      0.011871231719851494,
                      0.013362103141844273,
                      0.018921518698334694,
                      -0.011474130675196648,
                      -0.0010545820696279407,
                      0.006339683663100004,
                      -0.015591442584991455,
                      -0.013689536601305008,
                      -0.02423013187944889,
                      0.0035355929285287857,
                      -0.015605375170707703,
                      0.003765493631362915,
                      0.008617790415883064,
                      -0.008610823191702366,
                      -0.0030200581531971693,
                      0.024299798533320427,
                      -0.00043737722444348037,
                      0.011223330162465572,
                      0.01602337695658207,
                      -0.013118268921971321,
                      -0.009669759310781956,
                      0.007018935866653919,
                      -0.016455311328172684,
                      -0.008673523552715778,
                      -0.03112715110182762,
                      0.004406428430229425,
                      -0.016566777601838112,
                      -0.04166077822446823,
                      -0.013571103103458881,
                      0.02815934270620346,
                      0.01097949594259262,
                      -0.001757346442900598,
                      0.013341202400624752,
                      0.014121471904218197,
                      5.785614484921098e-05,
                      0.009293558076024055,
                      0.036449696868658066,
                      -0.0029138161335140467,
                      -0.00964885950088501,
                      -0.013647736981511116,
                      -0.007997754961252213,
                      0.0014725831570103765,
                      -0.013731337152421474,
                      -0.02312939614057541
                    ]
                  }
                ],
                "model": "ada",
                "usage": {
                  "prompt_tokens": 12,
                  "total_tokens": 12
                }
              },
              "start_time": 1695246509.207655,
              "end_time": 1695246509.967509,
              "error": null,
              "children": null,
              "node_name": null
            }
          ],
          "node_name": "embed_the_question"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "",
        "stderr": ""
      },
      "system_metrics": {
        "prompt_tokens": 12,
        "total_tokens": 12,
        "duration": 0.796005
      },
      "result": [
        -0.011118830181658268,
        -0.01631597802042961,
        0.02559560351073742,
        -0.002060397295281291,
        -0.005500198341906071,
        0.031210750341415405,
        0.015424241311848164,
        -0.011599531397223473,
        -0.01773718185722828,
        -0.027727408334612846,
        0.028535543009638786,
        0.04366718605160713,
        -0.035139963030815125,
        0.005427048075944185,
        -0.0009474692051298916,
        0.008422723039984703,
        0.011244230903685093,
        0.009614026173949242,
        0.027797074988484383,
        -0.016956912353634834,
        -0.0016798420110717416,
        -0.006102816667407751,
        -0.016552845016121864,
        -0.0035739096347242594,
        -0.010742628946900368,
        -0.00849238969385624,
        0.029538746923208237,
        -0.020328788086771965,
        -0.003713243408128619,
        -0.009676726534962654,
        0.0021788307931274176,
        0.02357526309788227,
        -0.017918314784765244,
        -0.009948426857590675,
        -0.009258724749088287,
        0.006694984622299671,
        -0.0029294912237674,
        -0.016915112733840942,
        0.02438339963555336,
        -0.023212995380163193,
        0.0071164690889418125,
        0.021694257855415344,
        0.003977977205067873,
        -0.015270974487066269,
        -0.005953032989054918,
        0.01672004535794258,
        0.010652062483131886,
        -0.0065695843659341335,
        -0.012059332802891731,
        0.0093214251101017,
        0.00438204500824213,
        0.015856176614761353,
        -0.03945930674672127,
        -0.020997589454054832,
        -0.02251632697880268,
        -0.0034084508661180735,
        -0.002352998126298189,
        0.032966356724500656,
        -0.0272118728607893,
        -0.013543237000703812,
        0.0005050846957601607,
        -0.008130121976137161,
        0.0009370191837660968,
        0.008053488098084927,
        -0.022404860705137253,
        -0.024466998875141144,
        -0.008548122830688953,
        0.008018654771149158,
        0.019437052309513092,
        0.007398620247840881,
        0.01959032006561756,
        0.010038994252681732,
        0.0009866568725556135,
        -0.001406399649567902,
        0.0428033173084259,
        -0.01388460397720337,
        0.002535873558372259,
        -0.014755439944565296,
        0.001035423600114882,
        0.03592022880911827,
        0.012735101394355297,
        -0.002624698681756854,
        -0.0010606779251247644,
        0.026487339287996292,
        0.015897976234555244,
        -0.012742067687213421,
        0.010436095297336578,
        0.025275135412812233,
        0.002154447603970766,
        0.0040302276611328125,
        0.027379073202610016,
        0.03536289557814598,
        0.012944101355969906,
        0.0067054349929094315,
        -0.0038839271292090416,
        0.012247432954609394,
        -0.01118153054267168,
        0.038818370550870895,
        0.02634800411760807,
        -0.009523458778858185,
        -0.010157427750527859,
        0.008513289503753185,
        -0.04497692361474037,
        -0.005552448332309723,
        -0.024606334045529366,
        0.014852973632514477,
        0.010749596171081066,
        -0.01130693033337593,
        0.003262150567024946,
        0.001316703506745398,
        -0.019074784591794014,
        0.030207548290491104,
        0.006221250165253878,
        -0.024216199293732643,
        -0.0005220660241320729,
        -0.002276364481076598,
        0.015479975380003452,
        -0.039319973438978195,
        -0.009669759310781956,
        0.012602734379470348,
        0.017165912315249443,
        -0.004075510893017054,
        0.010721729137003422,
        -0.034582626074552536,
        0.03288275748491287,
        -0.003422384150326252,
        -0.010819262824952602,
        -0.03380235657095909,
        -0.006520817521959543,
        -0.04288691654801369,
        0.027420874685049057,
        0.022641727700829506,
        0.015939775854349136,
        -0.022753195837140083,
        0.005636048503220081,
        0.01854531653225422,
        -0.02760200761258602,
        -0.02050992287695408,
        -0.01959032006561756,
        -0.004643295891582966,
        0.02257206104695797,
        0.003828193759545684,
        -0.014253838919103146,
        -0.023993264883756638,
        -0.010192261077463627,
        0.018601050600409508,
        0.018782183527946472,
        0.020412389189004898,
        0.0024139564484357834,
        -0.010408228263258934,
        0.026334071531891823,
        -0.001065902877599001,
        -0.029761681333184242,
        0.013807971030473709,
        0.007726054172962904,
        0.0011695323046296835,
        0.00037707184674218297,
        0.009753359481692314,
        -0.009732459671795368,
        -0.005601215176284313,
        -0.020412389189004898,
        0.017012646421790123,
        0.017096245661377907,
        -0.009307491593062878,
        0.024118665605783463,
        0.03349582478404045,
        0.0020900056697428226,
        -0.014866907149553299,
        0.007328953128308058,
        -0.029204346239566803,
        -0.026222603395581245,
        0.018252715468406677,
        -0.01959032006561756,
        -0.003438059240579605,
        0.0040128109976649284,
        0.00180088821798563,
        0.03391382470726967,
        -0.02806180901825428,
        -0.02987314760684967,
        -0.015619308687746525,
        0.0193952526897192,
        0.0065521677024662495,
        0.020119788125157356,
        0.023547397926449776,
        -0.03098781779408455,
        -0.02559560351073742,
        -0.003594809677451849,
        -0.01974358595907688,
        0.01012259442359209,
        -0.034192491322755814,
        0.02382606454193592,
        0.007102536037564278,
        0.000681428937241435,
        -0.01843385025858879,
        -0.6148517727851868,
        -0.03820530325174332,
        -0.009300525300204754,
        -0.028535543009638786,
        0.01698477938771248,
        0.005514131393283606,
        -0.011627398431301117,
        0.03349582478404045,
        -0.024104731157422066,
        0.033579424023628235,
        -0.0008451460162177682,
        -0.023296596482396126,
        -0.0038212270010262728,
        0.007733020931482315,
        -0.010498794727027416,
        -0.012992868199944496,
        0.006503400858491659,
        0.004162594676017761,
        0.00922389142215252,
        0.022335194051265717,
        -0.03500062972307205,
        0.00739165348932147,
        -0.026069337502121925,
        -0.004152144771069288,
        -0.025372669100761414,
        0.0031245583668351173,
        0.006506884470582008,
        -0.021095123142004013,
        -0.009816059842705727,
        0.017151979729533195,
        -0.02045418880879879,
        -0.001589275081641972,
        0.021889325231313705,
        -0.030207548290491104,
        0.025010401383042336,
        0.021471325308084488,
        0.0021439974661916494,
        0.009084558114409447,
        -0.0007767854258418083,
        0.027880676090717316,
        -0.01596764288842678,
        -0.002070847200229764,
        -0.0024035065434873104,
        -0.0031228167936205864,
        0.017360979691147804,
        -0.000540788983926177,
        0.0186428502202034,
        -0.010498794727027416,
        0.006841285154223442,
        -0.004591045901179314,
        -0.008708356879651546,
        -0.003521659644320607,
        -0.007572787348181009,
        -0.002574190264567733,
        0.03550222888588905,
        0.011418397538363934,
        0.0342482253909111,
        -0.0024975568521767855,
        0.0014272996922954917,
        -0.011919998563826084,
        0.008631723001599312,
        0.017305247485637665,
        -0.012303167022764683,
        -0.01918625272810459,
        0.018823985010385513,
        0.009565259329974651,
        -0.006287433672696352,
        0.003291758941486478,
        0.00914725847542286,
        -0.04558999091386795,
        0.0003217737830709666,
        0.015229174867272377,
        0.0004941992810927331,
        -0.015284908004105091,
        0.00554199842736125,
        0.012338000349700451,
        0.0033718757331371307,
        0.011188496835529804,
        -0.002103938953951001,
        0.019757520407438278,
        -0.017444580793380737,
        -0.012247432954609394,
        -0.028842078521847725,
        0.002976516494527459,
        -0.0024052481167018414,
        0.0001037927286233753,
        -0.01738884672522545,
        0.017193779349327087,
        -0.01478330697864294,
        -0.0036505432799458504,
        0.0033718757331371307,
        0.03199101984500885,
        -0.028842078521847725,
        -0.042050912976264954,
        0.004918479826301336,
        0.011376597918570042,
        0.013125235214829445,
        -0.0074613201431930065,
        0.03112715110182762,
        -0.03722996637225151,
        0.014246871694922447,
        -0.020565655082464218,
        0.012351933866739273,
        -0.005886849481612444,
        0.02675207331776619,
        0.03600383177399635,
        -0.007475253660231829,
        0.0011259905295446515,
        -0.025707069784402847,
        0.0035303679760545492,
        -0.005911232437938452,
        -0.010401261039078236,
        -0.0035390763077884912,
        -0.0003302644472569227,
        -0.014212038367986679,
        -0.04040677472949028,
        0.011843365617096424,
        0.01065902877599001,
        0.01631597802042961,
        -0.003516434459015727,
        0.02599967084825039,
        0.01843385025858879,
        0.015521774999797344,
        -0.02886994555592537,
        0.01868464983999729,
        0.012045399285852909,
        0.0024522733874619007,
        0.006210800260305405,
        0.020844323560595512,
        0.02886994555592537,
        -0.011495031416416168,
        0.02172212488949299,
        0.010213160887360573,
        -0.010213160887360573,
        -0.002133547328412533,
        0.01571684330701828,
        0.0012836117530241609,
        -0.007726054172962904,
        0.01097949594259262,
        -0.0035094679333269596,
        -0.02130412496626377,
        -0.008144055493175983,
        -0.00489758001640439,
        -0.012553967535495758,
        -0.028591277077794075,
        -0.015647174790501595,
        -0.021931126713752747,
        -0.0011024780105799437,
        0.0001773239200701937,
        -0.02266959473490715,
        0.011599531397223473,
        -0.018475649878382683,
        -0.02528906799852848,
        -0.017347047105431557,
        -0.023394130170345306,
        -0.014616106636822224,
        -0.011655264534056187,
        -0.032213952392339706,
        -0.008617790415883064,
        -0.029092878103256226,
        0.006754201836884022,
        0.02534480206668377,
        -0.027588075026869774,
        -0.017765048891305923,
        -0.009676726534962654,
        -0.013877637684345245,
        0.0418279804289341,
        0.030792750418186188,
        0.0045875622890889645,
        -0.016190577298402786,
        0.013034668751060963,
        -0.03792663663625717,
        0.0009213441517204046,
        0.02201472595334053,
        0.0011190238874405622,
        0.01848958246409893,
        -0.02605540305376053,
        0.009906627237796783,
        -0.00020399325876496732,
        -0.032576221972703934,
        -0.009857860393822193,
        0.040573976933956146,
        -0.002218889305368066,
        0.006458117626607418,
        0.012233499437570572,
        0.0017199005233123899,
        0.003911793697625399,
        -0.0036261598579585552,
        -0.021025456488132477,
        0.015842242166399956,
        0.011237263679504395,
        0.019534585997462273,
        -0.017904382199048996,
        0.01627417653799057,
        -0.004322828259319067,
        0.02237699367105961,
        0.007948988117277622,
        0.027615942060947418,
        -0.014532506465911865,
        0.018754318356513977,
        0.025163667276501656,
        0.009850893169641495,
        -0.0022711395286023617,
        -0.0011756281601265073,
        -0.006064499728381634,
        -0.00982302613556385,
        -0.0011277322191745043,
        -0.03553009778261185,
        0.023338396102190018,
        0.0015709876315668225,
        0.02220979332923889,
        -0.02906501293182373,
        -0.033217158168554306,
        -0.01804371550679207,
        -0.008206755854189396,
        0.0012374575017020106,
        -0.009084558114409447,
        0.03265982121229172,
        -0.0279642753303051,
        0.00955132581293583,
        -0.006064499728381634,
        -0.012909268029034138,
        0.038706906139850616,
        0.012254400178790092,
        0.006893535144627094,
        0.006043599918484688,
        -0.0029660663567483425,
        -0.00960009265691042,
        0.015521774999797344,
        -0.020342722535133362,
        0.019228052347898483,
        0.02076072245836258,
        0.029956746846437454,
        -0.0038665104657411575,
        -0.015424241311848164,
        -0.010533628053963184,
        0.02630620449781418,
        0.009356258437037468,
        0.03279915452003479,
        0.003523401217535138,
        -0.012156866490840912,
        0.0030531499069184065,
        -0.004110344685614109,
        -0.0040406775660812855,
        0.053977880626916885,
        -0.01005292683839798,
        0.01995258778333664,
        0.030820617452263832,
        -0.01141143124550581,
        -0.0074404203332960606,
        0.017263446003198624,
        -0.011947865597903728,
        -0.017360979691147804,
        0.007551887072622776,
        0.017207713797688484,
        0.0009274400072172284,
        0.024801399558782578,
        0.0003629207785706967,
        -0.011195464059710503,
        0.010011127218604088,
        0.004267094656825066,
        -0.01249823346734047,
        0.02024518884718418,
        0.01787651516497135,
        0.023561330512166023,
        -0.014643972739577293,
        -0.030151814222335815,
        -0.028298676013946533,
        -0.012017532251775265,
        0.00023577875981573015,
        -0.017514247447252274,
        -0.004256644751876593,
        -0.010247994214296341,
        0.006573067978024483,
        0.003876960603520274,
        0.004364628344774246,
        -0.02363099716603756,
        0.01733311451971531,
        0.005022980272769928,
        0.02181965857744217,
        -0.006931852083653212,
        -0.020886123180389404,
        0.028131475672125816,
        -0.0040580942295491695,
        -0.011146697215735912,
        -0.015438174828886986,
        -0.019910788163542747,
        -0.010617229156196117,
        -0.028075743466615677,
        0.017597848549485207,
        0.008882524445652962,
        0.0077887545339763165,
        -0.020273054018616676,
        -0.003695826744660735,
        -0.002946907887235284,
        0.0015431208303198218,
        0.008861623704433441,
        -0.01118153054267168,
        0.013076468370854855,
        0.01125816348940134,
        -0.012045399285852909,
        0.005364347714930773,
        -0.016761844977736473,
        0.014448905363678932,
        0.049909334629774094,
        -0.006653184536844492,
        -0.02172212488949299,
        -0.01985505409538746,
        -0.018768250942230225,
        -0.018670717254281044,
        0.02332446351647377,
        0.010387328453361988,
        -0.019116584211587906,
        0.01153683103621006,
        0.024759599938988686,
        0.02372853085398674,
        -0.009425925090909004,
        0.0016868087695911527,
        0.011919998563826084,
        -0.0067054349929094315,
        -0.017402781173586845,
        -0.03753650188446045,
        -0.029455145820975304,
        0.0005564640159718692,
        0.025804603472352028,
        0.06325750797986984,
        0.0035094679333269596,
        0.0036993101239204407,
        -0.019464919343590736,
        0.0037480767350643873,
        -0.018015848472714424,
        -0.01592584326863289,
        0.0011303446954116225,
        -0.015633242204785347,
        -0.021485257893800735,
        -0.00836002267897129,
        0.0035652013029903173,
        0.0056499820202589035,
        0.02126232348382473,
        0.0009013149538077414,
        0.0013184451963752508,
        -0.05768415704369545,
        0.02559560351073742,
        -0.000957048439886421,
        0.003974494058638811,
        -0.014337439090013504,
        0.020816456526517868,
        0.03461049497127533,
        0.005099613685160875,
        -0.01752818003296852,
        0.01899118535220623,
        0.02509400062263012,
        0.018712516874074936,
        -0.04010024294257164,
        0.0060157328844070435,
        0.006917918566614389,
        -0.008798924274742603,
        0.00041408237302675843,
        -0.013139168731868267,
        0.022599928081035614,
        0.01708231307566166,
        0.006802968215197325,
        0.03380235657095909,
        0.0037202101666480303,
        -0.003915277309715748,
        0.012066299095749855,
        0.008645656518638134,
        -0.006245633587241173,
        -0.0037027932703495026,
        0.00836002267897129,
        -0.0035808763932436705,
        0.018503516912460327,
        0.02282286249101162,
        0.0013524078531190753,
        0.007238386198878288,
        -0.014379238709807396,
        -0.01783471554517746,
        -0.004451712127774954,
        -0.0074891867116093636,
        -0.007384686730802059,
        -0.011982698924839497,
        -0.009405025281012058,
        -0.025261200964450836,
        -0.023254796862602234,
        -0.003335300600156188,
        -0.00876409001648426,
        0.018475649878382683,
        -0.01139053050428629,
        0.009614026173949242,
        -0.005716165527701378,
        -0.003869993845000863,
        -0.017165912315249443,
        -0.013362103141844273,
        -0.0016040793852880597,
        -0.02634800411760807,
        -0.006130683235824108,
        -0.04840452969074249,
        -0.020328788086771965,
        0.023686731234192848,
        0.024397332221269608,
        0.029148612171411514,
        -0.013313336297869682,
        0.005172763951122761,
        -0.00545143149793148,
        -0.010059894062578678,
        -0.008018654771149158,
        0.016538910567760468,
        -0.005611665081232786,
        -0.016455311328172684,
        0.02609720453619957,
        -0.034136757254600525,
        -0.026111137121915817,
        -0.01950671896338463,
        0.02161065861582756,
        -0.011550764553248882,
        0.01779291406273842,
        -0.01468577329069376,
        -0.022237660363316536,
        -0.0067054349929094315,
        0.029287945479154587,
        -0.004845329560339451,
        0.02746267430484295,
        -0.0020969724282622337,
        -0.02605540305376053,
        0.020872190594673157,
        -0.03722996637225151,
        0.0004397720331326127,
        -0.03472195938229561,
        -0.012888368219137192,
        -0.003988427575677633,
        -0.0021770892199128866,
        0.012331033125519753,
        -0.014072705060243607,
        -0.003828193759545684,
        -0.0120941661298275,
        -0.01315310224890709,
        -0.01251913420855999,
        -0.0035025011748075485,
        0.046119458973407745,
        0.013055568560957909,
        0.004500478971749544,
        -0.014797240495681763,
        0.009202991612255573,
        -0.020523855462670326,
        -0.006771618500351906,
        -0.015340641140937805,
        0.02553986944258213,
        0.00311759184114635,
        -0.014866907149553299,
        0.03867903724312782,
        -0.01282566785812378,
        -0.011926965788006783,
        -0.015438174828886986,
        0.035335030406713486,
        -0.0234359297901392,
        0.045617856085300446,
        0.0035460428334772587,
        0.0027309407014399767,
        -0.017500314861536026,
        0.008276422508060932,
        -0.005280747544020414,
        -0.010895895771682262,
        -0.03148941695690155,
        0.018614983186125755,
        0.026361938565969467,
        -0.012303167022764683,
        -0.003533851122483611,
        -0.022530261427164078,
        -0.01612091064453125,
        -0.020091921091079712,
        -0.024271931499242783,
        0.04508838802576065,
        -0.0038978606462478638,
        0.003025283105671406,
        0.007168719545006752,
        0.009363225661218166,
        -0.0028371827211230993,
        -0.0002782320079859346,
        -0.037257835268974304,
        -0.020426321774721146,
        0.0015509583754464984,
        0.0397937074303627,
        0.0193952526897192,
        0.024620266631245613,
        0.04701119288802147,
        -0.011947865597903728,
        0.018127316609025,
        0.020217321813106537,
        -0.0008625627378933132,
        -0.0031837753485888243,
        -0.030346881598234177,
        -0.008234621956944466,
        -0.015995509922504425,
        0.014191138558089733,
        0.007067702244967222,
        -0.0076424540020525455,
        0.006956235505640507,
        0.01985505409538746,
        0.015006240457296371,
        0.011077029630541801,
        -0.003838643664494157,
        -0.0021370307076722383,
        0.0010876738233491778,
        -0.019172318279743195,
        -0.01256790105253458,
        0.01869858428835869,
        -0.018308449536561966,
        0.00748222041875124,
        -0.03380235657095909,
        -0.004695545881986618,
        0.0031559085473418236,
        -0.018865784630179405,
        0.011940899305045605,
        -0.00720355287194252,
        0.019074784591794014,
        -0.019269851967692375,
        0.010282827541232109,
        -0.016998711973428726,
        0.013821904547512531,
        -0.013229736126959324,
        -0.010888929478824139,
        -0.023463796824216843,
        -0.001105090486817062,
        0.01677577942609787,
        0.019966520369052887,
        -0.011620431207120419,
        0.0018583633936941624,
        -0.030514081940054893,
        0.015326708555221558,
        -0.008137088268995285,
        -0.029148612171411514,
        -0.003768976777791977,
        0.001748638111166656,
        0.007879321463406086,
        -0.016817579045891762,
        -0.04946346580982208,
        -0.034638360142707825,
        -0.013362103141844273,
        -0.009634925983846188,
        -0.001558795920573175,
        -0.016399577260017395,
        0.015298841521143913,
        -0.011042196303606033,
        -0.04135424643754959,
        -0.004496995359659195,
        -0.0026560488622635603,
        0.03023541532456875,
        0.022335194051265717,
        0.005200630519539118,
        0.012985901907086372,
        0.030904216691851616,
        0.013389969244599342,
        -0.011007362976670265,
        0.00028258617385290563,
        -0.019019052386283875,
        0.009920559823513031,
        0.011188496835529804,
        -0.023867864161729813,
        -0.0428311824798584,
        0.0299846138805151,
        0.027393007650971413,
        -0.017667515203356743,
        -0.029343679547309875,
        0.015856176614761353,
        -0.018782183527946472,
        -0.001386370393447578,
        0.00997629389166832,
        -0.011425363831222057,
        -0.021122990176081657,
        -0.009035791270434856,
        -0.012170800007879734,
        0.014295638538897038,
        0.01468577329069376,
        -0.005298164207488298,
        -0.017848648130893707,
        0.013013768941164017,
        -0.020119788125157356,
        0.005218047648668289,
        0.010881963185966015,
        -0.010847128927707672,
        0.024007197469472885,
        0.01471363939344883,
        0.022711394354701042,
        0.014267771504819393,
        0.03304995596408844,
        0.026334071531891823,
        -0.030709149315953255,
        0.018378116190433502,
        -0.013452669605612755,
        -0.003040958195924759,
        -0.007224452681839466,
        0.005360864568501711,
        0.004469128791242838,
        0.0304862167686224,
        -0.022711394354701042,
        -0.00927265826612711,
        0.004099894315004349,
        -0.009767292998731136,
        0.017653580754995346,
        0.008729256689548492,
        -0.00025994444149546325,
        0.004911513067781925,
        -0.011522897519171238,
        0.007928088307380676,
        0.034638360142707825,
        0.006123716477304697,
        -0.02176392637193203,
        -0.0007519666105508804,
        -0.0016493628500029445,
        0.0037445933558046818,
        -0.02690533921122551,
        0.009453792124986649,
        0.029510879889130592,
        -0.0418279804289341,
        -0.00914029125124216,
        0.027518408372998238,
        0.011334797367453575,
        0.011049163527786732,
        -0.029148612171411514,
        0.011090963147580624,
        0.013167035765945911,
        0.021025456488132477,
        -0.013410869985818863,
        -0.010206194594502449,
        -0.007503120228648186,
        0.015326708555221558,
        -0.03006821498274803,
        -0.017165912315249443,
        -0.019924720749258995,
        -0.011857299134135246,
        0.0158701092004776,
        0.021039390936493874,
        0.007565820589661598,
        -0.007872354239225388,
        0.010540595278143883,
        -0.029343679547309875,
        0.010241027921438217,
        0.009753359481692314,
        -0.004260128363966942,
        -0.00318725872784853,
        0.014769373461604118,
        -0.0038665104657411575,
        0.010450027883052826,
        0.0176257137209177,
        0.020133720710873604,
        -0.0065626176074147224,
        0.01000415999442339,
        -0.030597683042287827,
        -0.015897976234555244,
        -0.007955954410135746,
        0.004124277736991644,
        -0.00038055519689805806,
        -0.02800607495009899,
        0.006447667255997658,
        0.004775662906467915,
        0.00568133220076561,
        -0.0022397893480956554,
        -0.015354574657976627,
        -0.005329514387995005,
        0.002267656149342656,
        -0.011676165275275707,
        0.00816495530307293,
        0.016566777601838112,
        -0.023310529068112373,
        0.037007033824920654,
        0.0067054349929094315,
        0.04951919987797737,
        0.0018670717254281044,
        -0.01606517657637596,
        -0.03491702675819397,
        -0.026125071570277214,
        -0.017556047067046165,
        -0.026877472177147865,
        -0.03304995596408844,
        -0.025484135374426842,
        0.02790854312479496,
        0.019562453031539917,
        -0.027128273621201515,
        -0.03260408714413643,
        -0.018196983262896538,
        -0.019659986719489098,
        0.00997629389166832,
        0.01646924391388893,
        0.01779291406273842,
        0.010575428605079651,
        0.01617664471268654,
        -0.02841014415025711,
        0.03714636713266373,
        0.010087761096656322,
        -0.0006901372689753771,
        -0.013550203293561935,
        0.012581833638250828,
        0.011397497728466988,
        -0.01612091064453125,
        0.004803529474884272,
        0.02473173290491104,
        -0.04751279577612877,
        -0.0007872354472056031,
        0.03784303739666939,
        -0.006451150868088007,
        0.02403506450355053,
        0.03918064013123512,
        -0.019715720787644386,
        -0.03480556234717369,
        -0.01430957205593586,
        0.018322382122278214,
        0.033022090792655945,
        0.01692904531955719,
        0.003636609762907028,
        -0.0019855054561048746,
        -0.01342480257153511,
        0.0052772643975913525,
        0.013717403635382652,
        -0.026724206283688545,
        0.0027675158344209194,
        0.0054792980663478374,
        0.0037724601570516825,
        -0.02397933229804039,
        -0.017221646383404732,
        0.011676165275275707,
        0.012059332802891731,
        -0.014978373423218727,
        0.020523855462670326,
        -0.004420361947268248,
        -0.02967808023095131,
        0.0049393801018595695,
        0.02679387293756008,
        0.0015971126267686486,
        -0.020900055766105652,
        0.0110839968547225,
        -0.027936408296227455,
        0.0032029335852712393,
        0.02161065861582756,
        0.012958034873008728,
        -0.0052842311561107635,
        -0.002953874645754695,
        0.005691782105714083,
        -0.02573493681848049,
        -0.00224153115414083,
        -0.007691220846027136,
        0.0058694323524832726,
        -0.018461717292666435,
        0.04625879228115082,
        0.02917647920548916,
        -0.05105186998844147,
        0.017458513379096985,
        -0.013780103996396065,
        -0.008833757601678371,
        -0.019032984972000122,
        0.0032691170927137136,
        0.005033430177718401,
        -0.021136924624443054,
        0.02101152390241623,
        0.0037480767350643873,
        -0.03505636006593704,
        -0.0019611220341175795,
        -0.0012052366510033607,
        0.008680489845573902,
        -0.01047789491713047,
        0.02574886940419674,
        0.2039845585823059,
        -0.03405315801501274,
        0.01833631657063961,
        0.0029573580250144005,
        0.013780103996396065,
        0.0176257137209177,
        0.0037550434935837984,
        0.017709314823150635,
        -0.00217186426743865,
        -0.001844429993070662,
        0.00048462007544003427,
        0.008060455322265625,
        -0.013222768902778625,
        0.0024365982972085476,
        0.0224187932908535,
        0.0010824487544596195,
        -0.012066299095749855,
        -0.046175193041563034,
        -0.00687611848115921,
        -0.0056778485886752605,
        -0.005225013941526413,
        0.014699706807732582,
        -0.003213383723050356,
        -0.036505430936813354,
        0.014351372607052326,
        -0.012916235253214836,
        -0.006691501475870609,
        -0.026919273659586906,
        0.006419800687581301,
        0.03472195938229561,
        -0.007377719972282648,
        0.01858711615204811,
        -0.008241589181125164,
        0.015479975380003452,
        -0.009690659120678902,
        -0.020384522154927254,
        -0.02272532880306244,
        0.02222372591495514,
        0.011056129820644855,
        -0.009425925090909004,
        0.035084228962659836,
        0.010401261039078236,
        -0.008025621995329857,
        -0.012331033125519753,
        -0.00031154145835898817,
        -0.00023947982117533684,
        0.000322644627885893,
        -0.013814937323331833,
        -0.002095230622217059,
        0.03023541532456875,
        -0.010512728244066238,
        0.02654307149350643,
        0.039124906063079834,
        0.042691849172115326,
        0.0030461831483989954,
        -0.014267771504819393,
        0.002694365568459034,
        0.00595651613548398,
        0.005886849481612444,
        0.01949278637766838,
        -0.025762803852558136,
        0.0186428502202034,
        0.002546323463320732,
        0.03112715110182762,
        0.03179595246911049,
        0.015856176614761353,
        -0.019882921129465103,
        0.012393733486533165,
        0.015730775892734528,
        -0.004723412916064262,
        -0.015382441692054272,
        -0.016817579045891762,
        -0.005925165954977274,
        -0.004653745796531439,
        -0.03502849489450455,
        -0.02126232348382473,
        0.051414139568805695,
        0.01818304881453514,
        0.009565259329974651,
        0.034081026911735535,
        -0.018754318356513977,
        -0.005834599025547504,
        0.011369630694389343,
        -0.014894773252308369,
        -0.026779938489198685,
        -0.043054115027189255,
        0.017305247485637665,
        -0.01395427156239748,
        -0.017946181818842888,
        -0.016483178362250328,
        0.0023791231215000153,
        -0.021081190556287766,
        -0.008959157392382622,
        0.008617790415883064,
        -0.0004689450142905116,
        -0.010735662654042244,
        -0.019172318279743195,
        0.03940357267856598,
        -0.0014569080667570233,
        -0.021596726030111313,
        -0.03452689200639725,
        0.04703906178474426,
        0.03842823579907417,
        0.02644553780555725,
        -0.0008155376417562366,
        0.004946346860378981,
        -0.01019922737032175,
        -0.008157989010214806,
        0.022502394393086433,
        -0.0013027702225372195,
        0.001785213127732277,
        -0.012386767193675041,
        0.0019053885480389,
        0.007997754961252213,
        -0.0018931968370452523,
        -0.011327831074595451,
        0.005221530795097351,
        -0.014393172226846218,
        0.012637567706406116,
        -0.005221530795097351,
        0.0005899911629967391,
        -0.03388595953583717,
        -0.039821576327085495,
        0.017946181818842888,
        -0.0014830331783741713,
        -0.004685095977038145,
        -0.016413511708378792,
        -0.02619473822414875,
        -0.01430957205593586,
        -0.007405586540699005,
        0.011327831074595451,
        -0.03265982121229172,
        0.03530716150999069,
        -0.021582791581749916,
        -0.02906501293182373,
        0.00651036761701107,
        0.01950671896338463,
        -0.017542114481329918,
        -0.01779291406273842,
        0.009941460564732552,
        0.009460759349167347,
        -5.8073856052942574e-05,
        0.006141133140772581,
        -0.006984102074056864,
        0.01627417653799057,
        -0.0468439944088459,
        0.0076215541921556,
        -0.014434972777962685,
        -0.0014351372374221683,
        -0.03625463321805,
        -0.01478330697864294,
        -0.007649420760571957,
        0.0015466042095795274,
        -0.019311651587486267,
        0.023463796824216843,
        -0.002380864694714546,
        -0.018670717254281044,
        -0.015159507282078266,
        -0.013264569453895092,
        0.0021109057124704123,
        -0.028786344453692436,
        -0.015591442584991455,
        -0.0022641727700829506,
        -0.025219401344656944,
        -0.009077590890228748,
        -0.009579192847013474,
        -0.17745541036128998,
        0.010847128927707672,
        0.02076072245836258,
        -0.032074619084596634,
        -0.005552448332309723,
        0.02711433917284012,
        0.032269686460494995,
        -0.009850893169641495,
        -0.020482055842876434,
        0.003523401217535138,
        -0.0010632904013618827,
        -0.017012646421790123,
        -0.02856341004371643,
        -0.0007136498461477458,
        0.004350695293396711,
        0.01306950207799673,
        0.0018235299503430724,
        0.02045418880879879,
        0.024898933246731758,
        0.02825687639415264,
        0.03143368661403656,
        -0.044614654034376144,
        0.013536269776523113,
        -0.007321986369788647,
        0.02064925618469715,
        -0.009767292998731136,
        -0.010171361267566681,
        0.03137795254588127,
        -0.017151979729533195,
        -0.014671839773654938,
        0.012128999456763268,
        0.02725367434322834,
        0.012101132422685623,
        -0.004552728962153196,
        0.00025515485322102904,
        0.020886123180389404,
        0.01123029738664627,
        -0.01334816962480545,
        -0.0011651781387627125,
        -0.008415755815804005,
        0.05252880975604057,
        -0.00992752704769373,
        -0.0076145874336361885,
        -0.014121471904218197,
        -0.023644929751753807,
        -0.011097930371761322,
        0.007670320570468903,
        -0.0022746229078620672,
        0.016009442508220673,
        0.018364183604717255,
        -0.01748638041317463,
        -0.020384522154927254,
        0.020635321736335754,
        0.006398900877684355,
        0.007005002349615097,
        0.02679387293756008,
        0.011662231758236885,
        0.00821372214704752,
        -0.026166871190071106,
        0.015535708516836166,
        0.004977697040885687,
        -0.033022090792655945,
        0.026013603433966637,
        0.002319906372576952,
        -0.014978373423218727,
        -0.0016868087695911527,
        0.0018479133723303676,
        0.004333278629928827,
        -0.021986858919262886,
        0.011202430352568626,
        -0.03647756576538086,
        -0.023463796824216843,
        0.0003302644472569227,
        0.0047408295795321465,
        0.022599928081035614,
        -0.00012757742661051452,
        -0.019367385655641556,
        -0.0012609701370820403,
        0.001722512999549508,
        -0.01123029738664627,
        -0.033022090792655945,
        0.0317402184009552,
        -0.0096976263448596,
        0.026975005865097046,
        0.0032081587705761194,
        -0.012087199836969376,
        0.005521098151803017,
        0.018322382122278214,
        -0.019715720787644386,
        -0.026431605219841003,
        0.011432331055402756,
        0.0060226996429264545,
        0.017542114481329918,
        0.0019785386975854635,
        0.007412553299218416,
        0.01779291406273842,
        0.010220128111541271,
        -0.015284908004105091,
        0.022181926295161247,
        -0.012853534892201424,
        -0.02105332352221012,
        -0.017347047105431557,
        0.006381483748555183,
        0.009307491593062878,
        0.012783868238329887,
        0.02533086948096752,
        -0.019618187099695206,
        0.006694984622299671,
        0.029037145897746086,
        0.011481097899377346,
        -0.006043599918484688,
        -0.010833196341991425,
        0.00023664960463065654,
        0.02629227191209793,
        0.017820781096816063,
        0.009133324958384037,
        -0.03193528577685356,
        -0.0027918992564082146,
        0.005256364122033119,
        -0.02599967084825039,
        0.043499983847141266,
        -0.010603295639157295,
        0.003612226340919733,
        0.014978373423218727,
        -0.0007920250645838678,
        -0.01804371550679207,
        -0.11425364017486572,
        -0.008206755854189396,
        -0.002555032027885318,
        0.004354178439825773,
        -0.014490705914795399,
        -0.004120794590562582,
        0.02172212488949299,
        0.029483012855052948,
        -0.026027537882328033,
        0.010624195449054241,
        -0.020579589530825615,
        -0.02711433917284012,
        0.013090401887893677,
        0.004803529474884272,
        -0.005319064483046532,
        0.008185855112969875,
        0.0033701341599226,
        -0.0047582462430000305,
        -0.009251758456230164,
        0.026682404801249504,
        -0.0026473405305296183,
        -0.01959032006561756,
        0.014393172226846218,
        -0.024048998951911926,
        0.015842242166399956,
        0.00677858479321003,
        -0.029705947265028954,
        0.0015788250602781773,
        0.021889325231313705,
        0.011028262786567211,
        0.007398620247840881,
        -0.03455476090312004,
        -0.00881285686045885,
        -0.003410192672163248,
        -0.0019837636500597,
        -0.003326592268422246,
        -0.012909268029034138,
        -0.022558128461241722,
        0.005782349035143852,
        -0.03405315801501274,
        0.0029051078017801046,
        0.013292435556650162,
        -0.004169561434537172,
        -0.010157427750527859,
        -0.010136527009308338,
        -0.00012942795001436025,
        -0.014434972777962685,
        0.030151814222335815,
        0.023770330473780632,
        -0.00803258828818798,
        -0.012198666110634804,
        -0.02045418880879879,
        -0.02499646693468094,
        -0.005367831327021122,
        0.020941857248544693,
        0.033133555203676224,
        0.0186428502202034,
        0.013598970137536526,
        -0.004517895635217428,
        -0.012965002097189426,
        -0.000530774355866015,
        -0.005092646926641464,
        -0.01698477938771248,
        0.005144897382706404,
        0.018628917634487152,
        -0.001567504252307117,
        -0.05473028123378754,
        -0.011167597025632858,
        -0.008673523552715778,
        -0.005350414663553238,
        -0.014156305231153965,
        0.01793224923312664,
        0.013194902800023556,
        0.02155492454767227,
        -0.01858711615204811,
        -0.01012259442359209,
        -0.014281705021858215,
        -0.0021091641392558813,
        0.0022066975943744183,
        -0.018726451322436333,
        -0.01090286299586296,
        -0.0274487417191267,
        0.007698187604546547,
        -0.002624698681756854,
        -0.010220128111541271,
        -0.012804768048226833,
        -0.0012182991486042738,
        -0.02499646693468094,
        -0.011745831929147243,
        -0.029761681333184242,
        0.009662793017923832,
        0.008645656518638134,
        -0.029761681333184242,
        -0.008234621956944466,
        -0.004180011339485645,
        -0.0024644650984555483,
        -0.0031994504388421774,
        -0.01390550471842289,
        0.020217321813106537,
        0.005987866315990686,
        -0.015075907111167908,
        0.0043750787153840065,
        -0.04391798377037048,
        0.016162710264325142,
        0.0032952423207461834,
        -0.01698477938771248,
        0.011655264534056187,
        0.018823985010385513,
        0.03692343458533287,
        -0.017040513455867767,
        0.002717007417231798,
        0.008854657411575317,
        -0.007363786455243826,
        0.021485257893800735,
        -0.01738884672522545,
        -0.018977250903844833,
        -0.006447667255997658,
        -0.010993429459631443,
        0.005925165954977274,
        -0.004507445730268955,
        0.029204346239566803,
        0.005803248845040798,
        -0.01499230694025755,
        -0.016538910567760468,
        0.009572225622832775,
        -0.00025798508431762457,
        -0.007774821016937494,
        0.013828870840370655,
        -0.010234060697257519,
        0.026320137083530426,
        -0.026612738147377968,
        -0.011780665256083012,
        0.020077988505363464,
        -0.02403506450355053,
        -0.0008969607879407704,
        0.030040347948670387,
        -0.0010589362354949117,
        0.016664311289787292,
        0.02423013187944889,
        0.008596889674663544,
        0.03622676432132721,
        0.004639812745153904,
        -0.014755439944565296,
        -0.03600383177399635,
        0.0020307889208197594,
        -0.012540034018456936,
        -0.00260728201828897,
        0.022251592949032784,
        -0.0027222323697060347,
        0.003500759368762374,
        0.015479975380003452,
        -0.006398900877684355,
        0.008778023533523083,
        0.02091399021446705,
        0.019074784591794014,
        -0.029009278863668442,
        -0.015953710302710533,
        -0.00849238969385624,
        -0.0035286261700093746,
        0.02278106100857258,
        0.002276364481076598,
        -0.0034415426198393106,
        0.0367841012775898,
        -0.015048041008412838,
        0.022279459983110428,
        0.003981460817158222,
        0.0076424540020525455,
        -0.008750157430768013,
        0.012428566813468933,
        0.007039835676550865,
        -0.02912074513733387,
        -0.02827080897986889,
        -0.010568462312221527,
        0.009251758456230164,
        -0.011773698963224888,
        0.02565133571624756,
        0.029037145897746086,
        0.005252880975604057,
        -0.0007088602287694812,
        0.0114601980894804,
        -0.005998316220939159,
        0.01478330697864294,
        0.012484300881624222,
        -0.02459239959716797,
        -0.016845446079969406,
        -0.014448905363678932,
        0.024453066289424896,
        0.002779707545414567,
        -0.004451712127774954,
        0.012170800007879734,
        -0.005378281231969595,
        0.02195899188518524,
        0.012184732593595982,
        0.001861846656538546,
        0.009760326705873013,
        0.03555796295404434,
        0.00401629414409399,
        0.01490870676934719,
        0.007349853403866291,
        -0.019269851967692375,
        0.006796001922339201,
        -0.021749991923570633,
        0.006127200089395046,
        0.015828309580683708,
        -0.012463400140404701,
        -0.010094727389514446,
        -0.023157263174653053,
        0.021136924624443054,
        -0.013229736126959324,
        -0.00457362923771143,
        0.003953594248741865,
        0.03770370036363602,
        -0.0011442780960351229,
        0.015298841521143913,
        -0.01219169981777668,
        0.01080532930791378,
        -0.034276094287633896,
        0.02765774168074131,
        -0.007210519630461931,
        -0.036505430936813354,
        -0.026570938527584076,
        0.033022090792655945,
        0.021931126713752747,
        -0.01518737431615591,
        0.044558919966220856,
        -0.012212599627673626,
        0.018559250980615616,
        0.010819262824952602,
        0.024717800319194794,
        -0.011906065978109837,
        0.02638980560004711,
        0.02278106100857258,
        0.017597848549485207,
        -0.008680489845573902,
        0.01989685371518135,
        -0.01874038390815258,
        -0.021136924624443054,
        -0.012310133315622807,
        -0.006607901304960251,
        0.03279915452003479,
        -0.0008817211491987109,
        0.10160214453935623,
        0.021248390898108482,
        -0.0030949499923735857,
        0.012115065939724445,
        -0.015702908858656883,
        0.011871231719851494,
        0.013362103141844273,
        0.018921518698334694,
        -0.011474130675196648,
        -0.0010545820696279407,
        0.006339683663100004,
        -0.015591442584991455,
        -0.013689536601305008,
        -0.02423013187944889,
        0.0035355929285287857,
        -0.015605375170707703,
        0.003765493631362915,
        0.008617790415883064,
        -0.008610823191702366,
        -0.0030200581531971693,
        0.024299798533320427,
        -0.00043737722444348037,
        0.011223330162465572,
        0.01602337695658207,
        -0.013118268921971321,
        -0.009669759310781956,
        0.007018935866653919,
        -0.016455311328172684,
        -0.008673523552715778,
        -0.03112715110182762,
        0.004406428430229425,
        -0.016566777601838112,
        -0.04166077822446823,
        -0.013571103103458881,
        0.02815934270620346,
        0.01097949594259262,
        -0.001757346442900598,
        0.013341202400624752,
        0.014121471904218197,
        5.785614484921098e-05,
        0.009293558076024055,
        0.036449696868658066,
        -0.0029138161335140467,
        -0.00964885950088501,
        -0.013647736981511116,
        -0.007997754961252213,
        0.0014725831570103765,
        -0.013731337152421474,
        -0.02312939614057541
      ]
    },
    {
      "node": "check_cache_answer",
      "flow_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7",
      "run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_check_cache_answer_0",
      "status": "Completed",
      "inputs": {
        "conn": "entaoai",
        "embeddedQuestion": [
          -0.011118830181658268,
          -0.01631597802042961,
          0.02559560351073742,
          -0.002060397295281291,
          -0.005500198341906071,
          0.031210750341415405,
          0.015424241311848164,
          -0.011599531397223473,
          -0.01773718185722828,
          -0.027727408334612846,
          0.028535543009638786,
          0.04366718605160713,
          -0.035139963030815125,
          0.005427048075944185,
          -0.0009474692051298916,
          0.008422723039984703,
          0.011244230903685093,
          0.009614026173949242,
          0.027797074988484383,
          -0.016956912353634834,
          -0.0016798420110717416,
          -0.006102816667407751,
          -0.016552845016121864,
          -0.0035739096347242594,
          -0.010742628946900368,
          -0.00849238969385624,
          0.029538746923208237,
          -0.020328788086771965,
          -0.003713243408128619,
          -0.009676726534962654,
          0.0021788307931274176,
          0.02357526309788227,
          -0.017918314784765244,
          -0.009948426857590675,
          -0.009258724749088287,
          0.006694984622299671,
          -0.0029294912237674,
          -0.016915112733840942,
          0.02438339963555336,
          -0.023212995380163193,
          0.0071164690889418125,
          0.021694257855415344,
          0.003977977205067873,
          -0.015270974487066269,
          -0.005953032989054918,
          0.01672004535794258,
          0.010652062483131886,
          -0.0065695843659341335,
          -0.012059332802891731,
          0.0093214251101017,
          0.00438204500824213,
          0.015856176614761353,
          -0.03945930674672127,
          -0.020997589454054832,
          -0.02251632697880268,
          -0.0034084508661180735,
          -0.002352998126298189,
          0.032966356724500656,
          -0.0272118728607893,
          -0.013543237000703812,
          0.0005050846957601607,
          -0.008130121976137161,
          0.0009370191837660968,
          0.008053488098084927,
          -0.022404860705137253,
          -0.024466998875141144,
          -0.008548122830688953,
          0.008018654771149158,
          0.019437052309513092,
          0.007398620247840881,
          0.01959032006561756,
          0.010038994252681732,
          0.0009866568725556135,
          -0.001406399649567902,
          0.0428033173084259,
          -0.01388460397720337,
          0.002535873558372259,
          -0.014755439944565296,
          0.001035423600114882,
          0.03592022880911827,
          0.012735101394355297,
          -0.002624698681756854,
          -0.0010606779251247644,
          0.026487339287996292,
          0.015897976234555244,
          -0.012742067687213421,
          0.010436095297336578,
          0.025275135412812233,
          0.002154447603970766,
          0.0040302276611328125,
          0.027379073202610016,
          0.03536289557814598,
          0.012944101355969906,
          0.0067054349929094315,
          -0.0038839271292090416,
          0.012247432954609394,
          -0.01118153054267168,
          0.038818370550870895,
          0.02634800411760807,
          -0.009523458778858185,
          -0.010157427750527859,
          0.008513289503753185,
          -0.04497692361474037,
          -0.005552448332309723,
          -0.024606334045529366,
          0.014852973632514477,
          0.010749596171081066,
          -0.01130693033337593,
          0.003262150567024946,
          0.001316703506745398,
          -0.019074784591794014,
          0.030207548290491104,
          0.006221250165253878,
          -0.024216199293732643,
          -0.0005220660241320729,
          -0.002276364481076598,
          0.015479975380003452,
          -0.039319973438978195,
          -0.009669759310781956,
          0.012602734379470348,
          0.017165912315249443,
          -0.004075510893017054,
          0.010721729137003422,
          -0.034582626074552536,
          0.03288275748491287,
          -0.003422384150326252,
          -0.010819262824952602,
          -0.03380235657095909,
          -0.006520817521959543,
          -0.04288691654801369,
          0.027420874685049057,
          0.022641727700829506,
          0.015939775854349136,
          -0.022753195837140083,
          0.005636048503220081,
          0.01854531653225422,
          -0.02760200761258602,
          -0.02050992287695408,
          -0.01959032006561756,
          -0.004643295891582966,
          0.02257206104695797,
          0.003828193759545684,
          -0.014253838919103146,
          -0.023993264883756638,
          -0.010192261077463627,
          0.018601050600409508,
          0.018782183527946472,
          0.020412389189004898,
          0.0024139564484357834,
          -0.010408228263258934,
          0.026334071531891823,
          -0.001065902877599001,
          -0.029761681333184242,
          0.013807971030473709,
          0.007726054172962904,
          0.0011695323046296835,
          0.00037707184674218297,
          0.009753359481692314,
          -0.009732459671795368,
          -0.005601215176284313,
          -0.020412389189004898,
          0.017012646421790123,
          0.017096245661377907,
          -0.009307491593062878,
          0.024118665605783463,
          0.03349582478404045,
          0.0020900056697428226,
          -0.014866907149553299,
          0.007328953128308058,
          -0.029204346239566803,
          -0.026222603395581245,
          0.018252715468406677,
          -0.01959032006561756,
          -0.003438059240579605,
          0.0040128109976649284,
          0.00180088821798563,
          0.03391382470726967,
          -0.02806180901825428,
          -0.02987314760684967,
          -0.015619308687746525,
          0.0193952526897192,
          0.0065521677024662495,
          0.020119788125157356,
          0.023547397926449776,
          -0.03098781779408455,
          -0.02559560351073742,
          -0.003594809677451849,
          -0.01974358595907688,
          0.01012259442359209,
          -0.034192491322755814,
          0.02382606454193592,
          0.007102536037564278,
          0.000681428937241435,
          -0.01843385025858879,
          -0.6148517727851868,
          -0.03820530325174332,
          -0.009300525300204754,
          -0.028535543009638786,
          0.01698477938771248,
          0.005514131393283606,
          -0.011627398431301117,
          0.03349582478404045,
          -0.024104731157422066,
          0.033579424023628235,
          -0.0008451460162177682,
          -0.023296596482396126,
          -0.0038212270010262728,
          0.007733020931482315,
          -0.010498794727027416,
          -0.012992868199944496,
          0.006503400858491659,
          0.004162594676017761,
          0.00922389142215252,
          0.022335194051265717,
          -0.03500062972307205,
          0.00739165348932147,
          -0.026069337502121925,
          -0.004152144771069288,
          -0.025372669100761414,
          0.0031245583668351173,
          0.006506884470582008,
          -0.021095123142004013,
          -0.009816059842705727,
          0.017151979729533195,
          -0.02045418880879879,
          -0.001589275081641972,
          0.021889325231313705,
          -0.030207548290491104,
          0.025010401383042336,
          0.021471325308084488,
          0.0021439974661916494,
          0.009084558114409447,
          -0.0007767854258418083,
          0.027880676090717316,
          -0.01596764288842678,
          -0.002070847200229764,
          -0.0024035065434873104,
          -0.0031228167936205864,
          0.017360979691147804,
          -0.000540788983926177,
          0.0186428502202034,
          -0.010498794727027416,
          0.006841285154223442,
          -0.004591045901179314,
          -0.008708356879651546,
          -0.003521659644320607,
          -0.007572787348181009,
          -0.002574190264567733,
          0.03550222888588905,
          0.011418397538363934,
          0.0342482253909111,
          -0.0024975568521767855,
          0.0014272996922954917,
          -0.011919998563826084,
          0.008631723001599312,
          0.017305247485637665,
          -0.012303167022764683,
          -0.01918625272810459,
          0.018823985010385513,
          0.009565259329974651,
          -0.006287433672696352,
          0.003291758941486478,
          0.00914725847542286,
          -0.04558999091386795,
          0.0003217737830709666,
          0.015229174867272377,
          0.0004941992810927331,
          -0.015284908004105091,
          0.00554199842736125,
          0.012338000349700451,
          0.0033718757331371307,
          0.011188496835529804,
          -0.002103938953951001,
          0.019757520407438278,
          -0.017444580793380737,
          -0.012247432954609394,
          -0.028842078521847725,
          0.002976516494527459,
          -0.0024052481167018414,
          0.0001037927286233753,
          -0.01738884672522545,
          0.017193779349327087,
          -0.01478330697864294,
          -0.0036505432799458504,
          0.0033718757331371307,
          0.03199101984500885,
          -0.028842078521847725,
          -0.042050912976264954,
          0.004918479826301336,
          0.011376597918570042,
          0.013125235214829445,
          -0.0074613201431930065,
          0.03112715110182762,
          -0.03722996637225151,
          0.014246871694922447,
          -0.020565655082464218,
          0.012351933866739273,
          -0.005886849481612444,
          0.02675207331776619,
          0.03600383177399635,
          -0.007475253660231829,
          0.0011259905295446515,
          -0.025707069784402847,
          0.0035303679760545492,
          -0.005911232437938452,
          -0.010401261039078236,
          -0.0035390763077884912,
          -0.0003302644472569227,
          -0.014212038367986679,
          -0.04040677472949028,
          0.011843365617096424,
          0.01065902877599001,
          0.01631597802042961,
          -0.003516434459015727,
          0.02599967084825039,
          0.01843385025858879,
          0.015521774999797344,
          -0.02886994555592537,
          0.01868464983999729,
          0.012045399285852909,
          0.0024522733874619007,
          0.006210800260305405,
          0.020844323560595512,
          0.02886994555592537,
          -0.011495031416416168,
          0.02172212488949299,
          0.010213160887360573,
          -0.010213160887360573,
          -0.002133547328412533,
          0.01571684330701828,
          0.0012836117530241609,
          -0.007726054172962904,
          0.01097949594259262,
          -0.0035094679333269596,
          -0.02130412496626377,
          -0.008144055493175983,
          -0.00489758001640439,
          -0.012553967535495758,
          -0.028591277077794075,
          -0.015647174790501595,
          -0.021931126713752747,
          -0.0011024780105799437,
          0.0001773239200701937,
          -0.02266959473490715,
          0.011599531397223473,
          -0.018475649878382683,
          -0.02528906799852848,
          -0.017347047105431557,
          -0.023394130170345306,
          -0.014616106636822224,
          -0.011655264534056187,
          -0.032213952392339706,
          -0.008617790415883064,
          -0.029092878103256226,
          0.006754201836884022,
          0.02534480206668377,
          -0.027588075026869774,
          -0.017765048891305923,
          -0.009676726534962654,
          -0.013877637684345245,
          0.0418279804289341,
          0.030792750418186188,
          0.0045875622890889645,
          -0.016190577298402786,
          0.013034668751060963,
          -0.03792663663625717,
          0.0009213441517204046,
          0.02201472595334053,
          0.0011190238874405622,
          0.01848958246409893,
          -0.02605540305376053,
          0.009906627237796783,
          -0.00020399325876496732,
          -0.032576221972703934,
          -0.009857860393822193,
          0.040573976933956146,
          -0.002218889305368066,
          0.006458117626607418,
          0.012233499437570572,
          0.0017199005233123899,
          0.003911793697625399,
          -0.0036261598579585552,
          -0.021025456488132477,
          0.015842242166399956,
          0.011237263679504395,
          0.019534585997462273,
          -0.017904382199048996,
          0.01627417653799057,
          -0.004322828259319067,
          0.02237699367105961,
          0.007948988117277622,
          0.027615942060947418,
          -0.014532506465911865,
          0.018754318356513977,
          0.025163667276501656,
          0.009850893169641495,
          -0.0022711395286023617,
          -0.0011756281601265073,
          -0.006064499728381634,
          -0.00982302613556385,
          -0.0011277322191745043,
          -0.03553009778261185,
          0.023338396102190018,
          0.0015709876315668225,
          0.02220979332923889,
          -0.02906501293182373,
          -0.033217158168554306,
          -0.01804371550679207,
          -0.008206755854189396,
          0.0012374575017020106,
          -0.009084558114409447,
          0.03265982121229172,
          -0.0279642753303051,
          0.00955132581293583,
          -0.006064499728381634,
          -0.012909268029034138,
          0.038706906139850616,
          0.012254400178790092,
          0.006893535144627094,
          0.006043599918484688,
          -0.0029660663567483425,
          -0.00960009265691042,
          0.015521774999797344,
          -0.020342722535133362,
          0.019228052347898483,
          0.02076072245836258,
          0.029956746846437454,
          -0.0038665104657411575,
          -0.015424241311848164,
          -0.010533628053963184,
          0.02630620449781418,
          0.009356258437037468,
          0.03279915452003479,
          0.003523401217535138,
          -0.012156866490840912,
          0.0030531499069184065,
          -0.004110344685614109,
          -0.0040406775660812855,
          0.053977880626916885,
          -0.01005292683839798,
          0.01995258778333664,
          0.030820617452263832,
          -0.01141143124550581,
          -0.0074404203332960606,
          0.017263446003198624,
          -0.011947865597903728,
          -0.017360979691147804,
          0.007551887072622776,
          0.017207713797688484,
          0.0009274400072172284,
          0.024801399558782578,
          0.0003629207785706967,
          -0.011195464059710503,
          0.010011127218604088,
          0.004267094656825066,
          -0.01249823346734047,
          0.02024518884718418,
          0.01787651516497135,
          0.023561330512166023,
          -0.014643972739577293,
          -0.030151814222335815,
          -0.028298676013946533,
          -0.012017532251775265,
          0.00023577875981573015,
          -0.017514247447252274,
          -0.004256644751876593,
          -0.010247994214296341,
          0.006573067978024483,
          0.003876960603520274,
          0.004364628344774246,
          -0.02363099716603756,
          0.01733311451971531,
          0.005022980272769928,
          0.02181965857744217,
          -0.006931852083653212,
          -0.020886123180389404,
          0.028131475672125816,
          -0.0040580942295491695,
          -0.011146697215735912,
          -0.015438174828886986,
          -0.019910788163542747,
          -0.010617229156196117,
          -0.028075743466615677,
          0.017597848549485207,
          0.008882524445652962,
          0.0077887545339763165,
          -0.020273054018616676,
          -0.003695826744660735,
          -0.002946907887235284,
          0.0015431208303198218,
          0.008861623704433441,
          -0.01118153054267168,
          0.013076468370854855,
          0.01125816348940134,
          -0.012045399285852909,
          0.005364347714930773,
          -0.016761844977736473,
          0.014448905363678932,
          0.049909334629774094,
          -0.006653184536844492,
          -0.02172212488949299,
          -0.01985505409538746,
          -0.018768250942230225,
          -0.018670717254281044,
          0.02332446351647377,
          0.010387328453361988,
          -0.019116584211587906,
          0.01153683103621006,
          0.024759599938988686,
          0.02372853085398674,
          -0.009425925090909004,
          0.0016868087695911527,
          0.011919998563826084,
          -0.0067054349929094315,
          -0.017402781173586845,
          -0.03753650188446045,
          -0.029455145820975304,
          0.0005564640159718692,
          0.025804603472352028,
          0.06325750797986984,
          0.0035094679333269596,
          0.0036993101239204407,
          -0.019464919343590736,
          0.0037480767350643873,
          -0.018015848472714424,
          -0.01592584326863289,
          0.0011303446954116225,
          -0.015633242204785347,
          -0.021485257893800735,
          -0.00836002267897129,
          0.0035652013029903173,
          0.0056499820202589035,
          0.02126232348382473,
          0.0009013149538077414,
          0.0013184451963752508,
          -0.05768415704369545,
          0.02559560351073742,
          -0.000957048439886421,
          0.003974494058638811,
          -0.014337439090013504,
          0.020816456526517868,
          0.03461049497127533,
          0.005099613685160875,
          -0.01752818003296852,
          0.01899118535220623,
          0.02509400062263012,
          0.018712516874074936,
          -0.04010024294257164,
          0.0060157328844070435,
          0.006917918566614389,
          -0.008798924274742603,
          0.00041408237302675843,
          -0.013139168731868267,
          0.022599928081035614,
          0.01708231307566166,
          0.006802968215197325,
          0.03380235657095909,
          0.0037202101666480303,
          -0.003915277309715748,
          0.012066299095749855,
          0.008645656518638134,
          -0.006245633587241173,
          -0.0037027932703495026,
          0.00836002267897129,
          -0.0035808763932436705,
          0.018503516912460327,
          0.02282286249101162,
          0.0013524078531190753,
          0.007238386198878288,
          -0.014379238709807396,
          -0.01783471554517746,
          -0.004451712127774954,
          -0.0074891867116093636,
          -0.007384686730802059,
          -0.011982698924839497,
          -0.009405025281012058,
          -0.025261200964450836,
          -0.023254796862602234,
          -0.003335300600156188,
          -0.00876409001648426,
          0.018475649878382683,
          -0.01139053050428629,
          0.009614026173949242,
          -0.005716165527701378,
          -0.003869993845000863,
          -0.017165912315249443,
          -0.013362103141844273,
          -0.0016040793852880597,
          -0.02634800411760807,
          -0.006130683235824108,
          -0.04840452969074249,
          -0.020328788086771965,
          0.023686731234192848,
          0.024397332221269608,
          0.029148612171411514,
          -0.013313336297869682,
          0.005172763951122761,
          -0.00545143149793148,
          -0.010059894062578678,
          -0.008018654771149158,
          0.016538910567760468,
          -0.005611665081232786,
          -0.016455311328172684,
          0.02609720453619957,
          -0.034136757254600525,
          -0.026111137121915817,
          -0.01950671896338463,
          0.02161065861582756,
          -0.011550764553248882,
          0.01779291406273842,
          -0.01468577329069376,
          -0.022237660363316536,
          -0.0067054349929094315,
          0.029287945479154587,
          -0.004845329560339451,
          0.02746267430484295,
          -0.0020969724282622337,
          -0.02605540305376053,
          0.020872190594673157,
          -0.03722996637225151,
          0.0004397720331326127,
          -0.03472195938229561,
          -0.012888368219137192,
          -0.003988427575677633,
          -0.0021770892199128866,
          0.012331033125519753,
          -0.014072705060243607,
          -0.003828193759545684,
          -0.0120941661298275,
          -0.01315310224890709,
          -0.01251913420855999,
          -0.0035025011748075485,
          0.046119458973407745,
          0.013055568560957909,
          0.004500478971749544,
          -0.014797240495681763,
          0.009202991612255573,
          -0.020523855462670326,
          -0.006771618500351906,
          -0.015340641140937805,
          0.02553986944258213,
          0.00311759184114635,
          -0.014866907149553299,
          0.03867903724312782,
          -0.01282566785812378,
          -0.011926965788006783,
          -0.015438174828886986,
          0.035335030406713486,
          -0.0234359297901392,
          0.045617856085300446,
          0.0035460428334772587,
          0.0027309407014399767,
          -0.017500314861536026,
          0.008276422508060932,
          -0.005280747544020414,
          -0.010895895771682262,
          -0.03148941695690155,
          0.018614983186125755,
          0.026361938565969467,
          -0.012303167022764683,
          -0.003533851122483611,
          -0.022530261427164078,
          -0.01612091064453125,
          -0.020091921091079712,
          -0.024271931499242783,
          0.04508838802576065,
          -0.0038978606462478638,
          0.003025283105671406,
          0.007168719545006752,
          0.009363225661218166,
          -0.0028371827211230993,
          -0.0002782320079859346,
          -0.037257835268974304,
          -0.020426321774721146,
          0.0015509583754464984,
          0.0397937074303627,
          0.0193952526897192,
          0.024620266631245613,
          0.04701119288802147,
          -0.011947865597903728,
          0.018127316609025,
          0.020217321813106537,
          -0.0008625627378933132,
          -0.0031837753485888243,
          -0.030346881598234177,
          -0.008234621956944466,
          -0.015995509922504425,
          0.014191138558089733,
          0.007067702244967222,
          -0.0076424540020525455,
          0.006956235505640507,
          0.01985505409538746,
          0.015006240457296371,
          0.011077029630541801,
          -0.003838643664494157,
          -0.0021370307076722383,
          0.0010876738233491778,
          -0.019172318279743195,
          -0.01256790105253458,
          0.01869858428835869,
          -0.018308449536561966,
          0.00748222041875124,
          -0.03380235657095909,
          -0.004695545881986618,
          0.0031559085473418236,
          -0.018865784630179405,
          0.011940899305045605,
          -0.00720355287194252,
          0.019074784591794014,
          -0.019269851967692375,
          0.010282827541232109,
          -0.016998711973428726,
          0.013821904547512531,
          -0.013229736126959324,
          -0.010888929478824139,
          -0.023463796824216843,
          -0.001105090486817062,
          0.01677577942609787,
          0.019966520369052887,
          -0.011620431207120419,
          0.0018583633936941624,
          -0.030514081940054893,
          0.015326708555221558,
          -0.008137088268995285,
          -0.029148612171411514,
          -0.003768976777791977,
          0.001748638111166656,
          0.007879321463406086,
          -0.016817579045891762,
          -0.04946346580982208,
          -0.034638360142707825,
          -0.013362103141844273,
          -0.009634925983846188,
          -0.001558795920573175,
          -0.016399577260017395,
          0.015298841521143913,
          -0.011042196303606033,
          -0.04135424643754959,
          -0.004496995359659195,
          -0.0026560488622635603,
          0.03023541532456875,
          0.022335194051265717,
          0.005200630519539118,
          0.012985901907086372,
          0.030904216691851616,
          0.013389969244599342,
          -0.011007362976670265,
          0.00028258617385290563,
          -0.019019052386283875,
          0.009920559823513031,
          0.011188496835529804,
          -0.023867864161729813,
          -0.0428311824798584,
          0.0299846138805151,
          0.027393007650971413,
          -0.017667515203356743,
          -0.029343679547309875,
          0.015856176614761353,
          -0.018782183527946472,
          -0.001386370393447578,
          0.00997629389166832,
          -0.011425363831222057,
          -0.021122990176081657,
          -0.009035791270434856,
          -0.012170800007879734,
          0.014295638538897038,
          0.01468577329069376,
          -0.005298164207488298,
          -0.017848648130893707,
          0.013013768941164017,
          -0.020119788125157356,
          0.005218047648668289,
          0.010881963185966015,
          -0.010847128927707672,
          0.024007197469472885,
          0.01471363939344883,
          0.022711394354701042,
          0.014267771504819393,
          0.03304995596408844,
          0.026334071531891823,
          -0.030709149315953255,
          0.018378116190433502,
          -0.013452669605612755,
          -0.003040958195924759,
          -0.007224452681839466,
          0.005360864568501711,
          0.004469128791242838,
          0.0304862167686224,
          -0.022711394354701042,
          -0.00927265826612711,
          0.004099894315004349,
          -0.009767292998731136,
          0.017653580754995346,
          0.008729256689548492,
          -0.00025994444149546325,
          0.004911513067781925,
          -0.011522897519171238,
          0.007928088307380676,
          0.034638360142707825,
          0.006123716477304697,
          -0.02176392637193203,
          -0.0007519666105508804,
          -0.0016493628500029445,
          0.0037445933558046818,
          -0.02690533921122551,
          0.009453792124986649,
          0.029510879889130592,
          -0.0418279804289341,
          -0.00914029125124216,
          0.027518408372998238,
          0.011334797367453575,
          0.011049163527786732,
          -0.029148612171411514,
          0.011090963147580624,
          0.013167035765945911,
          0.021025456488132477,
          -0.013410869985818863,
          -0.010206194594502449,
          -0.007503120228648186,
          0.015326708555221558,
          -0.03006821498274803,
          -0.017165912315249443,
          -0.019924720749258995,
          -0.011857299134135246,
          0.0158701092004776,
          0.021039390936493874,
          0.007565820589661598,
          -0.007872354239225388,
          0.010540595278143883,
          -0.029343679547309875,
          0.010241027921438217,
          0.009753359481692314,
          -0.004260128363966942,
          -0.00318725872784853,
          0.014769373461604118,
          -0.0038665104657411575,
          0.010450027883052826,
          0.0176257137209177,
          0.020133720710873604,
          -0.0065626176074147224,
          0.01000415999442339,
          -0.030597683042287827,
          -0.015897976234555244,
          -0.007955954410135746,
          0.004124277736991644,
          -0.00038055519689805806,
          -0.02800607495009899,
          0.006447667255997658,
          0.004775662906467915,
          0.00568133220076561,
          -0.0022397893480956554,
          -0.015354574657976627,
          -0.005329514387995005,
          0.002267656149342656,
          -0.011676165275275707,
          0.00816495530307293,
          0.016566777601838112,
          -0.023310529068112373,
          0.037007033824920654,
          0.0067054349929094315,
          0.04951919987797737,
          0.0018670717254281044,
          -0.01606517657637596,
          -0.03491702675819397,
          -0.026125071570277214,
          -0.017556047067046165,
          -0.026877472177147865,
          -0.03304995596408844,
          -0.025484135374426842,
          0.02790854312479496,
          0.019562453031539917,
          -0.027128273621201515,
          -0.03260408714413643,
          -0.018196983262896538,
          -0.019659986719489098,
          0.00997629389166832,
          0.01646924391388893,
          0.01779291406273842,
          0.010575428605079651,
          0.01617664471268654,
          -0.02841014415025711,
          0.03714636713266373,
          0.010087761096656322,
          -0.0006901372689753771,
          -0.013550203293561935,
          0.012581833638250828,
          0.011397497728466988,
          -0.01612091064453125,
          0.004803529474884272,
          0.02473173290491104,
          -0.04751279577612877,
          -0.0007872354472056031,
          0.03784303739666939,
          -0.006451150868088007,
          0.02403506450355053,
          0.03918064013123512,
          -0.019715720787644386,
          -0.03480556234717369,
          -0.01430957205593586,
          0.018322382122278214,
          0.033022090792655945,
          0.01692904531955719,
          0.003636609762907028,
          -0.0019855054561048746,
          -0.01342480257153511,
          0.0052772643975913525,
          0.013717403635382652,
          -0.026724206283688545,
          0.0027675158344209194,
          0.0054792980663478374,
          0.0037724601570516825,
          -0.02397933229804039,
          -0.017221646383404732,
          0.011676165275275707,
          0.012059332802891731,
          -0.014978373423218727,
          0.020523855462670326,
          -0.004420361947268248,
          -0.02967808023095131,
          0.0049393801018595695,
          0.02679387293756008,
          0.0015971126267686486,
          -0.020900055766105652,
          0.0110839968547225,
          -0.027936408296227455,
          0.0032029335852712393,
          0.02161065861582756,
          0.012958034873008728,
          -0.0052842311561107635,
          -0.002953874645754695,
          0.005691782105714083,
          -0.02573493681848049,
          -0.00224153115414083,
          -0.007691220846027136,
          0.0058694323524832726,
          -0.018461717292666435,
          0.04625879228115082,
          0.02917647920548916,
          -0.05105186998844147,
          0.017458513379096985,
          -0.013780103996396065,
          -0.008833757601678371,
          -0.019032984972000122,
          0.0032691170927137136,
          0.005033430177718401,
          -0.021136924624443054,
          0.02101152390241623,
          0.0037480767350643873,
          -0.03505636006593704,
          -0.0019611220341175795,
          -0.0012052366510033607,
          0.008680489845573902,
          -0.01047789491713047,
          0.02574886940419674,
          0.2039845585823059,
          -0.03405315801501274,
          0.01833631657063961,
          0.0029573580250144005,
          0.013780103996396065,
          0.0176257137209177,
          0.0037550434935837984,
          0.017709314823150635,
          -0.00217186426743865,
          -0.001844429993070662,
          0.00048462007544003427,
          0.008060455322265625,
          -0.013222768902778625,
          0.0024365982972085476,
          0.0224187932908535,
          0.0010824487544596195,
          -0.012066299095749855,
          -0.046175193041563034,
          -0.00687611848115921,
          -0.0056778485886752605,
          -0.005225013941526413,
          0.014699706807732582,
          -0.003213383723050356,
          -0.036505430936813354,
          0.014351372607052326,
          -0.012916235253214836,
          -0.006691501475870609,
          -0.026919273659586906,
          0.006419800687581301,
          0.03472195938229561,
          -0.007377719972282648,
          0.01858711615204811,
          -0.008241589181125164,
          0.015479975380003452,
          -0.009690659120678902,
          -0.020384522154927254,
          -0.02272532880306244,
          0.02222372591495514,
          0.011056129820644855,
          -0.009425925090909004,
          0.035084228962659836,
          0.010401261039078236,
          -0.008025621995329857,
          -0.012331033125519753,
          -0.00031154145835898817,
          -0.00023947982117533684,
          0.000322644627885893,
          -0.013814937323331833,
          -0.002095230622217059,
          0.03023541532456875,
          -0.010512728244066238,
          0.02654307149350643,
          0.039124906063079834,
          0.042691849172115326,
          0.0030461831483989954,
          -0.014267771504819393,
          0.002694365568459034,
          0.00595651613548398,
          0.005886849481612444,
          0.01949278637766838,
          -0.025762803852558136,
          0.0186428502202034,
          0.002546323463320732,
          0.03112715110182762,
          0.03179595246911049,
          0.015856176614761353,
          -0.019882921129465103,
          0.012393733486533165,
          0.015730775892734528,
          -0.004723412916064262,
          -0.015382441692054272,
          -0.016817579045891762,
          -0.005925165954977274,
          -0.004653745796531439,
          -0.03502849489450455,
          -0.02126232348382473,
          0.051414139568805695,
          0.01818304881453514,
          0.009565259329974651,
          0.034081026911735535,
          -0.018754318356513977,
          -0.005834599025547504,
          0.011369630694389343,
          -0.014894773252308369,
          -0.026779938489198685,
          -0.043054115027189255,
          0.017305247485637665,
          -0.01395427156239748,
          -0.017946181818842888,
          -0.016483178362250328,
          0.0023791231215000153,
          -0.021081190556287766,
          -0.008959157392382622,
          0.008617790415883064,
          -0.0004689450142905116,
          -0.010735662654042244,
          -0.019172318279743195,
          0.03940357267856598,
          -0.0014569080667570233,
          -0.021596726030111313,
          -0.03452689200639725,
          0.04703906178474426,
          0.03842823579907417,
          0.02644553780555725,
          -0.0008155376417562366,
          0.004946346860378981,
          -0.01019922737032175,
          -0.008157989010214806,
          0.022502394393086433,
          -0.0013027702225372195,
          0.001785213127732277,
          -0.012386767193675041,
          0.0019053885480389,
          0.007997754961252213,
          -0.0018931968370452523,
          -0.011327831074595451,
          0.005221530795097351,
          -0.014393172226846218,
          0.012637567706406116,
          -0.005221530795097351,
          0.0005899911629967391,
          -0.03388595953583717,
          -0.039821576327085495,
          0.017946181818842888,
          -0.0014830331783741713,
          -0.004685095977038145,
          -0.016413511708378792,
          -0.02619473822414875,
          -0.01430957205593586,
          -0.007405586540699005,
          0.011327831074595451,
          -0.03265982121229172,
          0.03530716150999069,
          -0.021582791581749916,
          -0.02906501293182373,
          0.00651036761701107,
          0.01950671896338463,
          -0.017542114481329918,
          -0.01779291406273842,
          0.009941460564732552,
          0.009460759349167347,
          -5.8073856052942574e-05,
          0.006141133140772581,
          -0.006984102074056864,
          0.01627417653799057,
          -0.0468439944088459,
          0.0076215541921556,
          -0.014434972777962685,
          -0.0014351372374221683,
          -0.03625463321805,
          -0.01478330697864294,
          -0.007649420760571957,
          0.0015466042095795274,
          -0.019311651587486267,
          0.023463796824216843,
          -0.002380864694714546,
          -0.018670717254281044,
          -0.015159507282078266,
          -0.013264569453895092,
          0.0021109057124704123,
          -0.028786344453692436,
          -0.015591442584991455,
          -0.0022641727700829506,
          -0.025219401344656944,
          -0.009077590890228748,
          -0.009579192847013474,
          -0.17745541036128998,
          0.010847128927707672,
          0.02076072245836258,
          -0.032074619084596634,
          -0.005552448332309723,
          0.02711433917284012,
          0.032269686460494995,
          -0.009850893169641495,
          -0.020482055842876434,
          0.003523401217535138,
          -0.0010632904013618827,
          -0.017012646421790123,
          -0.02856341004371643,
          -0.0007136498461477458,
          0.004350695293396711,
          0.01306950207799673,
          0.0018235299503430724,
          0.02045418880879879,
          0.024898933246731758,
          0.02825687639415264,
          0.03143368661403656,
          -0.044614654034376144,
          0.013536269776523113,
          -0.007321986369788647,
          0.02064925618469715,
          -0.009767292998731136,
          -0.010171361267566681,
          0.03137795254588127,
          -0.017151979729533195,
          -0.014671839773654938,
          0.012128999456763268,
          0.02725367434322834,
          0.012101132422685623,
          -0.004552728962153196,
          0.00025515485322102904,
          0.020886123180389404,
          0.01123029738664627,
          -0.01334816962480545,
          -0.0011651781387627125,
          -0.008415755815804005,
          0.05252880975604057,
          -0.00992752704769373,
          -0.0076145874336361885,
          -0.014121471904218197,
          -0.023644929751753807,
          -0.011097930371761322,
          0.007670320570468903,
          -0.0022746229078620672,
          0.016009442508220673,
          0.018364183604717255,
          -0.01748638041317463,
          -0.020384522154927254,
          0.020635321736335754,
          0.006398900877684355,
          0.007005002349615097,
          0.02679387293756008,
          0.011662231758236885,
          0.00821372214704752,
          -0.026166871190071106,
          0.015535708516836166,
          0.004977697040885687,
          -0.033022090792655945,
          0.026013603433966637,
          0.002319906372576952,
          -0.014978373423218727,
          -0.0016868087695911527,
          0.0018479133723303676,
          0.004333278629928827,
          -0.021986858919262886,
          0.011202430352568626,
          -0.03647756576538086,
          -0.023463796824216843,
          0.0003302644472569227,
          0.0047408295795321465,
          0.022599928081035614,
          -0.00012757742661051452,
          -0.019367385655641556,
          -0.0012609701370820403,
          0.001722512999549508,
          -0.01123029738664627,
          -0.033022090792655945,
          0.0317402184009552,
          -0.0096976263448596,
          0.026975005865097046,
          0.0032081587705761194,
          -0.012087199836969376,
          0.005521098151803017,
          0.018322382122278214,
          -0.019715720787644386,
          -0.026431605219841003,
          0.011432331055402756,
          0.0060226996429264545,
          0.017542114481329918,
          0.0019785386975854635,
          0.007412553299218416,
          0.01779291406273842,
          0.010220128111541271,
          -0.015284908004105091,
          0.022181926295161247,
          -0.012853534892201424,
          -0.02105332352221012,
          -0.017347047105431557,
          0.006381483748555183,
          0.009307491593062878,
          0.012783868238329887,
          0.02533086948096752,
          -0.019618187099695206,
          0.006694984622299671,
          0.029037145897746086,
          0.011481097899377346,
          -0.006043599918484688,
          -0.010833196341991425,
          0.00023664960463065654,
          0.02629227191209793,
          0.017820781096816063,
          0.009133324958384037,
          -0.03193528577685356,
          -0.0027918992564082146,
          0.005256364122033119,
          -0.02599967084825039,
          0.043499983847141266,
          -0.010603295639157295,
          0.003612226340919733,
          0.014978373423218727,
          -0.0007920250645838678,
          -0.01804371550679207,
          -0.11425364017486572,
          -0.008206755854189396,
          -0.002555032027885318,
          0.004354178439825773,
          -0.014490705914795399,
          -0.004120794590562582,
          0.02172212488949299,
          0.029483012855052948,
          -0.026027537882328033,
          0.010624195449054241,
          -0.020579589530825615,
          -0.02711433917284012,
          0.013090401887893677,
          0.004803529474884272,
          -0.005319064483046532,
          0.008185855112969875,
          0.0033701341599226,
          -0.0047582462430000305,
          -0.009251758456230164,
          0.026682404801249504,
          -0.0026473405305296183,
          -0.01959032006561756,
          0.014393172226846218,
          -0.024048998951911926,
          0.015842242166399956,
          0.00677858479321003,
          -0.029705947265028954,
          0.0015788250602781773,
          0.021889325231313705,
          0.011028262786567211,
          0.007398620247840881,
          -0.03455476090312004,
          -0.00881285686045885,
          -0.003410192672163248,
          -0.0019837636500597,
          -0.003326592268422246,
          -0.012909268029034138,
          -0.022558128461241722,
          0.005782349035143852,
          -0.03405315801501274,
          0.0029051078017801046,
          0.013292435556650162,
          -0.004169561434537172,
          -0.010157427750527859,
          -0.010136527009308338,
          -0.00012942795001436025,
          -0.014434972777962685,
          0.030151814222335815,
          0.023770330473780632,
          -0.00803258828818798,
          -0.012198666110634804,
          -0.02045418880879879,
          -0.02499646693468094,
          -0.005367831327021122,
          0.020941857248544693,
          0.033133555203676224,
          0.0186428502202034,
          0.013598970137536526,
          -0.004517895635217428,
          -0.012965002097189426,
          -0.000530774355866015,
          -0.005092646926641464,
          -0.01698477938771248,
          0.005144897382706404,
          0.018628917634487152,
          -0.001567504252307117,
          -0.05473028123378754,
          -0.011167597025632858,
          -0.008673523552715778,
          -0.005350414663553238,
          -0.014156305231153965,
          0.01793224923312664,
          0.013194902800023556,
          0.02155492454767227,
          -0.01858711615204811,
          -0.01012259442359209,
          -0.014281705021858215,
          -0.0021091641392558813,
          0.0022066975943744183,
          -0.018726451322436333,
          -0.01090286299586296,
          -0.0274487417191267,
          0.007698187604546547,
          -0.002624698681756854,
          -0.010220128111541271,
          -0.012804768048226833,
          -0.0012182991486042738,
          -0.02499646693468094,
          -0.011745831929147243,
          -0.029761681333184242,
          0.009662793017923832,
          0.008645656518638134,
          -0.029761681333184242,
          -0.008234621956944466,
          -0.004180011339485645,
          -0.0024644650984555483,
          -0.0031994504388421774,
          -0.01390550471842289,
          0.020217321813106537,
          0.005987866315990686,
          -0.015075907111167908,
          0.0043750787153840065,
          -0.04391798377037048,
          0.016162710264325142,
          0.0032952423207461834,
          -0.01698477938771248,
          0.011655264534056187,
          0.018823985010385513,
          0.03692343458533287,
          -0.017040513455867767,
          0.002717007417231798,
          0.008854657411575317,
          -0.007363786455243826,
          0.021485257893800735,
          -0.01738884672522545,
          -0.018977250903844833,
          -0.006447667255997658,
          -0.010993429459631443,
          0.005925165954977274,
          -0.004507445730268955,
          0.029204346239566803,
          0.005803248845040798,
          -0.01499230694025755,
          -0.016538910567760468,
          0.009572225622832775,
          -0.00025798508431762457,
          -0.007774821016937494,
          0.013828870840370655,
          -0.010234060697257519,
          0.026320137083530426,
          -0.026612738147377968,
          -0.011780665256083012,
          0.020077988505363464,
          -0.02403506450355053,
          -0.0008969607879407704,
          0.030040347948670387,
          -0.0010589362354949117,
          0.016664311289787292,
          0.02423013187944889,
          0.008596889674663544,
          0.03622676432132721,
          0.004639812745153904,
          -0.014755439944565296,
          -0.03600383177399635,
          0.0020307889208197594,
          -0.012540034018456936,
          -0.00260728201828897,
          0.022251592949032784,
          -0.0027222323697060347,
          0.003500759368762374,
          0.015479975380003452,
          -0.006398900877684355,
          0.008778023533523083,
          0.02091399021446705,
          0.019074784591794014,
          -0.029009278863668442,
          -0.015953710302710533,
          -0.00849238969385624,
          -0.0035286261700093746,
          0.02278106100857258,
          0.002276364481076598,
          -0.0034415426198393106,
          0.0367841012775898,
          -0.015048041008412838,
          0.022279459983110428,
          0.003981460817158222,
          0.0076424540020525455,
          -0.008750157430768013,
          0.012428566813468933,
          0.007039835676550865,
          -0.02912074513733387,
          -0.02827080897986889,
          -0.010568462312221527,
          0.009251758456230164,
          -0.011773698963224888,
          0.02565133571624756,
          0.029037145897746086,
          0.005252880975604057,
          -0.0007088602287694812,
          0.0114601980894804,
          -0.005998316220939159,
          0.01478330697864294,
          0.012484300881624222,
          -0.02459239959716797,
          -0.016845446079969406,
          -0.014448905363678932,
          0.024453066289424896,
          0.002779707545414567,
          -0.004451712127774954,
          0.012170800007879734,
          -0.005378281231969595,
          0.02195899188518524,
          0.012184732593595982,
          0.001861846656538546,
          0.009760326705873013,
          0.03555796295404434,
          0.00401629414409399,
          0.01490870676934719,
          0.007349853403866291,
          -0.019269851967692375,
          0.006796001922339201,
          -0.021749991923570633,
          0.006127200089395046,
          0.015828309580683708,
          -0.012463400140404701,
          -0.010094727389514446,
          -0.023157263174653053,
          0.021136924624443054,
          -0.013229736126959324,
          -0.00457362923771143,
          0.003953594248741865,
          0.03770370036363602,
          -0.0011442780960351229,
          0.015298841521143913,
          -0.01219169981777668,
          0.01080532930791378,
          -0.034276094287633896,
          0.02765774168074131,
          -0.007210519630461931,
          -0.036505430936813354,
          -0.026570938527584076,
          0.033022090792655945,
          0.021931126713752747,
          -0.01518737431615591,
          0.044558919966220856,
          -0.012212599627673626,
          0.018559250980615616,
          0.010819262824952602,
          0.024717800319194794,
          -0.011906065978109837,
          0.02638980560004711,
          0.02278106100857258,
          0.017597848549485207,
          -0.008680489845573902,
          0.01989685371518135,
          -0.01874038390815258,
          -0.021136924624443054,
          -0.012310133315622807,
          -0.006607901304960251,
          0.03279915452003479,
          -0.0008817211491987109,
          0.10160214453935623,
          0.021248390898108482,
          -0.0030949499923735857,
          0.012115065939724445,
          -0.015702908858656883,
          0.011871231719851494,
          0.013362103141844273,
          0.018921518698334694,
          -0.011474130675196648,
          -0.0010545820696279407,
          0.006339683663100004,
          -0.015591442584991455,
          -0.013689536601305008,
          -0.02423013187944889,
          0.0035355929285287857,
          -0.015605375170707703,
          0.003765493631362915,
          0.008617790415883064,
          -0.008610823191702366,
          -0.0030200581531971693,
          0.024299798533320427,
          -0.00043737722444348037,
          0.011223330162465572,
          0.01602337695658207,
          -0.013118268921971321,
          -0.009669759310781956,
          0.007018935866653919,
          -0.016455311328172684,
          -0.008673523552715778,
          -0.03112715110182762,
          0.004406428430229425,
          -0.016566777601838112,
          -0.04166077822446823,
          -0.013571103103458881,
          0.02815934270620346,
          0.01097949594259262,
          -0.001757346442900598,
          0.013341202400624752,
          0.014121471904218197,
          5.785614484921098e-05,
          0.009293558076024055,
          0.036449696868658066,
          -0.0029138161335140467,
          -0.00964885950088501,
          -0.013647736981511116,
          -0.007997754961252213,
          0.0014725831570103765,
          -0.013731337152421474,
          -0.02312939614057541
        ],
        "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
        "indexType": "cogsearchvs",
        "question": "How are sentence pairs represented in the input to BERT?"
      },
      "output": {
        "jsonAnswer": null,
        "existingAnswer": false
      },
      "metrics": null,
      "error": null,
      "parent_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_0",
      "start_time": "2023-09-20T16:48:30.003197Z",
      "end_time": "2023-09-20T16:48:32.108483Z",
      "index": 0,
      "api_calls": [
        {
          "name": "checkCacheAnswer",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "embeddedQuestion": [
              -0.011118830181658268,
              -0.01631597802042961,
              0.02559560351073742,
              -0.002060397295281291,
              -0.005500198341906071,
              0.031210750341415405,
              0.015424241311848164,
              -0.011599531397223473,
              -0.01773718185722828,
              -0.027727408334612846,
              0.028535543009638786,
              0.04366718605160713,
              -0.035139963030815125,
              0.005427048075944185,
              -0.0009474692051298916,
              0.008422723039984703,
              0.011244230903685093,
              0.009614026173949242,
              0.027797074988484383,
              -0.016956912353634834,
              -0.0016798420110717416,
              -0.006102816667407751,
              -0.016552845016121864,
              -0.0035739096347242594,
              -0.010742628946900368,
              -0.00849238969385624,
              0.029538746923208237,
              -0.020328788086771965,
              -0.003713243408128619,
              -0.009676726534962654,
              0.0021788307931274176,
              0.02357526309788227,
              -0.017918314784765244,
              -0.009948426857590675,
              -0.009258724749088287,
              0.006694984622299671,
              -0.0029294912237674,
              -0.016915112733840942,
              0.02438339963555336,
              -0.023212995380163193,
              0.0071164690889418125,
              0.021694257855415344,
              0.003977977205067873,
              -0.015270974487066269,
              -0.005953032989054918,
              0.01672004535794258,
              0.010652062483131886,
              -0.0065695843659341335,
              -0.012059332802891731,
              0.0093214251101017,
              0.00438204500824213,
              0.015856176614761353,
              -0.03945930674672127,
              -0.020997589454054832,
              -0.02251632697880268,
              -0.0034084508661180735,
              -0.002352998126298189,
              0.032966356724500656,
              -0.0272118728607893,
              -0.013543237000703812,
              0.0005050846957601607,
              -0.008130121976137161,
              0.0009370191837660968,
              0.008053488098084927,
              -0.022404860705137253,
              -0.024466998875141144,
              -0.008548122830688953,
              0.008018654771149158,
              0.019437052309513092,
              0.007398620247840881,
              0.01959032006561756,
              0.010038994252681732,
              0.0009866568725556135,
              -0.001406399649567902,
              0.0428033173084259,
              -0.01388460397720337,
              0.002535873558372259,
              -0.014755439944565296,
              0.001035423600114882,
              0.03592022880911827,
              0.012735101394355297,
              -0.002624698681756854,
              -0.0010606779251247644,
              0.026487339287996292,
              0.015897976234555244,
              -0.012742067687213421,
              0.010436095297336578,
              0.025275135412812233,
              0.002154447603970766,
              0.0040302276611328125,
              0.027379073202610016,
              0.03536289557814598,
              0.012944101355969906,
              0.0067054349929094315,
              -0.0038839271292090416,
              0.012247432954609394,
              -0.01118153054267168,
              0.038818370550870895,
              0.02634800411760807,
              -0.009523458778858185,
              -0.010157427750527859,
              0.008513289503753185,
              -0.04497692361474037,
              -0.005552448332309723,
              -0.024606334045529366,
              0.014852973632514477,
              0.010749596171081066,
              -0.01130693033337593,
              0.003262150567024946,
              0.001316703506745398,
              -0.019074784591794014,
              0.030207548290491104,
              0.006221250165253878,
              -0.024216199293732643,
              -0.0005220660241320729,
              -0.002276364481076598,
              0.015479975380003452,
              -0.039319973438978195,
              -0.009669759310781956,
              0.012602734379470348,
              0.017165912315249443,
              -0.004075510893017054,
              0.010721729137003422,
              -0.034582626074552536,
              0.03288275748491287,
              -0.003422384150326252,
              -0.010819262824952602,
              -0.03380235657095909,
              -0.006520817521959543,
              -0.04288691654801369,
              0.027420874685049057,
              0.022641727700829506,
              0.015939775854349136,
              -0.022753195837140083,
              0.005636048503220081,
              0.01854531653225422,
              -0.02760200761258602,
              -0.02050992287695408,
              -0.01959032006561756,
              -0.004643295891582966,
              0.02257206104695797,
              0.003828193759545684,
              -0.014253838919103146,
              -0.023993264883756638,
              -0.010192261077463627,
              0.018601050600409508,
              0.018782183527946472,
              0.020412389189004898,
              0.0024139564484357834,
              -0.010408228263258934,
              0.026334071531891823,
              -0.001065902877599001,
              -0.029761681333184242,
              0.013807971030473709,
              0.007726054172962904,
              0.0011695323046296835,
              0.00037707184674218297,
              0.009753359481692314,
              -0.009732459671795368,
              -0.005601215176284313,
              -0.020412389189004898,
              0.017012646421790123,
              0.017096245661377907,
              -0.009307491593062878,
              0.024118665605783463,
              0.03349582478404045,
              0.0020900056697428226,
              -0.014866907149553299,
              0.007328953128308058,
              -0.029204346239566803,
              -0.026222603395581245,
              0.018252715468406677,
              -0.01959032006561756,
              -0.003438059240579605,
              0.0040128109976649284,
              0.00180088821798563,
              0.03391382470726967,
              -0.02806180901825428,
              -0.02987314760684967,
              -0.015619308687746525,
              0.0193952526897192,
              0.0065521677024662495,
              0.020119788125157356,
              0.023547397926449776,
              -0.03098781779408455,
              -0.02559560351073742,
              -0.003594809677451849,
              -0.01974358595907688,
              0.01012259442359209,
              -0.034192491322755814,
              0.02382606454193592,
              0.007102536037564278,
              0.000681428937241435,
              -0.01843385025858879,
              -0.6148517727851868,
              -0.03820530325174332,
              -0.009300525300204754,
              -0.028535543009638786,
              0.01698477938771248,
              0.005514131393283606,
              -0.011627398431301117,
              0.03349582478404045,
              -0.024104731157422066,
              0.033579424023628235,
              -0.0008451460162177682,
              -0.023296596482396126,
              -0.0038212270010262728,
              0.007733020931482315,
              -0.010498794727027416,
              -0.012992868199944496,
              0.006503400858491659,
              0.004162594676017761,
              0.00922389142215252,
              0.022335194051265717,
              -0.03500062972307205,
              0.00739165348932147,
              -0.026069337502121925,
              -0.004152144771069288,
              -0.025372669100761414,
              0.0031245583668351173,
              0.006506884470582008,
              -0.021095123142004013,
              -0.009816059842705727,
              0.017151979729533195,
              -0.02045418880879879,
              -0.001589275081641972,
              0.021889325231313705,
              -0.030207548290491104,
              0.025010401383042336,
              0.021471325308084488,
              0.0021439974661916494,
              0.009084558114409447,
              -0.0007767854258418083,
              0.027880676090717316,
              -0.01596764288842678,
              -0.002070847200229764,
              -0.0024035065434873104,
              -0.0031228167936205864,
              0.017360979691147804,
              -0.000540788983926177,
              0.0186428502202034,
              -0.010498794727027416,
              0.006841285154223442,
              -0.004591045901179314,
              -0.008708356879651546,
              -0.003521659644320607,
              -0.007572787348181009,
              -0.002574190264567733,
              0.03550222888588905,
              0.011418397538363934,
              0.0342482253909111,
              -0.0024975568521767855,
              0.0014272996922954917,
              -0.011919998563826084,
              0.008631723001599312,
              0.017305247485637665,
              -0.012303167022764683,
              -0.01918625272810459,
              0.018823985010385513,
              0.009565259329974651,
              -0.006287433672696352,
              0.003291758941486478,
              0.00914725847542286,
              -0.04558999091386795,
              0.0003217737830709666,
              0.015229174867272377,
              0.0004941992810927331,
              -0.015284908004105091,
              0.00554199842736125,
              0.012338000349700451,
              0.0033718757331371307,
              0.011188496835529804,
              -0.002103938953951001,
              0.019757520407438278,
              -0.017444580793380737,
              -0.012247432954609394,
              -0.028842078521847725,
              0.002976516494527459,
              -0.0024052481167018414,
              0.0001037927286233753,
              -0.01738884672522545,
              0.017193779349327087,
              -0.01478330697864294,
              -0.0036505432799458504,
              0.0033718757331371307,
              0.03199101984500885,
              -0.028842078521847725,
              -0.042050912976264954,
              0.004918479826301336,
              0.011376597918570042,
              0.013125235214829445,
              -0.0074613201431930065,
              0.03112715110182762,
              -0.03722996637225151,
              0.014246871694922447,
              -0.020565655082464218,
              0.012351933866739273,
              -0.005886849481612444,
              0.02675207331776619,
              0.03600383177399635,
              -0.007475253660231829,
              0.0011259905295446515,
              -0.025707069784402847,
              0.0035303679760545492,
              -0.005911232437938452,
              -0.010401261039078236,
              -0.0035390763077884912,
              -0.0003302644472569227,
              -0.014212038367986679,
              -0.04040677472949028,
              0.011843365617096424,
              0.01065902877599001,
              0.01631597802042961,
              -0.003516434459015727,
              0.02599967084825039,
              0.01843385025858879,
              0.015521774999797344,
              -0.02886994555592537,
              0.01868464983999729,
              0.012045399285852909,
              0.0024522733874619007,
              0.006210800260305405,
              0.020844323560595512,
              0.02886994555592537,
              -0.011495031416416168,
              0.02172212488949299,
              0.010213160887360573,
              -0.010213160887360573,
              -0.002133547328412533,
              0.01571684330701828,
              0.0012836117530241609,
              -0.007726054172962904,
              0.01097949594259262,
              -0.0035094679333269596,
              -0.02130412496626377,
              -0.008144055493175983,
              -0.00489758001640439,
              -0.012553967535495758,
              -0.028591277077794075,
              -0.015647174790501595,
              -0.021931126713752747,
              -0.0011024780105799437,
              0.0001773239200701937,
              -0.02266959473490715,
              0.011599531397223473,
              -0.018475649878382683,
              -0.02528906799852848,
              -0.017347047105431557,
              -0.023394130170345306,
              -0.014616106636822224,
              -0.011655264534056187,
              -0.032213952392339706,
              -0.008617790415883064,
              -0.029092878103256226,
              0.006754201836884022,
              0.02534480206668377,
              -0.027588075026869774,
              -0.017765048891305923,
              -0.009676726534962654,
              -0.013877637684345245,
              0.0418279804289341,
              0.030792750418186188,
              0.0045875622890889645,
              -0.016190577298402786,
              0.013034668751060963,
              -0.03792663663625717,
              0.0009213441517204046,
              0.02201472595334053,
              0.0011190238874405622,
              0.01848958246409893,
              -0.02605540305376053,
              0.009906627237796783,
              -0.00020399325876496732,
              -0.032576221972703934,
              -0.009857860393822193,
              0.040573976933956146,
              -0.002218889305368066,
              0.006458117626607418,
              0.012233499437570572,
              0.0017199005233123899,
              0.003911793697625399,
              -0.0036261598579585552,
              -0.021025456488132477,
              0.015842242166399956,
              0.011237263679504395,
              0.019534585997462273,
              -0.017904382199048996,
              0.01627417653799057,
              -0.004322828259319067,
              0.02237699367105961,
              0.007948988117277622,
              0.027615942060947418,
              -0.014532506465911865,
              0.018754318356513977,
              0.025163667276501656,
              0.009850893169641495,
              -0.0022711395286023617,
              -0.0011756281601265073,
              -0.006064499728381634,
              -0.00982302613556385,
              -0.0011277322191745043,
              -0.03553009778261185,
              0.023338396102190018,
              0.0015709876315668225,
              0.02220979332923889,
              -0.02906501293182373,
              -0.033217158168554306,
              -0.01804371550679207,
              -0.008206755854189396,
              0.0012374575017020106,
              -0.009084558114409447,
              0.03265982121229172,
              -0.0279642753303051,
              0.00955132581293583,
              -0.006064499728381634,
              -0.012909268029034138,
              0.038706906139850616,
              0.012254400178790092,
              0.006893535144627094,
              0.006043599918484688,
              -0.0029660663567483425,
              -0.00960009265691042,
              0.015521774999797344,
              -0.020342722535133362,
              0.019228052347898483,
              0.02076072245836258,
              0.029956746846437454,
              -0.0038665104657411575,
              -0.015424241311848164,
              -0.010533628053963184,
              0.02630620449781418,
              0.009356258437037468,
              0.03279915452003479,
              0.003523401217535138,
              -0.012156866490840912,
              0.0030531499069184065,
              -0.004110344685614109,
              -0.0040406775660812855,
              0.053977880626916885,
              -0.01005292683839798,
              0.01995258778333664,
              0.030820617452263832,
              -0.01141143124550581,
              -0.0074404203332960606,
              0.017263446003198624,
              -0.011947865597903728,
              -0.017360979691147804,
              0.007551887072622776,
              0.017207713797688484,
              0.0009274400072172284,
              0.024801399558782578,
              0.0003629207785706967,
              -0.011195464059710503,
              0.010011127218604088,
              0.004267094656825066,
              -0.01249823346734047,
              0.02024518884718418,
              0.01787651516497135,
              0.023561330512166023,
              -0.014643972739577293,
              -0.030151814222335815,
              -0.028298676013946533,
              -0.012017532251775265,
              0.00023577875981573015,
              -0.017514247447252274,
              -0.004256644751876593,
              -0.010247994214296341,
              0.006573067978024483,
              0.003876960603520274,
              0.004364628344774246,
              -0.02363099716603756,
              0.01733311451971531,
              0.005022980272769928,
              0.02181965857744217,
              -0.006931852083653212,
              -0.020886123180389404,
              0.028131475672125816,
              -0.0040580942295491695,
              -0.011146697215735912,
              -0.015438174828886986,
              -0.019910788163542747,
              -0.010617229156196117,
              -0.028075743466615677,
              0.017597848549485207,
              0.008882524445652962,
              0.0077887545339763165,
              -0.020273054018616676,
              -0.003695826744660735,
              -0.002946907887235284,
              0.0015431208303198218,
              0.008861623704433441,
              -0.01118153054267168,
              0.013076468370854855,
              0.01125816348940134,
              -0.012045399285852909,
              0.005364347714930773,
              -0.016761844977736473,
              0.014448905363678932,
              0.049909334629774094,
              -0.006653184536844492,
              -0.02172212488949299,
              -0.01985505409538746,
              -0.018768250942230225,
              -0.018670717254281044,
              0.02332446351647377,
              0.010387328453361988,
              -0.019116584211587906,
              0.01153683103621006,
              0.024759599938988686,
              0.02372853085398674,
              -0.009425925090909004,
              0.0016868087695911527,
              0.011919998563826084,
              -0.0067054349929094315,
              -0.017402781173586845,
              -0.03753650188446045,
              -0.029455145820975304,
              0.0005564640159718692,
              0.025804603472352028,
              0.06325750797986984,
              0.0035094679333269596,
              0.0036993101239204407,
              -0.019464919343590736,
              0.0037480767350643873,
              -0.018015848472714424,
              -0.01592584326863289,
              0.0011303446954116225,
              -0.015633242204785347,
              -0.021485257893800735,
              -0.00836002267897129,
              0.0035652013029903173,
              0.0056499820202589035,
              0.02126232348382473,
              0.0009013149538077414,
              0.0013184451963752508,
              -0.05768415704369545,
              0.02559560351073742,
              -0.000957048439886421,
              0.003974494058638811,
              -0.014337439090013504,
              0.020816456526517868,
              0.03461049497127533,
              0.005099613685160875,
              -0.01752818003296852,
              0.01899118535220623,
              0.02509400062263012,
              0.018712516874074936,
              -0.04010024294257164,
              0.0060157328844070435,
              0.006917918566614389,
              -0.008798924274742603,
              0.00041408237302675843,
              -0.013139168731868267,
              0.022599928081035614,
              0.01708231307566166,
              0.006802968215197325,
              0.03380235657095909,
              0.0037202101666480303,
              -0.003915277309715748,
              0.012066299095749855,
              0.008645656518638134,
              -0.006245633587241173,
              -0.0037027932703495026,
              0.00836002267897129,
              -0.0035808763932436705,
              0.018503516912460327,
              0.02282286249101162,
              0.0013524078531190753,
              0.007238386198878288,
              -0.014379238709807396,
              -0.01783471554517746,
              -0.004451712127774954,
              -0.0074891867116093636,
              -0.007384686730802059,
              -0.011982698924839497,
              -0.009405025281012058,
              -0.025261200964450836,
              -0.023254796862602234,
              -0.003335300600156188,
              -0.00876409001648426,
              0.018475649878382683,
              -0.01139053050428629,
              0.009614026173949242,
              -0.005716165527701378,
              -0.003869993845000863,
              -0.017165912315249443,
              -0.013362103141844273,
              -0.0016040793852880597,
              -0.02634800411760807,
              -0.006130683235824108,
              -0.04840452969074249,
              -0.020328788086771965,
              0.023686731234192848,
              0.024397332221269608,
              0.029148612171411514,
              -0.013313336297869682,
              0.005172763951122761,
              -0.00545143149793148,
              -0.010059894062578678,
              -0.008018654771149158,
              0.016538910567760468,
              -0.005611665081232786,
              -0.016455311328172684,
              0.02609720453619957,
              -0.034136757254600525,
              -0.026111137121915817,
              -0.01950671896338463,
              0.02161065861582756,
              -0.011550764553248882,
              0.01779291406273842,
              -0.01468577329069376,
              -0.022237660363316536,
              -0.0067054349929094315,
              0.029287945479154587,
              -0.004845329560339451,
              0.02746267430484295,
              -0.0020969724282622337,
              -0.02605540305376053,
              0.020872190594673157,
              -0.03722996637225151,
              0.0004397720331326127,
              -0.03472195938229561,
              -0.012888368219137192,
              -0.003988427575677633,
              -0.0021770892199128866,
              0.012331033125519753,
              -0.014072705060243607,
              -0.003828193759545684,
              -0.0120941661298275,
              -0.01315310224890709,
              -0.01251913420855999,
              -0.0035025011748075485,
              0.046119458973407745,
              0.013055568560957909,
              0.004500478971749544,
              -0.014797240495681763,
              0.009202991612255573,
              -0.020523855462670326,
              -0.006771618500351906,
              -0.015340641140937805,
              0.02553986944258213,
              0.00311759184114635,
              -0.014866907149553299,
              0.03867903724312782,
              -0.01282566785812378,
              -0.011926965788006783,
              -0.015438174828886986,
              0.035335030406713486,
              -0.0234359297901392,
              0.045617856085300446,
              0.0035460428334772587,
              0.0027309407014399767,
              -0.017500314861536026,
              0.008276422508060932,
              -0.005280747544020414,
              -0.010895895771682262,
              -0.03148941695690155,
              0.018614983186125755,
              0.026361938565969467,
              -0.012303167022764683,
              -0.003533851122483611,
              -0.022530261427164078,
              -0.01612091064453125,
              -0.020091921091079712,
              -0.024271931499242783,
              0.04508838802576065,
              -0.0038978606462478638,
              0.003025283105671406,
              0.007168719545006752,
              0.009363225661218166,
              -0.0028371827211230993,
              -0.0002782320079859346,
              -0.037257835268974304,
              -0.020426321774721146,
              0.0015509583754464984,
              0.0397937074303627,
              0.0193952526897192,
              0.024620266631245613,
              0.04701119288802147,
              -0.011947865597903728,
              0.018127316609025,
              0.020217321813106537,
              -0.0008625627378933132,
              -0.0031837753485888243,
              -0.030346881598234177,
              -0.008234621956944466,
              -0.015995509922504425,
              0.014191138558089733,
              0.007067702244967222,
              -0.0076424540020525455,
              0.006956235505640507,
              0.01985505409538746,
              0.015006240457296371,
              0.011077029630541801,
              -0.003838643664494157,
              -0.0021370307076722383,
              0.0010876738233491778,
              -0.019172318279743195,
              -0.01256790105253458,
              0.01869858428835869,
              -0.018308449536561966,
              0.00748222041875124,
              -0.03380235657095909,
              -0.004695545881986618,
              0.0031559085473418236,
              -0.018865784630179405,
              0.011940899305045605,
              -0.00720355287194252,
              0.019074784591794014,
              -0.019269851967692375,
              0.010282827541232109,
              -0.016998711973428726,
              0.013821904547512531,
              -0.013229736126959324,
              -0.010888929478824139,
              -0.023463796824216843,
              -0.001105090486817062,
              0.01677577942609787,
              0.019966520369052887,
              -0.011620431207120419,
              0.0018583633936941624,
              -0.030514081940054893,
              0.015326708555221558,
              -0.008137088268995285,
              -0.029148612171411514,
              -0.003768976777791977,
              0.001748638111166656,
              0.007879321463406086,
              -0.016817579045891762,
              -0.04946346580982208,
              -0.034638360142707825,
              -0.013362103141844273,
              -0.009634925983846188,
              -0.001558795920573175,
              -0.016399577260017395,
              0.015298841521143913,
              -0.011042196303606033,
              -0.04135424643754959,
              -0.004496995359659195,
              -0.0026560488622635603,
              0.03023541532456875,
              0.022335194051265717,
              0.005200630519539118,
              0.012985901907086372,
              0.030904216691851616,
              0.013389969244599342,
              -0.011007362976670265,
              0.00028258617385290563,
              -0.019019052386283875,
              0.009920559823513031,
              0.011188496835529804,
              -0.023867864161729813,
              -0.0428311824798584,
              0.0299846138805151,
              0.027393007650971413,
              -0.017667515203356743,
              -0.029343679547309875,
              0.015856176614761353,
              -0.018782183527946472,
              -0.001386370393447578,
              0.00997629389166832,
              -0.011425363831222057,
              -0.021122990176081657,
              -0.009035791270434856,
              -0.012170800007879734,
              0.014295638538897038,
              0.01468577329069376,
              -0.005298164207488298,
              -0.017848648130893707,
              0.013013768941164017,
              -0.020119788125157356,
              0.005218047648668289,
              0.010881963185966015,
              -0.010847128927707672,
              0.024007197469472885,
              0.01471363939344883,
              0.022711394354701042,
              0.014267771504819393,
              0.03304995596408844,
              0.026334071531891823,
              -0.030709149315953255,
              0.018378116190433502,
              -0.013452669605612755,
              -0.003040958195924759,
              -0.007224452681839466,
              0.005360864568501711,
              0.004469128791242838,
              0.0304862167686224,
              -0.022711394354701042,
              -0.00927265826612711,
              0.004099894315004349,
              -0.009767292998731136,
              0.017653580754995346,
              0.008729256689548492,
              -0.00025994444149546325,
              0.004911513067781925,
              -0.011522897519171238,
              0.007928088307380676,
              0.034638360142707825,
              0.006123716477304697,
              -0.02176392637193203,
              -0.0007519666105508804,
              -0.0016493628500029445,
              0.0037445933558046818,
              -0.02690533921122551,
              0.009453792124986649,
              0.029510879889130592,
              -0.0418279804289341,
              -0.00914029125124216,
              0.027518408372998238,
              0.011334797367453575,
              0.011049163527786732,
              -0.029148612171411514,
              0.011090963147580624,
              0.013167035765945911,
              0.021025456488132477,
              -0.013410869985818863,
              -0.010206194594502449,
              -0.007503120228648186,
              0.015326708555221558,
              -0.03006821498274803,
              -0.017165912315249443,
              -0.019924720749258995,
              -0.011857299134135246,
              0.0158701092004776,
              0.021039390936493874,
              0.007565820589661598,
              -0.007872354239225388,
              0.010540595278143883,
              -0.029343679547309875,
              0.010241027921438217,
              0.009753359481692314,
              -0.004260128363966942,
              -0.00318725872784853,
              0.014769373461604118,
              -0.0038665104657411575,
              0.010450027883052826,
              0.0176257137209177,
              0.020133720710873604,
              -0.0065626176074147224,
              0.01000415999442339,
              -0.030597683042287827,
              -0.015897976234555244,
              -0.007955954410135746,
              0.004124277736991644,
              -0.00038055519689805806,
              -0.02800607495009899,
              0.006447667255997658,
              0.004775662906467915,
              0.00568133220076561,
              -0.0022397893480956554,
              -0.015354574657976627,
              -0.005329514387995005,
              0.002267656149342656,
              -0.011676165275275707,
              0.00816495530307293,
              0.016566777601838112,
              -0.023310529068112373,
              0.037007033824920654,
              0.0067054349929094315,
              0.04951919987797737,
              0.0018670717254281044,
              -0.01606517657637596,
              -0.03491702675819397,
              -0.026125071570277214,
              -0.017556047067046165,
              -0.026877472177147865,
              -0.03304995596408844,
              -0.025484135374426842,
              0.02790854312479496,
              0.019562453031539917,
              -0.027128273621201515,
              -0.03260408714413643,
              -0.018196983262896538,
              -0.019659986719489098,
              0.00997629389166832,
              0.01646924391388893,
              0.01779291406273842,
              0.010575428605079651,
              0.01617664471268654,
              -0.02841014415025711,
              0.03714636713266373,
              0.010087761096656322,
              -0.0006901372689753771,
              -0.013550203293561935,
              0.012581833638250828,
              0.011397497728466988,
              -0.01612091064453125,
              0.004803529474884272,
              0.02473173290491104,
              -0.04751279577612877,
              -0.0007872354472056031,
              0.03784303739666939,
              -0.006451150868088007,
              0.02403506450355053,
              0.03918064013123512,
              -0.019715720787644386,
              -0.03480556234717369,
              -0.01430957205593586,
              0.018322382122278214,
              0.033022090792655945,
              0.01692904531955719,
              0.003636609762907028,
              -0.0019855054561048746,
              -0.01342480257153511,
              0.0052772643975913525,
              0.013717403635382652,
              -0.026724206283688545,
              0.0027675158344209194,
              0.0054792980663478374,
              0.0037724601570516825,
              -0.02397933229804039,
              -0.017221646383404732,
              0.011676165275275707,
              0.012059332802891731,
              -0.014978373423218727,
              0.020523855462670326,
              -0.004420361947268248,
              -0.02967808023095131,
              0.0049393801018595695,
              0.02679387293756008,
              0.0015971126267686486,
              -0.020900055766105652,
              0.0110839968547225,
              -0.027936408296227455,
              0.0032029335852712393,
              0.02161065861582756,
              0.012958034873008728,
              -0.0052842311561107635,
              -0.002953874645754695,
              0.005691782105714083,
              -0.02573493681848049,
              -0.00224153115414083,
              -0.007691220846027136,
              0.0058694323524832726,
              -0.018461717292666435,
              0.04625879228115082,
              0.02917647920548916,
              -0.05105186998844147,
              0.017458513379096985,
              -0.013780103996396065,
              -0.008833757601678371,
              -0.019032984972000122,
              0.0032691170927137136,
              0.005033430177718401,
              -0.021136924624443054,
              0.02101152390241623,
              0.0037480767350643873,
              -0.03505636006593704,
              -0.0019611220341175795,
              -0.0012052366510033607,
              0.008680489845573902,
              -0.01047789491713047,
              0.02574886940419674,
              0.2039845585823059,
              -0.03405315801501274,
              0.01833631657063961,
              0.0029573580250144005,
              0.013780103996396065,
              0.0176257137209177,
              0.0037550434935837984,
              0.017709314823150635,
              -0.00217186426743865,
              -0.001844429993070662,
              0.00048462007544003427,
              0.008060455322265625,
              -0.013222768902778625,
              0.0024365982972085476,
              0.0224187932908535,
              0.0010824487544596195,
              -0.012066299095749855,
              -0.046175193041563034,
              -0.00687611848115921,
              -0.0056778485886752605,
              -0.005225013941526413,
              0.014699706807732582,
              -0.003213383723050356,
              -0.036505430936813354,
              0.014351372607052326,
              -0.012916235253214836,
              -0.006691501475870609,
              -0.026919273659586906,
              0.006419800687581301,
              0.03472195938229561,
              -0.007377719972282648,
              0.01858711615204811,
              -0.008241589181125164,
              0.015479975380003452,
              -0.009690659120678902,
              -0.020384522154927254,
              -0.02272532880306244,
              0.02222372591495514,
              0.011056129820644855,
              -0.009425925090909004,
              0.035084228962659836,
              0.010401261039078236,
              -0.008025621995329857,
              -0.012331033125519753,
              -0.00031154145835898817,
              -0.00023947982117533684,
              0.000322644627885893,
              -0.013814937323331833,
              -0.002095230622217059,
              0.03023541532456875,
              -0.010512728244066238,
              0.02654307149350643,
              0.039124906063079834,
              0.042691849172115326,
              0.0030461831483989954,
              -0.014267771504819393,
              0.002694365568459034,
              0.00595651613548398,
              0.005886849481612444,
              0.01949278637766838,
              -0.025762803852558136,
              0.0186428502202034,
              0.002546323463320732,
              0.03112715110182762,
              0.03179595246911049,
              0.015856176614761353,
              -0.019882921129465103,
              0.012393733486533165,
              0.015730775892734528,
              -0.004723412916064262,
              -0.015382441692054272,
              -0.016817579045891762,
              -0.005925165954977274,
              -0.004653745796531439,
              -0.03502849489450455,
              -0.02126232348382473,
              0.051414139568805695,
              0.01818304881453514,
              0.009565259329974651,
              0.034081026911735535,
              -0.018754318356513977,
              -0.005834599025547504,
              0.011369630694389343,
              -0.014894773252308369,
              -0.026779938489198685,
              -0.043054115027189255,
              0.017305247485637665,
              -0.01395427156239748,
              -0.017946181818842888,
              -0.016483178362250328,
              0.0023791231215000153,
              -0.021081190556287766,
              -0.008959157392382622,
              0.008617790415883064,
              -0.0004689450142905116,
              -0.010735662654042244,
              -0.019172318279743195,
              0.03940357267856598,
              -0.0014569080667570233,
              -0.021596726030111313,
              -0.03452689200639725,
              0.04703906178474426,
              0.03842823579907417,
              0.02644553780555725,
              -0.0008155376417562366,
              0.004946346860378981,
              -0.01019922737032175,
              -0.008157989010214806,
              0.022502394393086433,
              -0.0013027702225372195,
              0.001785213127732277,
              -0.012386767193675041,
              0.0019053885480389,
              0.007997754961252213,
              -0.0018931968370452523,
              -0.011327831074595451,
              0.005221530795097351,
              -0.014393172226846218,
              0.012637567706406116,
              -0.005221530795097351,
              0.0005899911629967391,
              -0.03388595953583717,
              -0.039821576327085495,
              0.017946181818842888,
              -0.0014830331783741713,
              -0.004685095977038145,
              -0.016413511708378792,
              -0.02619473822414875,
              -0.01430957205593586,
              -0.007405586540699005,
              0.011327831074595451,
              -0.03265982121229172,
              0.03530716150999069,
              -0.021582791581749916,
              -0.02906501293182373,
              0.00651036761701107,
              0.01950671896338463,
              -0.017542114481329918,
              -0.01779291406273842,
              0.009941460564732552,
              0.009460759349167347,
              -5.8073856052942574e-05,
              0.006141133140772581,
              -0.006984102074056864,
              0.01627417653799057,
              -0.0468439944088459,
              0.0076215541921556,
              -0.014434972777962685,
              -0.0014351372374221683,
              -0.03625463321805,
              -0.01478330697864294,
              -0.007649420760571957,
              0.0015466042095795274,
              -0.019311651587486267,
              0.023463796824216843,
              -0.002380864694714546,
              -0.018670717254281044,
              -0.015159507282078266,
              -0.013264569453895092,
              0.0021109057124704123,
              -0.028786344453692436,
              -0.015591442584991455,
              -0.0022641727700829506,
              -0.025219401344656944,
              -0.009077590890228748,
              -0.009579192847013474,
              -0.17745541036128998,
              0.010847128927707672,
              0.02076072245836258,
              -0.032074619084596634,
              -0.005552448332309723,
              0.02711433917284012,
              0.032269686460494995,
              -0.009850893169641495,
              -0.020482055842876434,
              0.003523401217535138,
              -0.0010632904013618827,
              -0.017012646421790123,
              -0.02856341004371643,
              -0.0007136498461477458,
              0.004350695293396711,
              0.01306950207799673,
              0.0018235299503430724,
              0.02045418880879879,
              0.024898933246731758,
              0.02825687639415264,
              0.03143368661403656,
              -0.044614654034376144,
              0.013536269776523113,
              -0.007321986369788647,
              0.02064925618469715,
              -0.009767292998731136,
              -0.010171361267566681,
              0.03137795254588127,
              -0.017151979729533195,
              -0.014671839773654938,
              0.012128999456763268,
              0.02725367434322834,
              0.012101132422685623,
              -0.004552728962153196,
              0.00025515485322102904,
              0.020886123180389404,
              0.01123029738664627,
              -0.01334816962480545,
              -0.0011651781387627125,
              -0.008415755815804005,
              0.05252880975604057,
              -0.00992752704769373,
              -0.0076145874336361885,
              -0.014121471904218197,
              -0.023644929751753807,
              -0.011097930371761322,
              0.007670320570468903,
              -0.0022746229078620672,
              0.016009442508220673,
              0.018364183604717255,
              -0.01748638041317463,
              -0.020384522154927254,
              0.020635321736335754,
              0.006398900877684355,
              0.007005002349615097,
              0.02679387293756008,
              0.011662231758236885,
              0.00821372214704752,
              -0.026166871190071106,
              0.015535708516836166,
              0.004977697040885687,
              -0.033022090792655945,
              0.026013603433966637,
              0.002319906372576952,
              -0.014978373423218727,
              -0.0016868087695911527,
              0.0018479133723303676,
              0.004333278629928827,
              -0.021986858919262886,
              0.011202430352568626,
              -0.03647756576538086,
              -0.023463796824216843,
              0.0003302644472569227,
              0.0047408295795321465,
              0.022599928081035614,
              -0.00012757742661051452,
              -0.019367385655641556,
              -0.0012609701370820403,
              0.001722512999549508,
              -0.01123029738664627,
              -0.033022090792655945,
              0.0317402184009552,
              -0.0096976263448596,
              0.026975005865097046,
              0.0032081587705761194,
              -0.012087199836969376,
              0.005521098151803017,
              0.018322382122278214,
              -0.019715720787644386,
              -0.026431605219841003,
              0.011432331055402756,
              0.0060226996429264545,
              0.017542114481329918,
              0.0019785386975854635,
              0.007412553299218416,
              0.01779291406273842,
              0.010220128111541271,
              -0.015284908004105091,
              0.022181926295161247,
              -0.012853534892201424,
              -0.02105332352221012,
              -0.017347047105431557,
              0.006381483748555183,
              0.009307491593062878,
              0.012783868238329887,
              0.02533086948096752,
              -0.019618187099695206,
              0.006694984622299671,
              0.029037145897746086,
              0.011481097899377346,
              -0.006043599918484688,
              -0.010833196341991425,
              0.00023664960463065654,
              0.02629227191209793,
              0.017820781096816063,
              0.009133324958384037,
              -0.03193528577685356,
              -0.0027918992564082146,
              0.005256364122033119,
              -0.02599967084825039,
              0.043499983847141266,
              -0.010603295639157295,
              0.003612226340919733,
              0.014978373423218727,
              -0.0007920250645838678,
              -0.01804371550679207,
              -0.11425364017486572,
              -0.008206755854189396,
              -0.002555032027885318,
              0.004354178439825773,
              -0.014490705914795399,
              -0.004120794590562582,
              0.02172212488949299,
              0.029483012855052948,
              -0.026027537882328033,
              0.010624195449054241,
              -0.020579589530825615,
              -0.02711433917284012,
              0.013090401887893677,
              0.004803529474884272,
              -0.005319064483046532,
              0.008185855112969875,
              0.0033701341599226,
              -0.0047582462430000305,
              -0.009251758456230164,
              0.026682404801249504,
              -0.0026473405305296183,
              -0.01959032006561756,
              0.014393172226846218,
              -0.024048998951911926,
              0.015842242166399956,
              0.00677858479321003,
              -0.029705947265028954,
              0.0015788250602781773,
              0.021889325231313705,
              0.011028262786567211,
              0.007398620247840881,
              -0.03455476090312004,
              -0.00881285686045885,
              -0.003410192672163248,
              -0.0019837636500597,
              -0.003326592268422246,
              -0.012909268029034138,
              -0.022558128461241722,
              0.005782349035143852,
              -0.03405315801501274,
              0.0029051078017801046,
              0.013292435556650162,
              -0.004169561434537172,
              -0.010157427750527859,
              -0.010136527009308338,
              -0.00012942795001436025,
              -0.014434972777962685,
              0.030151814222335815,
              0.023770330473780632,
              -0.00803258828818798,
              -0.012198666110634804,
              -0.02045418880879879,
              -0.02499646693468094,
              -0.005367831327021122,
              0.020941857248544693,
              0.033133555203676224,
              0.0186428502202034,
              0.013598970137536526,
              -0.004517895635217428,
              -0.012965002097189426,
              -0.000530774355866015,
              -0.005092646926641464,
              -0.01698477938771248,
              0.005144897382706404,
              0.018628917634487152,
              -0.001567504252307117,
              -0.05473028123378754,
              -0.011167597025632858,
              -0.008673523552715778,
              -0.005350414663553238,
              -0.014156305231153965,
              0.01793224923312664,
              0.013194902800023556,
              0.02155492454767227,
              -0.01858711615204811,
              -0.01012259442359209,
              -0.014281705021858215,
              -0.0021091641392558813,
              0.0022066975943744183,
              -0.018726451322436333,
              -0.01090286299586296,
              -0.0274487417191267,
              0.007698187604546547,
              -0.002624698681756854,
              -0.010220128111541271,
              -0.012804768048226833,
              -0.0012182991486042738,
              -0.02499646693468094,
              -0.011745831929147243,
              -0.029761681333184242,
              0.009662793017923832,
              0.008645656518638134,
              -0.029761681333184242,
              -0.008234621956944466,
              -0.004180011339485645,
              -0.0024644650984555483,
              -0.0031994504388421774,
              -0.01390550471842289,
              0.020217321813106537,
              0.005987866315990686,
              -0.015075907111167908,
              0.0043750787153840065,
              -0.04391798377037048,
              0.016162710264325142,
              0.0032952423207461834,
              -0.01698477938771248,
              0.011655264534056187,
              0.018823985010385513,
              0.03692343458533287,
              -0.017040513455867767,
              0.002717007417231798,
              0.008854657411575317,
              -0.007363786455243826,
              0.021485257893800735,
              -0.01738884672522545,
              -0.018977250903844833,
              -0.006447667255997658,
              -0.010993429459631443,
              0.005925165954977274,
              -0.004507445730268955,
              0.029204346239566803,
              0.005803248845040798,
              -0.01499230694025755,
              -0.016538910567760468,
              0.009572225622832775,
              -0.00025798508431762457,
              -0.007774821016937494,
              0.013828870840370655,
              -0.010234060697257519,
              0.026320137083530426,
              -0.026612738147377968,
              -0.011780665256083012,
              0.020077988505363464,
              -0.02403506450355053,
              -0.0008969607879407704,
              0.030040347948670387,
              -0.0010589362354949117,
              0.016664311289787292,
              0.02423013187944889,
              0.008596889674663544,
              0.03622676432132721,
              0.004639812745153904,
              -0.014755439944565296,
              -0.03600383177399635,
              0.0020307889208197594,
              -0.012540034018456936,
              -0.00260728201828897,
              0.022251592949032784,
              -0.0027222323697060347,
              0.003500759368762374,
              0.015479975380003452,
              -0.006398900877684355,
              0.008778023533523083,
              0.02091399021446705,
              0.019074784591794014,
              -0.029009278863668442,
              -0.015953710302710533,
              -0.00849238969385624,
              -0.0035286261700093746,
              0.02278106100857258,
              0.002276364481076598,
              -0.0034415426198393106,
              0.0367841012775898,
              -0.015048041008412838,
              0.022279459983110428,
              0.003981460817158222,
              0.0076424540020525455,
              -0.008750157430768013,
              0.012428566813468933,
              0.007039835676550865,
              -0.02912074513733387,
              -0.02827080897986889,
              -0.010568462312221527,
              0.009251758456230164,
              -0.011773698963224888,
              0.02565133571624756,
              0.029037145897746086,
              0.005252880975604057,
              -0.0007088602287694812,
              0.0114601980894804,
              -0.005998316220939159,
              0.01478330697864294,
              0.012484300881624222,
              -0.02459239959716797,
              -0.016845446079969406,
              -0.014448905363678932,
              0.024453066289424896,
              0.002779707545414567,
              -0.004451712127774954,
              0.012170800007879734,
              -0.005378281231969595,
              0.02195899188518524,
              0.012184732593595982,
              0.001861846656538546,
              0.009760326705873013,
              0.03555796295404434,
              0.00401629414409399,
              0.01490870676934719,
              0.007349853403866291,
              -0.019269851967692375,
              0.006796001922339201,
              -0.021749991923570633,
              0.006127200089395046,
              0.015828309580683708,
              -0.012463400140404701,
              -0.010094727389514446,
              -0.023157263174653053,
              0.021136924624443054,
              -0.013229736126959324,
              -0.00457362923771143,
              0.003953594248741865,
              0.03770370036363602,
              -0.0011442780960351229,
              0.015298841521143913,
              -0.01219169981777668,
              0.01080532930791378,
              -0.034276094287633896,
              0.02765774168074131,
              -0.007210519630461931,
              -0.036505430936813354,
              -0.026570938527584076,
              0.033022090792655945,
              0.021931126713752747,
              -0.01518737431615591,
              0.044558919966220856,
              -0.012212599627673626,
              0.018559250980615616,
              0.010819262824952602,
              0.024717800319194794,
              -0.011906065978109837,
              0.02638980560004711,
              0.02278106100857258,
              0.017597848549485207,
              -0.008680489845573902,
              0.01989685371518135,
              -0.01874038390815258,
              -0.021136924624443054,
              -0.012310133315622807,
              -0.006607901304960251,
              0.03279915452003479,
              -0.0008817211491987109,
              0.10160214453935623,
              0.021248390898108482,
              -0.0030949499923735857,
              0.012115065939724445,
              -0.015702908858656883,
              0.011871231719851494,
              0.013362103141844273,
              0.018921518698334694,
              -0.011474130675196648,
              -0.0010545820696279407,
              0.006339683663100004,
              -0.015591442584991455,
              -0.013689536601305008,
              -0.02423013187944889,
              0.0035355929285287857,
              -0.015605375170707703,
              0.003765493631362915,
              0.008617790415883064,
              -0.008610823191702366,
              -0.0030200581531971693,
              0.024299798533320427,
              -0.00043737722444348037,
              0.011223330162465572,
              0.01602337695658207,
              -0.013118268921971321,
              -0.009669759310781956,
              0.007018935866653919,
              -0.016455311328172684,
              -0.008673523552715778,
              -0.03112715110182762,
              0.004406428430229425,
              -0.016566777601838112,
              -0.04166077822446823,
              -0.013571103103458881,
              0.02815934270620346,
              0.01097949594259262,
              -0.001757346442900598,
              0.013341202400624752,
              0.014121471904218197,
              5.785614484921098e-05,
              0.009293558076024055,
              0.036449696868658066,
              -0.0029138161335140467,
              -0.00964885950088501,
              -0.013647736981511116,
              -0.007997754961252213,
              0.0014725831570103765,
              -0.013731337152421474,
              -0.02312939614057541
            ],
            "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
            "indexType": "cogsearchvs",
            "question": "How are sentence pairs represented in the input to BERT?"
          },
          "output": {
            "jsonAnswer": null,
            "existingAnswer": false
          },
          "start_time": 1695246510.003197,
          "end_time": 1695246512.094216,
          "error": null,
          "children": null,
          "node_name": "check_cache_answer"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "[2023-09-20T16:48:31+0000] Search index aoaikb already exists\n[2023-09-20T16:48:31+0000] KB Search Count: 1\n[2023-09-20T16:48:32+0000] 0.92985696\n",
        "stderr": ""
      },
      "system_metrics": {
        "duration": 2.105286
      },
      "result": {
        "jsonAnswer": null,
        "existingAnswer": false
      }
    },
    {
      "node": "search_question_from_vectordb",
      "flow_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7",
      "run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_search_question_from_vectordb_0",
      "status": "Completed",
      "inputs": {
        "conn": "entaoai",
        "embeddedQuestion": [
          -0.011118830181658268,
          -0.01631597802042961,
          0.02559560351073742,
          -0.002060397295281291,
          -0.005500198341906071,
          0.031210750341415405,
          0.015424241311848164,
          -0.011599531397223473,
          -0.01773718185722828,
          -0.027727408334612846,
          0.028535543009638786,
          0.04366718605160713,
          -0.035139963030815125,
          0.005427048075944185,
          -0.0009474692051298916,
          0.008422723039984703,
          0.011244230903685093,
          0.009614026173949242,
          0.027797074988484383,
          -0.016956912353634834,
          -0.0016798420110717416,
          -0.006102816667407751,
          -0.016552845016121864,
          -0.0035739096347242594,
          -0.010742628946900368,
          -0.00849238969385624,
          0.029538746923208237,
          -0.020328788086771965,
          -0.003713243408128619,
          -0.009676726534962654,
          0.0021788307931274176,
          0.02357526309788227,
          -0.017918314784765244,
          -0.009948426857590675,
          -0.009258724749088287,
          0.006694984622299671,
          -0.0029294912237674,
          -0.016915112733840942,
          0.02438339963555336,
          -0.023212995380163193,
          0.0071164690889418125,
          0.021694257855415344,
          0.003977977205067873,
          -0.015270974487066269,
          -0.005953032989054918,
          0.01672004535794258,
          0.010652062483131886,
          -0.0065695843659341335,
          -0.012059332802891731,
          0.0093214251101017,
          0.00438204500824213,
          0.015856176614761353,
          -0.03945930674672127,
          -0.020997589454054832,
          -0.02251632697880268,
          -0.0034084508661180735,
          -0.002352998126298189,
          0.032966356724500656,
          -0.0272118728607893,
          -0.013543237000703812,
          0.0005050846957601607,
          -0.008130121976137161,
          0.0009370191837660968,
          0.008053488098084927,
          -0.022404860705137253,
          -0.024466998875141144,
          -0.008548122830688953,
          0.008018654771149158,
          0.019437052309513092,
          0.007398620247840881,
          0.01959032006561756,
          0.010038994252681732,
          0.0009866568725556135,
          -0.001406399649567902,
          0.0428033173084259,
          -0.01388460397720337,
          0.002535873558372259,
          -0.014755439944565296,
          0.001035423600114882,
          0.03592022880911827,
          0.012735101394355297,
          -0.002624698681756854,
          -0.0010606779251247644,
          0.026487339287996292,
          0.015897976234555244,
          -0.012742067687213421,
          0.010436095297336578,
          0.025275135412812233,
          0.002154447603970766,
          0.0040302276611328125,
          0.027379073202610016,
          0.03536289557814598,
          0.012944101355969906,
          0.0067054349929094315,
          -0.0038839271292090416,
          0.012247432954609394,
          -0.01118153054267168,
          0.038818370550870895,
          0.02634800411760807,
          -0.009523458778858185,
          -0.010157427750527859,
          0.008513289503753185,
          -0.04497692361474037,
          -0.005552448332309723,
          -0.024606334045529366,
          0.014852973632514477,
          0.010749596171081066,
          -0.01130693033337593,
          0.003262150567024946,
          0.001316703506745398,
          -0.019074784591794014,
          0.030207548290491104,
          0.006221250165253878,
          -0.024216199293732643,
          -0.0005220660241320729,
          -0.002276364481076598,
          0.015479975380003452,
          -0.039319973438978195,
          -0.009669759310781956,
          0.012602734379470348,
          0.017165912315249443,
          -0.004075510893017054,
          0.010721729137003422,
          -0.034582626074552536,
          0.03288275748491287,
          -0.003422384150326252,
          -0.010819262824952602,
          -0.03380235657095909,
          -0.006520817521959543,
          -0.04288691654801369,
          0.027420874685049057,
          0.022641727700829506,
          0.015939775854349136,
          -0.022753195837140083,
          0.005636048503220081,
          0.01854531653225422,
          -0.02760200761258602,
          -0.02050992287695408,
          -0.01959032006561756,
          -0.004643295891582966,
          0.02257206104695797,
          0.003828193759545684,
          -0.014253838919103146,
          -0.023993264883756638,
          -0.010192261077463627,
          0.018601050600409508,
          0.018782183527946472,
          0.020412389189004898,
          0.0024139564484357834,
          -0.010408228263258934,
          0.026334071531891823,
          -0.001065902877599001,
          -0.029761681333184242,
          0.013807971030473709,
          0.007726054172962904,
          0.0011695323046296835,
          0.00037707184674218297,
          0.009753359481692314,
          -0.009732459671795368,
          -0.005601215176284313,
          -0.020412389189004898,
          0.017012646421790123,
          0.017096245661377907,
          -0.009307491593062878,
          0.024118665605783463,
          0.03349582478404045,
          0.0020900056697428226,
          -0.014866907149553299,
          0.007328953128308058,
          -0.029204346239566803,
          -0.026222603395581245,
          0.018252715468406677,
          -0.01959032006561756,
          -0.003438059240579605,
          0.0040128109976649284,
          0.00180088821798563,
          0.03391382470726967,
          -0.02806180901825428,
          -0.02987314760684967,
          -0.015619308687746525,
          0.0193952526897192,
          0.0065521677024662495,
          0.020119788125157356,
          0.023547397926449776,
          -0.03098781779408455,
          -0.02559560351073742,
          -0.003594809677451849,
          -0.01974358595907688,
          0.01012259442359209,
          -0.034192491322755814,
          0.02382606454193592,
          0.007102536037564278,
          0.000681428937241435,
          -0.01843385025858879,
          -0.6148517727851868,
          -0.03820530325174332,
          -0.009300525300204754,
          -0.028535543009638786,
          0.01698477938771248,
          0.005514131393283606,
          -0.011627398431301117,
          0.03349582478404045,
          -0.024104731157422066,
          0.033579424023628235,
          -0.0008451460162177682,
          -0.023296596482396126,
          -0.0038212270010262728,
          0.007733020931482315,
          -0.010498794727027416,
          -0.012992868199944496,
          0.006503400858491659,
          0.004162594676017761,
          0.00922389142215252,
          0.022335194051265717,
          -0.03500062972307205,
          0.00739165348932147,
          -0.026069337502121925,
          -0.004152144771069288,
          -0.025372669100761414,
          0.0031245583668351173,
          0.006506884470582008,
          -0.021095123142004013,
          -0.009816059842705727,
          0.017151979729533195,
          -0.02045418880879879,
          -0.001589275081641972,
          0.021889325231313705,
          -0.030207548290491104,
          0.025010401383042336,
          0.021471325308084488,
          0.0021439974661916494,
          0.009084558114409447,
          -0.0007767854258418083,
          0.027880676090717316,
          -0.01596764288842678,
          -0.002070847200229764,
          -0.0024035065434873104,
          -0.0031228167936205864,
          0.017360979691147804,
          -0.000540788983926177,
          0.0186428502202034,
          -0.010498794727027416,
          0.006841285154223442,
          -0.004591045901179314,
          -0.008708356879651546,
          -0.003521659644320607,
          -0.007572787348181009,
          -0.002574190264567733,
          0.03550222888588905,
          0.011418397538363934,
          0.0342482253909111,
          -0.0024975568521767855,
          0.0014272996922954917,
          -0.011919998563826084,
          0.008631723001599312,
          0.017305247485637665,
          -0.012303167022764683,
          -0.01918625272810459,
          0.018823985010385513,
          0.009565259329974651,
          -0.006287433672696352,
          0.003291758941486478,
          0.00914725847542286,
          -0.04558999091386795,
          0.0003217737830709666,
          0.015229174867272377,
          0.0004941992810927331,
          -0.015284908004105091,
          0.00554199842736125,
          0.012338000349700451,
          0.0033718757331371307,
          0.011188496835529804,
          -0.002103938953951001,
          0.019757520407438278,
          -0.017444580793380737,
          -0.012247432954609394,
          -0.028842078521847725,
          0.002976516494527459,
          -0.0024052481167018414,
          0.0001037927286233753,
          -0.01738884672522545,
          0.017193779349327087,
          -0.01478330697864294,
          -0.0036505432799458504,
          0.0033718757331371307,
          0.03199101984500885,
          -0.028842078521847725,
          -0.042050912976264954,
          0.004918479826301336,
          0.011376597918570042,
          0.013125235214829445,
          -0.0074613201431930065,
          0.03112715110182762,
          -0.03722996637225151,
          0.014246871694922447,
          -0.020565655082464218,
          0.012351933866739273,
          -0.005886849481612444,
          0.02675207331776619,
          0.03600383177399635,
          -0.007475253660231829,
          0.0011259905295446515,
          -0.025707069784402847,
          0.0035303679760545492,
          -0.005911232437938452,
          -0.010401261039078236,
          -0.0035390763077884912,
          -0.0003302644472569227,
          -0.014212038367986679,
          -0.04040677472949028,
          0.011843365617096424,
          0.01065902877599001,
          0.01631597802042961,
          -0.003516434459015727,
          0.02599967084825039,
          0.01843385025858879,
          0.015521774999797344,
          -0.02886994555592537,
          0.01868464983999729,
          0.012045399285852909,
          0.0024522733874619007,
          0.006210800260305405,
          0.020844323560595512,
          0.02886994555592537,
          -0.011495031416416168,
          0.02172212488949299,
          0.010213160887360573,
          -0.010213160887360573,
          -0.002133547328412533,
          0.01571684330701828,
          0.0012836117530241609,
          -0.007726054172962904,
          0.01097949594259262,
          -0.0035094679333269596,
          -0.02130412496626377,
          -0.008144055493175983,
          -0.00489758001640439,
          -0.012553967535495758,
          -0.028591277077794075,
          -0.015647174790501595,
          -0.021931126713752747,
          -0.0011024780105799437,
          0.0001773239200701937,
          -0.02266959473490715,
          0.011599531397223473,
          -0.018475649878382683,
          -0.02528906799852848,
          -0.017347047105431557,
          -0.023394130170345306,
          -0.014616106636822224,
          -0.011655264534056187,
          -0.032213952392339706,
          -0.008617790415883064,
          -0.029092878103256226,
          0.006754201836884022,
          0.02534480206668377,
          -0.027588075026869774,
          -0.017765048891305923,
          -0.009676726534962654,
          -0.013877637684345245,
          0.0418279804289341,
          0.030792750418186188,
          0.0045875622890889645,
          -0.016190577298402786,
          0.013034668751060963,
          -0.03792663663625717,
          0.0009213441517204046,
          0.02201472595334053,
          0.0011190238874405622,
          0.01848958246409893,
          -0.02605540305376053,
          0.009906627237796783,
          -0.00020399325876496732,
          -0.032576221972703934,
          -0.009857860393822193,
          0.040573976933956146,
          -0.002218889305368066,
          0.006458117626607418,
          0.012233499437570572,
          0.0017199005233123899,
          0.003911793697625399,
          -0.0036261598579585552,
          -0.021025456488132477,
          0.015842242166399956,
          0.011237263679504395,
          0.019534585997462273,
          -0.017904382199048996,
          0.01627417653799057,
          -0.004322828259319067,
          0.02237699367105961,
          0.007948988117277622,
          0.027615942060947418,
          -0.014532506465911865,
          0.018754318356513977,
          0.025163667276501656,
          0.009850893169641495,
          -0.0022711395286023617,
          -0.0011756281601265073,
          -0.006064499728381634,
          -0.00982302613556385,
          -0.0011277322191745043,
          -0.03553009778261185,
          0.023338396102190018,
          0.0015709876315668225,
          0.02220979332923889,
          -0.02906501293182373,
          -0.033217158168554306,
          -0.01804371550679207,
          -0.008206755854189396,
          0.0012374575017020106,
          -0.009084558114409447,
          0.03265982121229172,
          -0.0279642753303051,
          0.00955132581293583,
          -0.006064499728381634,
          -0.012909268029034138,
          0.038706906139850616,
          0.012254400178790092,
          0.006893535144627094,
          0.006043599918484688,
          -0.0029660663567483425,
          -0.00960009265691042,
          0.015521774999797344,
          -0.020342722535133362,
          0.019228052347898483,
          0.02076072245836258,
          0.029956746846437454,
          -0.0038665104657411575,
          -0.015424241311848164,
          -0.010533628053963184,
          0.02630620449781418,
          0.009356258437037468,
          0.03279915452003479,
          0.003523401217535138,
          -0.012156866490840912,
          0.0030531499069184065,
          -0.004110344685614109,
          -0.0040406775660812855,
          0.053977880626916885,
          -0.01005292683839798,
          0.01995258778333664,
          0.030820617452263832,
          -0.01141143124550581,
          -0.0074404203332960606,
          0.017263446003198624,
          -0.011947865597903728,
          -0.017360979691147804,
          0.007551887072622776,
          0.017207713797688484,
          0.0009274400072172284,
          0.024801399558782578,
          0.0003629207785706967,
          -0.011195464059710503,
          0.010011127218604088,
          0.004267094656825066,
          -0.01249823346734047,
          0.02024518884718418,
          0.01787651516497135,
          0.023561330512166023,
          -0.014643972739577293,
          -0.030151814222335815,
          -0.028298676013946533,
          -0.012017532251775265,
          0.00023577875981573015,
          -0.017514247447252274,
          -0.004256644751876593,
          -0.010247994214296341,
          0.006573067978024483,
          0.003876960603520274,
          0.004364628344774246,
          -0.02363099716603756,
          0.01733311451971531,
          0.005022980272769928,
          0.02181965857744217,
          -0.006931852083653212,
          -0.020886123180389404,
          0.028131475672125816,
          -0.0040580942295491695,
          -0.011146697215735912,
          -0.015438174828886986,
          -0.019910788163542747,
          -0.010617229156196117,
          -0.028075743466615677,
          0.017597848549485207,
          0.008882524445652962,
          0.0077887545339763165,
          -0.020273054018616676,
          -0.003695826744660735,
          -0.002946907887235284,
          0.0015431208303198218,
          0.008861623704433441,
          -0.01118153054267168,
          0.013076468370854855,
          0.01125816348940134,
          -0.012045399285852909,
          0.005364347714930773,
          -0.016761844977736473,
          0.014448905363678932,
          0.049909334629774094,
          -0.006653184536844492,
          -0.02172212488949299,
          -0.01985505409538746,
          -0.018768250942230225,
          -0.018670717254281044,
          0.02332446351647377,
          0.010387328453361988,
          -0.019116584211587906,
          0.01153683103621006,
          0.024759599938988686,
          0.02372853085398674,
          -0.009425925090909004,
          0.0016868087695911527,
          0.011919998563826084,
          -0.0067054349929094315,
          -0.017402781173586845,
          -0.03753650188446045,
          -0.029455145820975304,
          0.0005564640159718692,
          0.025804603472352028,
          0.06325750797986984,
          0.0035094679333269596,
          0.0036993101239204407,
          -0.019464919343590736,
          0.0037480767350643873,
          -0.018015848472714424,
          -0.01592584326863289,
          0.0011303446954116225,
          -0.015633242204785347,
          -0.021485257893800735,
          -0.00836002267897129,
          0.0035652013029903173,
          0.0056499820202589035,
          0.02126232348382473,
          0.0009013149538077414,
          0.0013184451963752508,
          -0.05768415704369545,
          0.02559560351073742,
          -0.000957048439886421,
          0.003974494058638811,
          -0.014337439090013504,
          0.020816456526517868,
          0.03461049497127533,
          0.005099613685160875,
          -0.01752818003296852,
          0.01899118535220623,
          0.02509400062263012,
          0.018712516874074936,
          -0.04010024294257164,
          0.0060157328844070435,
          0.006917918566614389,
          -0.008798924274742603,
          0.00041408237302675843,
          -0.013139168731868267,
          0.022599928081035614,
          0.01708231307566166,
          0.006802968215197325,
          0.03380235657095909,
          0.0037202101666480303,
          -0.003915277309715748,
          0.012066299095749855,
          0.008645656518638134,
          -0.006245633587241173,
          -0.0037027932703495026,
          0.00836002267897129,
          -0.0035808763932436705,
          0.018503516912460327,
          0.02282286249101162,
          0.0013524078531190753,
          0.007238386198878288,
          -0.014379238709807396,
          -0.01783471554517746,
          -0.004451712127774954,
          -0.0074891867116093636,
          -0.007384686730802059,
          -0.011982698924839497,
          -0.009405025281012058,
          -0.025261200964450836,
          -0.023254796862602234,
          -0.003335300600156188,
          -0.00876409001648426,
          0.018475649878382683,
          -0.01139053050428629,
          0.009614026173949242,
          -0.005716165527701378,
          -0.003869993845000863,
          -0.017165912315249443,
          -0.013362103141844273,
          -0.0016040793852880597,
          -0.02634800411760807,
          -0.006130683235824108,
          -0.04840452969074249,
          -0.020328788086771965,
          0.023686731234192848,
          0.024397332221269608,
          0.029148612171411514,
          -0.013313336297869682,
          0.005172763951122761,
          -0.00545143149793148,
          -0.010059894062578678,
          -0.008018654771149158,
          0.016538910567760468,
          -0.005611665081232786,
          -0.016455311328172684,
          0.02609720453619957,
          -0.034136757254600525,
          -0.026111137121915817,
          -0.01950671896338463,
          0.02161065861582756,
          -0.011550764553248882,
          0.01779291406273842,
          -0.01468577329069376,
          -0.022237660363316536,
          -0.0067054349929094315,
          0.029287945479154587,
          -0.004845329560339451,
          0.02746267430484295,
          -0.0020969724282622337,
          -0.02605540305376053,
          0.020872190594673157,
          -0.03722996637225151,
          0.0004397720331326127,
          -0.03472195938229561,
          -0.012888368219137192,
          -0.003988427575677633,
          -0.0021770892199128866,
          0.012331033125519753,
          -0.014072705060243607,
          -0.003828193759545684,
          -0.0120941661298275,
          -0.01315310224890709,
          -0.01251913420855999,
          -0.0035025011748075485,
          0.046119458973407745,
          0.013055568560957909,
          0.004500478971749544,
          -0.014797240495681763,
          0.009202991612255573,
          -0.020523855462670326,
          -0.006771618500351906,
          -0.015340641140937805,
          0.02553986944258213,
          0.00311759184114635,
          -0.014866907149553299,
          0.03867903724312782,
          -0.01282566785812378,
          -0.011926965788006783,
          -0.015438174828886986,
          0.035335030406713486,
          -0.0234359297901392,
          0.045617856085300446,
          0.0035460428334772587,
          0.0027309407014399767,
          -0.017500314861536026,
          0.008276422508060932,
          -0.005280747544020414,
          -0.010895895771682262,
          -0.03148941695690155,
          0.018614983186125755,
          0.026361938565969467,
          -0.012303167022764683,
          -0.003533851122483611,
          -0.022530261427164078,
          -0.01612091064453125,
          -0.020091921091079712,
          -0.024271931499242783,
          0.04508838802576065,
          -0.0038978606462478638,
          0.003025283105671406,
          0.007168719545006752,
          0.009363225661218166,
          -0.0028371827211230993,
          -0.0002782320079859346,
          -0.037257835268974304,
          -0.020426321774721146,
          0.0015509583754464984,
          0.0397937074303627,
          0.0193952526897192,
          0.024620266631245613,
          0.04701119288802147,
          -0.011947865597903728,
          0.018127316609025,
          0.020217321813106537,
          -0.0008625627378933132,
          -0.0031837753485888243,
          -0.030346881598234177,
          -0.008234621956944466,
          -0.015995509922504425,
          0.014191138558089733,
          0.007067702244967222,
          -0.0076424540020525455,
          0.006956235505640507,
          0.01985505409538746,
          0.015006240457296371,
          0.011077029630541801,
          -0.003838643664494157,
          -0.0021370307076722383,
          0.0010876738233491778,
          -0.019172318279743195,
          -0.01256790105253458,
          0.01869858428835869,
          -0.018308449536561966,
          0.00748222041875124,
          -0.03380235657095909,
          -0.004695545881986618,
          0.0031559085473418236,
          -0.018865784630179405,
          0.011940899305045605,
          -0.00720355287194252,
          0.019074784591794014,
          -0.019269851967692375,
          0.010282827541232109,
          -0.016998711973428726,
          0.013821904547512531,
          -0.013229736126959324,
          -0.010888929478824139,
          -0.023463796824216843,
          -0.001105090486817062,
          0.01677577942609787,
          0.019966520369052887,
          -0.011620431207120419,
          0.0018583633936941624,
          -0.030514081940054893,
          0.015326708555221558,
          -0.008137088268995285,
          -0.029148612171411514,
          -0.003768976777791977,
          0.001748638111166656,
          0.007879321463406086,
          -0.016817579045891762,
          -0.04946346580982208,
          -0.034638360142707825,
          -0.013362103141844273,
          -0.009634925983846188,
          -0.001558795920573175,
          -0.016399577260017395,
          0.015298841521143913,
          -0.011042196303606033,
          -0.04135424643754959,
          -0.004496995359659195,
          -0.0026560488622635603,
          0.03023541532456875,
          0.022335194051265717,
          0.005200630519539118,
          0.012985901907086372,
          0.030904216691851616,
          0.013389969244599342,
          -0.011007362976670265,
          0.00028258617385290563,
          -0.019019052386283875,
          0.009920559823513031,
          0.011188496835529804,
          -0.023867864161729813,
          -0.0428311824798584,
          0.0299846138805151,
          0.027393007650971413,
          -0.017667515203356743,
          -0.029343679547309875,
          0.015856176614761353,
          -0.018782183527946472,
          -0.001386370393447578,
          0.00997629389166832,
          -0.011425363831222057,
          -0.021122990176081657,
          -0.009035791270434856,
          -0.012170800007879734,
          0.014295638538897038,
          0.01468577329069376,
          -0.005298164207488298,
          -0.017848648130893707,
          0.013013768941164017,
          -0.020119788125157356,
          0.005218047648668289,
          0.010881963185966015,
          -0.010847128927707672,
          0.024007197469472885,
          0.01471363939344883,
          0.022711394354701042,
          0.014267771504819393,
          0.03304995596408844,
          0.026334071531891823,
          -0.030709149315953255,
          0.018378116190433502,
          -0.013452669605612755,
          -0.003040958195924759,
          -0.007224452681839466,
          0.005360864568501711,
          0.004469128791242838,
          0.0304862167686224,
          -0.022711394354701042,
          -0.00927265826612711,
          0.004099894315004349,
          -0.009767292998731136,
          0.017653580754995346,
          0.008729256689548492,
          -0.00025994444149546325,
          0.004911513067781925,
          -0.011522897519171238,
          0.007928088307380676,
          0.034638360142707825,
          0.006123716477304697,
          -0.02176392637193203,
          -0.0007519666105508804,
          -0.0016493628500029445,
          0.0037445933558046818,
          -0.02690533921122551,
          0.009453792124986649,
          0.029510879889130592,
          -0.0418279804289341,
          -0.00914029125124216,
          0.027518408372998238,
          0.011334797367453575,
          0.011049163527786732,
          -0.029148612171411514,
          0.011090963147580624,
          0.013167035765945911,
          0.021025456488132477,
          -0.013410869985818863,
          -0.010206194594502449,
          -0.007503120228648186,
          0.015326708555221558,
          -0.03006821498274803,
          -0.017165912315249443,
          -0.019924720749258995,
          -0.011857299134135246,
          0.0158701092004776,
          0.021039390936493874,
          0.007565820589661598,
          -0.007872354239225388,
          0.010540595278143883,
          -0.029343679547309875,
          0.010241027921438217,
          0.009753359481692314,
          -0.004260128363966942,
          -0.00318725872784853,
          0.014769373461604118,
          -0.0038665104657411575,
          0.010450027883052826,
          0.0176257137209177,
          0.020133720710873604,
          -0.0065626176074147224,
          0.01000415999442339,
          -0.030597683042287827,
          -0.015897976234555244,
          -0.007955954410135746,
          0.004124277736991644,
          -0.00038055519689805806,
          -0.02800607495009899,
          0.006447667255997658,
          0.004775662906467915,
          0.00568133220076561,
          -0.0022397893480956554,
          -0.015354574657976627,
          -0.005329514387995005,
          0.002267656149342656,
          -0.011676165275275707,
          0.00816495530307293,
          0.016566777601838112,
          -0.023310529068112373,
          0.037007033824920654,
          0.0067054349929094315,
          0.04951919987797737,
          0.0018670717254281044,
          -0.01606517657637596,
          -0.03491702675819397,
          -0.026125071570277214,
          -0.017556047067046165,
          -0.026877472177147865,
          -0.03304995596408844,
          -0.025484135374426842,
          0.02790854312479496,
          0.019562453031539917,
          -0.027128273621201515,
          -0.03260408714413643,
          -0.018196983262896538,
          -0.019659986719489098,
          0.00997629389166832,
          0.01646924391388893,
          0.01779291406273842,
          0.010575428605079651,
          0.01617664471268654,
          -0.02841014415025711,
          0.03714636713266373,
          0.010087761096656322,
          -0.0006901372689753771,
          -0.013550203293561935,
          0.012581833638250828,
          0.011397497728466988,
          -0.01612091064453125,
          0.004803529474884272,
          0.02473173290491104,
          -0.04751279577612877,
          -0.0007872354472056031,
          0.03784303739666939,
          -0.006451150868088007,
          0.02403506450355053,
          0.03918064013123512,
          -0.019715720787644386,
          -0.03480556234717369,
          -0.01430957205593586,
          0.018322382122278214,
          0.033022090792655945,
          0.01692904531955719,
          0.003636609762907028,
          -0.0019855054561048746,
          -0.01342480257153511,
          0.0052772643975913525,
          0.013717403635382652,
          -0.026724206283688545,
          0.0027675158344209194,
          0.0054792980663478374,
          0.0037724601570516825,
          -0.02397933229804039,
          -0.017221646383404732,
          0.011676165275275707,
          0.012059332802891731,
          -0.014978373423218727,
          0.020523855462670326,
          -0.004420361947268248,
          -0.02967808023095131,
          0.0049393801018595695,
          0.02679387293756008,
          0.0015971126267686486,
          -0.020900055766105652,
          0.0110839968547225,
          -0.027936408296227455,
          0.0032029335852712393,
          0.02161065861582756,
          0.012958034873008728,
          -0.0052842311561107635,
          -0.002953874645754695,
          0.005691782105714083,
          -0.02573493681848049,
          -0.00224153115414083,
          -0.007691220846027136,
          0.0058694323524832726,
          -0.018461717292666435,
          0.04625879228115082,
          0.02917647920548916,
          -0.05105186998844147,
          0.017458513379096985,
          -0.013780103996396065,
          -0.008833757601678371,
          -0.019032984972000122,
          0.0032691170927137136,
          0.005033430177718401,
          -0.021136924624443054,
          0.02101152390241623,
          0.0037480767350643873,
          -0.03505636006593704,
          -0.0019611220341175795,
          -0.0012052366510033607,
          0.008680489845573902,
          -0.01047789491713047,
          0.02574886940419674,
          0.2039845585823059,
          -0.03405315801501274,
          0.01833631657063961,
          0.0029573580250144005,
          0.013780103996396065,
          0.0176257137209177,
          0.0037550434935837984,
          0.017709314823150635,
          -0.00217186426743865,
          -0.001844429993070662,
          0.00048462007544003427,
          0.008060455322265625,
          -0.013222768902778625,
          0.0024365982972085476,
          0.0224187932908535,
          0.0010824487544596195,
          -0.012066299095749855,
          -0.046175193041563034,
          -0.00687611848115921,
          -0.0056778485886752605,
          -0.005225013941526413,
          0.014699706807732582,
          -0.003213383723050356,
          -0.036505430936813354,
          0.014351372607052326,
          -0.012916235253214836,
          -0.006691501475870609,
          -0.026919273659586906,
          0.006419800687581301,
          0.03472195938229561,
          -0.007377719972282648,
          0.01858711615204811,
          -0.008241589181125164,
          0.015479975380003452,
          -0.009690659120678902,
          -0.020384522154927254,
          -0.02272532880306244,
          0.02222372591495514,
          0.011056129820644855,
          -0.009425925090909004,
          0.035084228962659836,
          0.010401261039078236,
          -0.008025621995329857,
          -0.012331033125519753,
          -0.00031154145835898817,
          -0.00023947982117533684,
          0.000322644627885893,
          -0.013814937323331833,
          -0.002095230622217059,
          0.03023541532456875,
          -0.010512728244066238,
          0.02654307149350643,
          0.039124906063079834,
          0.042691849172115326,
          0.0030461831483989954,
          -0.014267771504819393,
          0.002694365568459034,
          0.00595651613548398,
          0.005886849481612444,
          0.01949278637766838,
          -0.025762803852558136,
          0.0186428502202034,
          0.002546323463320732,
          0.03112715110182762,
          0.03179595246911049,
          0.015856176614761353,
          -0.019882921129465103,
          0.012393733486533165,
          0.015730775892734528,
          -0.004723412916064262,
          -0.015382441692054272,
          -0.016817579045891762,
          -0.005925165954977274,
          -0.004653745796531439,
          -0.03502849489450455,
          -0.02126232348382473,
          0.051414139568805695,
          0.01818304881453514,
          0.009565259329974651,
          0.034081026911735535,
          -0.018754318356513977,
          -0.005834599025547504,
          0.011369630694389343,
          -0.014894773252308369,
          -0.026779938489198685,
          -0.043054115027189255,
          0.017305247485637665,
          -0.01395427156239748,
          -0.017946181818842888,
          -0.016483178362250328,
          0.0023791231215000153,
          -0.021081190556287766,
          -0.008959157392382622,
          0.008617790415883064,
          -0.0004689450142905116,
          -0.010735662654042244,
          -0.019172318279743195,
          0.03940357267856598,
          -0.0014569080667570233,
          -0.021596726030111313,
          -0.03452689200639725,
          0.04703906178474426,
          0.03842823579907417,
          0.02644553780555725,
          -0.0008155376417562366,
          0.004946346860378981,
          -0.01019922737032175,
          -0.008157989010214806,
          0.022502394393086433,
          -0.0013027702225372195,
          0.001785213127732277,
          -0.012386767193675041,
          0.0019053885480389,
          0.007997754961252213,
          -0.0018931968370452523,
          -0.011327831074595451,
          0.005221530795097351,
          -0.014393172226846218,
          0.012637567706406116,
          -0.005221530795097351,
          0.0005899911629967391,
          -0.03388595953583717,
          -0.039821576327085495,
          0.017946181818842888,
          -0.0014830331783741713,
          -0.004685095977038145,
          -0.016413511708378792,
          -0.02619473822414875,
          -0.01430957205593586,
          -0.007405586540699005,
          0.011327831074595451,
          -0.03265982121229172,
          0.03530716150999069,
          -0.021582791581749916,
          -0.02906501293182373,
          0.00651036761701107,
          0.01950671896338463,
          -0.017542114481329918,
          -0.01779291406273842,
          0.009941460564732552,
          0.009460759349167347,
          -5.8073856052942574e-05,
          0.006141133140772581,
          -0.006984102074056864,
          0.01627417653799057,
          -0.0468439944088459,
          0.0076215541921556,
          -0.014434972777962685,
          -0.0014351372374221683,
          -0.03625463321805,
          -0.01478330697864294,
          -0.007649420760571957,
          0.0015466042095795274,
          -0.019311651587486267,
          0.023463796824216843,
          -0.002380864694714546,
          -0.018670717254281044,
          -0.015159507282078266,
          -0.013264569453895092,
          0.0021109057124704123,
          -0.028786344453692436,
          -0.015591442584991455,
          -0.0022641727700829506,
          -0.025219401344656944,
          -0.009077590890228748,
          -0.009579192847013474,
          -0.17745541036128998,
          0.010847128927707672,
          0.02076072245836258,
          -0.032074619084596634,
          -0.005552448332309723,
          0.02711433917284012,
          0.032269686460494995,
          -0.009850893169641495,
          -0.020482055842876434,
          0.003523401217535138,
          -0.0010632904013618827,
          -0.017012646421790123,
          -0.02856341004371643,
          -0.0007136498461477458,
          0.004350695293396711,
          0.01306950207799673,
          0.0018235299503430724,
          0.02045418880879879,
          0.024898933246731758,
          0.02825687639415264,
          0.03143368661403656,
          -0.044614654034376144,
          0.013536269776523113,
          -0.007321986369788647,
          0.02064925618469715,
          -0.009767292998731136,
          -0.010171361267566681,
          0.03137795254588127,
          -0.017151979729533195,
          -0.014671839773654938,
          0.012128999456763268,
          0.02725367434322834,
          0.012101132422685623,
          -0.004552728962153196,
          0.00025515485322102904,
          0.020886123180389404,
          0.01123029738664627,
          -0.01334816962480545,
          -0.0011651781387627125,
          -0.008415755815804005,
          0.05252880975604057,
          -0.00992752704769373,
          -0.0076145874336361885,
          -0.014121471904218197,
          -0.023644929751753807,
          -0.011097930371761322,
          0.007670320570468903,
          -0.0022746229078620672,
          0.016009442508220673,
          0.018364183604717255,
          -0.01748638041317463,
          -0.020384522154927254,
          0.020635321736335754,
          0.006398900877684355,
          0.007005002349615097,
          0.02679387293756008,
          0.011662231758236885,
          0.00821372214704752,
          -0.026166871190071106,
          0.015535708516836166,
          0.004977697040885687,
          -0.033022090792655945,
          0.026013603433966637,
          0.002319906372576952,
          -0.014978373423218727,
          -0.0016868087695911527,
          0.0018479133723303676,
          0.004333278629928827,
          -0.021986858919262886,
          0.011202430352568626,
          -0.03647756576538086,
          -0.023463796824216843,
          0.0003302644472569227,
          0.0047408295795321465,
          0.022599928081035614,
          -0.00012757742661051452,
          -0.019367385655641556,
          -0.0012609701370820403,
          0.001722512999549508,
          -0.01123029738664627,
          -0.033022090792655945,
          0.0317402184009552,
          -0.0096976263448596,
          0.026975005865097046,
          0.0032081587705761194,
          -0.012087199836969376,
          0.005521098151803017,
          0.018322382122278214,
          -0.019715720787644386,
          -0.026431605219841003,
          0.011432331055402756,
          0.0060226996429264545,
          0.017542114481329918,
          0.0019785386975854635,
          0.007412553299218416,
          0.01779291406273842,
          0.010220128111541271,
          -0.015284908004105091,
          0.022181926295161247,
          -0.012853534892201424,
          -0.02105332352221012,
          -0.017347047105431557,
          0.006381483748555183,
          0.009307491593062878,
          0.012783868238329887,
          0.02533086948096752,
          -0.019618187099695206,
          0.006694984622299671,
          0.029037145897746086,
          0.011481097899377346,
          -0.006043599918484688,
          -0.010833196341991425,
          0.00023664960463065654,
          0.02629227191209793,
          0.017820781096816063,
          0.009133324958384037,
          -0.03193528577685356,
          -0.0027918992564082146,
          0.005256364122033119,
          -0.02599967084825039,
          0.043499983847141266,
          -0.010603295639157295,
          0.003612226340919733,
          0.014978373423218727,
          -0.0007920250645838678,
          -0.01804371550679207,
          -0.11425364017486572,
          -0.008206755854189396,
          -0.002555032027885318,
          0.004354178439825773,
          -0.014490705914795399,
          -0.004120794590562582,
          0.02172212488949299,
          0.029483012855052948,
          -0.026027537882328033,
          0.010624195449054241,
          -0.020579589530825615,
          -0.02711433917284012,
          0.013090401887893677,
          0.004803529474884272,
          -0.005319064483046532,
          0.008185855112969875,
          0.0033701341599226,
          -0.0047582462430000305,
          -0.009251758456230164,
          0.026682404801249504,
          -0.0026473405305296183,
          -0.01959032006561756,
          0.014393172226846218,
          -0.024048998951911926,
          0.015842242166399956,
          0.00677858479321003,
          -0.029705947265028954,
          0.0015788250602781773,
          0.021889325231313705,
          0.011028262786567211,
          0.007398620247840881,
          -0.03455476090312004,
          -0.00881285686045885,
          -0.003410192672163248,
          -0.0019837636500597,
          -0.003326592268422246,
          -0.012909268029034138,
          -0.022558128461241722,
          0.005782349035143852,
          -0.03405315801501274,
          0.0029051078017801046,
          0.013292435556650162,
          -0.004169561434537172,
          -0.010157427750527859,
          -0.010136527009308338,
          -0.00012942795001436025,
          -0.014434972777962685,
          0.030151814222335815,
          0.023770330473780632,
          -0.00803258828818798,
          -0.012198666110634804,
          -0.02045418880879879,
          -0.02499646693468094,
          -0.005367831327021122,
          0.020941857248544693,
          0.033133555203676224,
          0.0186428502202034,
          0.013598970137536526,
          -0.004517895635217428,
          -0.012965002097189426,
          -0.000530774355866015,
          -0.005092646926641464,
          -0.01698477938771248,
          0.005144897382706404,
          0.018628917634487152,
          -0.001567504252307117,
          -0.05473028123378754,
          -0.011167597025632858,
          -0.008673523552715778,
          -0.005350414663553238,
          -0.014156305231153965,
          0.01793224923312664,
          0.013194902800023556,
          0.02155492454767227,
          -0.01858711615204811,
          -0.01012259442359209,
          -0.014281705021858215,
          -0.0021091641392558813,
          0.0022066975943744183,
          -0.018726451322436333,
          -0.01090286299586296,
          -0.0274487417191267,
          0.007698187604546547,
          -0.002624698681756854,
          -0.010220128111541271,
          -0.012804768048226833,
          -0.0012182991486042738,
          -0.02499646693468094,
          -0.011745831929147243,
          -0.029761681333184242,
          0.009662793017923832,
          0.008645656518638134,
          -0.029761681333184242,
          -0.008234621956944466,
          -0.004180011339485645,
          -0.0024644650984555483,
          -0.0031994504388421774,
          -0.01390550471842289,
          0.020217321813106537,
          0.005987866315990686,
          -0.015075907111167908,
          0.0043750787153840065,
          -0.04391798377037048,
          0.016162710264325142,
          0.0032952423207461834,
          -0.01698477938771248,
          0.011655264534056187,
          0.018823985010385513,
          0.03692343458533287,
          -0.017040513455867767,
          0.002717007417231798,
          0.008854657411575317,
          -0.007363786455243826,
          0.021485257893800735,
          -0.01738884672522545,
          -0.018977250903844833,
          -0.006447667255997658,
          -0.010993429459631443,
          0.005925165954977274,
          -0.004507445730268955,
          0.029204346239566803,
          0.005803248845040798,
          -0.01499230694025755,
          -0.016538910567760468,
          0.009572225622832775,
          -0.00025798508431762457,
          -0.007774821016937494,
          0.013828870840370655,
          -0.010234060697257519,
          0.026320137083530426,
          -0.026612738147377968,
          -0.011780665256083012,
          0.020077988505363464,
          -0.02403506450355053,
          -0.0008969607879407704,
          0.030040347948670387,
          -0.0010589362354949117,
          0.016664311289787292,
          0.02423013187944889,
          0.008596889674663544,
          0.03622676432132721,
          0.004639812745153904,
          -0.014755439944565296,
          -0.03600383177399635,
          0.0020307889208197594,
          -0.012540034018456936,
          -0.00260728201828897,
          0.022251592949032784,
          -0.0027222323697060347,
          0.003500759368762374,
          0.015479975380003452,
          -0.006398900877684355,
          0.008778023533523083,
          0.02091399021446705,
          0.019074784591794014,
          -0.029009278863668442,
          -0.015953710302710533,
          -0.00849238969385624,
          -0.0035286261700093746,
          0.02278106100857258,
          0.002276364481076598,
          -0.0034415426198393106,
          0.0367841012775898,
          -0.015048041008412838,
          0.022279459983110428,
          0.003981460817158222,
          0.0076424540020525455,
          -0.008750157430768013,
          0.012428566813468933,
          0.007039835676550865,
          -0.02912074513733387,
          -0.02827080897986889,
          -0.010568462312221527,
          0.009251758456230164,
          -0.011773698963224888,
          0.02565133571624756,
          0.029037145897746086,
          0.005252880975604057,
          -0.0007088602287694812,
          0.0114601980894804,
          -0.005998316220939159,
          0.01478330697864294,
          0.012484300881624222,
          -0.02459239959716797,
          -0.016845446079969406,
          -0.014448905363678932,
          0.024453066289424896,
          0.002779707545414567,
          -0.004451712127774954,
          0.012170800007879734,
          -0.005378281231969595,
          0.02195899188518524,
          0.012184732593595982,
          0.001861846656538546,
          0.009760326705873013,
          0.03555796295404434,
          0.00401629414409399,
          0.01490870676934719,
          0.007349853403866291,
          -0.019269851967692375,
          0.006796001922339201,
          -0.021749991923570633,
          0.006127200089395046,
          0.015828309580683708,
          -0.012463400140404701,
          -0.010094727389514446,
          -0.023157263174653053,
          0.021136924624443054,
          -0.013229736126959324,
          -0.00457362923771143,
          0.003953594248741865,
          0.03770370036363602,
          -0.0011442780960351229,
          0.015298841521143913,
          -0.01219169981777668,
          0.01080532930791378,
          -0.034276094287633896,
          0.02765774168074131,
          -0.007210519630461931,
          -0.036505430936813354,
          -0.026570938527584076,
          0.033022090792655945,
          0.021931126713752747,
          -0.01518737431615591,
          0.044558919966220856,
          -0.012212599627673626,
          0.018559250980615616,
          0.010819262824952602,
          0.024717800319194794,
          -0.011906065978109837,
          0.02638980560004711,
          0.02278106100857258,
          0.017597848549485207,
          -0.008680489845573902,
          0.01989685371518135,
          -0.01874038390815258,
          -0.021136924624443054,
          -0.012310133315622807,
          -0.006607901304960251,
          0.03279915452003479,
          -0.0008817211491987109,
          0.10160214453935623,
          0.021248390898108482,
          -0.0030949499923735857,
          0.012115065939724445,
          -0.015702908858656883,
          0.011871231719851494,
          0.013362103141844273,
          0.018921518698334694,
          -0.011474130675196648,
          -0.0010545820696279407,
          0.006339683663100004,
          -0.015591442584991455,
          -0.013689536601305008,
          -0.02423013187944889,
          0.0035355929285287857,
          -0.015605375170707703,
          0.003765493631362915,
          0.008617790415883064,
          -0.008610823191702366,
          -0.0030200581531971693,
          0.024299798533320427,
          -0.00043737722444348037,
          0.011223330162465572,
          0.01602337695658207,
          -0.013118268921971321,
          -0.009669759310781956,
          0.007018935866653919,
          -0.016455311328172684,
          -0.008673523552715778,
          -0.03112715110182762,
          0.004406428430229425,
          -0.016566777601838112,
          -0.04166077822446823,
          -0.013571103103458881,
          0.02815934270620346,
          0.01097949594259262,
          -0.001757346442900598,
          0.013341202400624752,
          0.014121471904218197,
          5.785614484921098e-05,
          0.009293558076024055,
          0.036449696868658066,
          -0.0029138161335140467,
          -0.00964885950088501,
          -0.013647736981511116,
          -0.007997754961252213,
          0.0014725831570103765,
          -0.013731337152421474,
          -0.02312939614057541
        ],
        "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
        "indexType": "cogsearchvs",
        "overrides": {
          "chainType": "stuff",
          "deploymentType": "gpt3516k",
          "embeddingModelType": "azureopenai",
          "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
          "searchType": "hybridrerank",
          "semantic_captions": false,
          "semantic_ranker": true,
          "temperature": 0,
          "tokenLength": 1000,
          "top": 3
        },
        "question": "How are sentence pairs represented in the input to BERT?"
      },
      "output": "[Document(page_content='A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.', metadata={'id': 'Bert_pdf-9', 'source': 'Bert.pdf'}), Document(page_content='2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', metadata={'id': 'Bert_pdf-2', 'source': 'Bert.pdf'}), Document(page_content='BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', metadata={'id': 'Bert_pdf-1', 'source': 'Bert.pdf'})]",
      "metrics": null,
      "error": null,
      "parent_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_0",
      "start_time": "2023-09-20T16:48:32.116644Z",
      "end_time": "2023-09-20T16:48:32.713470Z",
      "index": 0,
      "api_calls": [
        {
          "name": "searchVectorDb",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "embeddedQuestion": [
              -0.011118830181658268,
              -0.01631597802042961,
              0.02559560351073742,
              -0.002060397295281291,
              -0.005500198341906071,
              0.031210750341415405,
              0.015424241311848164,
              -0.011599531397223473,
              -0.01773718185722828,
              -0.027727408334612846,
              0.028535543009638786,
              0.04366718605160713,
              -0.035139963030815125,
              0.005427048075944185,
              -0.0009474692051298916,
              0.008422723039984703,
              0.011244230903685093,
              0.009614026173949242,
              0.027797074988484383,
              -0.016956912353634834,
              -0.0016798420110717416,
              -0.006102816667407751,
              -0.016552845016121864,
              -0.0035739096347242594,
              -0.010742628946900368,
              -0.00849238969385624,
              0.029538746923208237,
              -0.020328788086771965,
              -0.003713243408128619,
              -0.009676726534962654,
              0.0021788307931274176,
              0.02357526309788227,
              -0.017918314784765244,
              -0.009948426857590675,
              -0.009258724749088287,
              0.006694984622299671,
              -0.0029294912237674,
              -0.016915112733840942,
              0.02438339963555336,
              -0.023212995380163193,
              0.0071164690889418125,
              0.021694257855415344,
              0.003977977205067873,
              -0.015270974487066269,
              -0.005953032989054918,
              0.01672004535794258,
              0.010652062483131886,
              -0.0065695843659341335,
              -0.012059332802891731,
              0.0093214251101017,
              0.00438204500824213,
              0.015856176614761353,
              -0.03945930674672127,
              -0.020997589454054832,
              -0.02251632697880268,
              -0.0034084508661180735,
              -0.002352998126298189,
              0.032966356724500656,
              -0.0272118728607893,
              -0.013543237000703812,
              0.0005050846957601607,
              -0.008130121976137161,
              0.0009370191837660968,
              0.008053488098084927,
              -0.022404860705137253,
              -0.024466998875141144,
              -0.008548122830688953,
              0.008018654771149158,
              0.019437052309513092,
              0.007398620247840881,
              0.01959032006561756,
              0.010038994252681732,
              0.0009866568725556135,
              -0.001406399649567902,
              0.0428033173084259,
              -0.01388460397720337,
              0.002535873558372259,
              -0.014755439944565296,
              0.001035423600114882,
              0.03592022880911827,
              0.012735101394355297,
              -0.002624698681756854,
              -0.0010606779251247644,
              0.026487339287996292,
              0.015897976234555244,
              -0.012742067687213421,
              0.010436095297336578,
              0.025275135412812233,
              0.002154447603970766,
              0.0040302276611328125,
              0.027379073202610016,
              0.03536289557814598,
              0.012944101355969906,
              0.0067054349929094315,
              -0.0038839271292090416,
              0.012247432954609394,
              -0.01118153054267168,
              0.038818370550870895,
              0.02634800411760807,
              -0.009523458778858185,
              -0.010157427750527859,
              0.008513289503753185,
              -0.04497692361474037,
              -0.005552448332309723,
              -0.024606334045529366,
              0.014852973632514477,
              0.010749596171081066,
              -0.01130693033337593,
              0.003262150567024946,
              0.001316703506745398,
              -0.019074784591794014,
              0.030207548290491104,
              0.006221250165253878,
              -0.024216199293732643,
              -0.0005220660241320729,
              -0.002276364481076598,
              0.015479975380003452,
              -0.039319973438978195,
              -0.009669759310781956,
              0.012602734379470348,
              0.017165912315249443,
              -0.004075510893017054,
              0.010721729137003422,
              -0.034582626074552536,
              0.03288275748491287,
              -0.003422384150326252,
              -0.010819262824952602,
              -0.03380235657095909,
              -0.006520817521959543,
              -0.04288691654801369,
              0.027420874685049057,
              0.022641727700829506,
              0.015939775854349136,
              -0.022753195837140083,
              0.005636048503220081,
              0.01854531653225422,
              -0.02760200761258602,
              -0.02050992287695408,
              -0.01959032006561756,
              -0.004643295891582966,
              0.02257206104695797,
              0.003828193759545684,
              -0.014253838919103146,
              -0.023993264883756638,
              -0.010192261077463627,
              0.018601050600409508,
              0.018782183527946472,
              0.020412389189004898,
              0.0024139564484357834,
              -0.010408228263258934,
              0.026334071531891823,
              -0.001065902877599001,
              -0.029761681333184242,
              0.013807971030473709,
              0.007726054172962904,
              0.0011695323046296835,
              0.00037707184674218297,
              0.009753359481692314,
              -0.009732459671795368,
              -0.005601215176284313,
              -0.020412389189004898,
              0.017012646421790123,
              0.017096245661377907,
              -0.009307491593062878,
              0.024118665605783463,
              0.03349582478404045,
              0.0020900056697428226,
              -0.014866907149553299,
              0.007328953128308058,
              -0.029204346239566803,
              -0.026222603395581245,
              0.018252715468406677,
              -0.01959032006561756,
              -0.003438059240579605,
              0.0040128109976649284,
              0.00180088821798563,
              0.03391382470726967,
              -0.02806180901825428,
              -0.02987314760684967,
              -0.015619308687746525,
              0.0193952526897192,
              0.0065521677024662495,
              0.020119788125157356,
              0.023547397926449776,
              -0.03098781779408455,
              -0.02559560351073742,
              -0.003594809677451849,
              -0.01974358595907688,
              0.01012259442359209,
              -0.034192491322755814,
              0.02382606454193592,
              0.007102536037564278,
              0.000681428937241435,
              -0.01843385025858879,
              -0.6148517727851868,
              -0.03820530325174332,
              -0.009300525300204754,
              -0.028535543009638786,
              0.01698477938771248,
              0.005514131393283606,
              -0.011627398431301117,
              0.03349582478404045,
              -0.024104731157422066,
              0.033579424023628235,
              -0.0008451460162177682,
              -0.023296596482396126,
              -0.0038212270010262728,
              0.007733020931482315,
              -0.010498794727027416,
              -0.012992868199944496,
              0.006503400858491659,
              0.004162594676017761,
              0.00922389142215252,
              0.022335194051265717,
              -0.03500062972307205,
              0.00739165348932147,
              -0.026069337502121925,
              -0.004152144771069288,
              -0.025372669100761414,
              0.0031245583668351173,
              0.006506884470582008,
              -0.021095123142004013,
              -0.009816059842705727,
              0.017151979729533195,
              -0.02045418880879879,
              -0.001589275081641972,
              0.021889325231313705,
              -0.030207548290491104,
              0.025010401383042336,
              0.021471325308084488,
              0.0021439974661916494,
              0.009084558114409447,
              -0.0007767854258418083,
              0.027880676090717316,
              -0.01596764288842678,
              -0.002070847200229764,
              -0.0024035065434873104,
              -0.0031228167936205864,
              0.017360979691147804,
              -0.000540788983926177,
              0.0186428502202034,
              -0.010498794727027416,
              0.006841285154223442,
              -0.004591045901179314,
              -0.008708356879651546,
              -0.003521659644320607,
              -0.007572787348181009,
              -0.002574190264567733,
              0.03550222888588905,
              0.011418397538363934,
              0.0342482253909111,
              -0.0024975568521767855,
              0.0014272996922954917,
              -0.011919998563826084,
              0.008631723001599312,
              0.017305247485637665,
              -0.012303167022764683,
              -0.01918625272810459,
              0.018823985010385513,
              0.009565259329974651,
              -0.006287433672696352,
              0.003291758941486478,
              0.00914725847542286,
              -0.04558999091386795,
              0.0003217737830709666,
              0.015229174867272377,
              0.0004941992810927331,
              -0.015284908004105091,
              0.00554199842736125,
              0.012338000349700451,
              0.0033718757331371307,
              0.011188496835529804,
              -0.002103938953951001,
              0.019757520407438278,
              -0.017444580793380737,
              -0.012247432954609394,
              -0.028842078521847725,
              0.002976516494527459,
              -0.0024052481167018414,
              0.0001037927286233753,
              -0.01738884672522545,
              0.017193779349327087,
              -0.01478330697864294,
              -0.0036505432799458504,
              0.0033718757331371307,
              0.03199101984500885,
              -0.028842078521847725,
              -0.042050912976264954,
              0.004918479826301336,
              0.011376597918570042,
              0.013125235214829445,
              -0.0074613201431930065,
              0.03112715110182762,
              -0.03722996637225151,
              0.014246871694922447,
              -0.020565655082464218,
              0.012351933866739273,
              -0.005886849481612444,
              0.02675207331776619,
              0.03600383177399635,
              -0.007475253660231829,
              0.0011259905295446515,
              -0.025707069784402847,
              0.0035303679760545492,
              -0.005911232437938452,
              -0.010401261039078236,
              -0.0035390763077884912,
              -0.0003302644472569227,
              -0.014212038367986679,
              -0.04040677472949028,
              0.011843365617096424,
              0.01065902877599001,
              0.01631597802042961,
              -0.003516434459015727,
              0.02599967084825039,
              0.01843385025858879,
              0.015521774999797344,
              -0.02886994555592537,
              0.01868464983999729,
              0.012045399285852909,
              0.0024522733874619007,
              0.006210800260305405,
              0.020844323560595512,
              0.02886994555592537,
              -0.011495031416416168,
              0.02172212488949299,
              0.010213160887360573,
              -0.010213160887360573,
              -0.002133547328412533,
              0.01571684330701828,
              0.0012836117530241609,
              -0.007726054172962904,
              0.01097949594259262,
              -0.0035094679333269596,
              -0.02130412496626377,
              -0.008144055493175983,
              -0.00489758001640439,
              -0.012553967535495758,
              -0.028591277077794075,
              -0.015647174790501595,
              -0.021931126713752747,
              -0.0011024780105799437,
              0.0001773239200701937,
              -0.02266959473490715,
              0.011599531397223473,
              -0.018475649878382683,
              -0.02528906799852848,
              -0.017347047105431557,
              -0.023394130170345306,
              -0.014616106636822224,
              -0.011655264534056187,
              -0.032213952392339706,
              -0.008617790415883064,
              -0.029092878103256226,
              0.006754201836884022,
              0.02534480206668377,
              -0.027588075026869774,
              -0.017765048891305923,
              -0.009676726534962654,
              -0.013877637684345245,
              0.0418279804289341,
              0.030792750418186188,
              0.0045875622890889645,
              -0.016190577298402786,
              0.013034668751060963,
              -0.03792663663625717,
              0.0009213441517204046,
              0.02201472595334053,
              0.0011190238874405622,
              0.01848958246409893,
              -0.02605540305376053,
              0.009906627237796783,
              -0.00020399325876496732,
              -0.032576221972703934,
              -0.009857860393822193,
              0.040573976933956146,
              -0.002218889305368066,
              0.006458117626607418,
              0.012233499437570572,
              0.0017199005233123899,
              0.003911793697625399,
              -0.0036261598579585552,
              -0.021025456488132477,
              0.015842242166399956,
              0.011237263679504395,
              0.019534585997462273,
              -0.017904382199048996,
              0.01627417653799057,
              -0.004322828259319067,
              0.02237699367105961,
              0.007948988117277622,
              0.027615942060947418,
              -0.014532506465911865,
              0.018754318356513977,
              0.025163667276501656,
              0.009850893169641495,
              -0.0022711395286023617,
              -0.0011756281601265073,
              -0.006064499728381634,
              -0.00982302613556385,
              -0.0011277322191745043,
              -0.03553009778261185,
              0.023338396102190018,
              0.0015709876315668225,
              0.02220979332923889,
              -0.02906501293182373,
              -0.033217158168554306,
              -0.01804371550679207,
              -0.008206755854189396,
              0.0012374575017020106,
              -0.009084558114409447,
              0.03265982121229172,
              -0.0279642753303051,
              0.00955132581293583,
              -0.006064499728381634,
              -0.012909268029034138,
              0.038706906139850616,
              0.012254400178790092,
              0.006893535144627094,
              0.006043599918484688,
              -0.0029660663567483425,
              -0.00960009265691042,
              0.015521774999797344,
              -0.020342722535133362,
              0.019228052347898483,
              0.02076072245836258,
              0.029956746846437454,
              -0.0038665104657411575,
              -0.015424241311848164,
              -0.010533628053963184,
              0.02630620449781418,
              0.009356258437037468,
              0.03279915452003479,
              0.003523401217535138,
              -0.012156866490840912,
              0.0030531499069184065,
              -0.004110344685614109,
              -0.0040406775660812855,
              0.053977880626916885,
              -0.01005292683839798,
              0.01995258778333664,
              0.030820617452263832,
              -0.01141143124550581,
              -0.0074404203332960606,
              0.017263446003198624,
              -0.011947865597903728,
              -0.017360979691147804,
              0.007551887072622776,
              0.017207713797688484,
              0.0009274400072172284,
              0.024801399558782578,
              0.0003629207785706967,
              -0.011195464059710503,
              0.010011127218604088,
              0.004267094656825066,
              -0.01249823346734047,
              0.02024518884718418,
              0.01787651516497135,
              0.023561330512166023,
              -0.014643972739577293,
              -0.030151814222335815,
              -0.028298676013946533,
              -0.012017532251775265,
              0.00023577875981573015,
              -0.017514247447252274,
              -0.004256644751876593,
              -0.010247994214296341,
              0.006573067978024483,
              0.003876960603520274,
              0.004364628344774246,
              -0.02363099716603756,
              0.01733311451971531,
              0.005022980272769928,
              0.02181965857744217,
              -0.006931852083653212,
              -0.020886123180389404,
              0.028131475672125816,
              -0.0040580942295491695,
              -0.011146697215735912,
              -0.015438174828886986,
              -0.019910788163542747,
              -0.010617229156196117,
              -0.028075743466615677,
              0.017597848549485207,
              0.008882524445652962,
              0.0077887545339763165,
              -0.020273054018616676,
              -0.003695826744660735,
              -0.002946907887235284,
              0.0015431208303198218,
              0.008861623704433441,
              -0.01118153054267168,
              0.013076468370854855,
              0.01125816348940134,
              -0.012045399285852909,
              0.005364347714930773,
              -0.016761844977736473,
              0.014448905363678932,
              0.049909334629774094,
              -0.006653184536844492,
              -0.02172212488949299,
              -0.01985505409538746,
              -0.018768250942230225,
              -0.018670717254281044,
              0.02332446351647377,
              0.010387328453361988,
              -0.019116584211587906,
              0.01153683103621006,
              0.024759599938988686,
              0.02372853085398674,
              -0.009425925090909004,
              0.0016868087695911527,
              0.011919998563826084,
              -0.0067054349929094315,
              -0.017402781173586845,
              -0.03753650188446045,
              -0.029455145820975304,
              0.0005564640159718692,
              0.025804603472352028,
              0.06325750797986984,
              0.0035094679333269596,
              0.0036993101239204407,
              -0.019464919343590736,
              0.0037480767350643873,
              -0.018015848472714424,
              -0.01592584326863289,
              0.0011303446954116225,
              -0.015633242204785347,
              -0.021485257893800735,
              -0.00836002267897129,
              0.0035652013029903173,
              0.0056499820202589035,
              0.02126232348382473,
              0.0009013149538077414,
              0.0013184451963752508,
              -0.05768415704369545,
              0.02559560351073742,
              -0.000957048439886421,
              0.003974494058638811,
              -0.014337439090013504,
              0.020816456526517868,
              0.03461049497127533,
              0.005099613685160875,
              -0.01752818003296852,
              0.01899118535220623,
              0.02509400062263012,
              0.018712516874074936,
              -0.04010024294257164,
              0.0060157328844070435,
              0.006917918566614389,
              -0.008798924274742603,
              0.00041408237302675843,
              -0.013139168731868267,
              0.022599928081035614,
              0.01708231307566166,
              0.006802968215197325,
              0.03380235657095909,
              0.0037202101666480303,
              -0.003915277309715748,
              0.012066299095749855,
              0.008645656518638134,
              -0.006245633587241173,
              -0.0037027932703495026,
              0.00836002267897129,
              -0.0035808763932436705,
              0.018503516912460327,
              0.02282286249101162,
              0.0013524078531190753,
              0.007238386198878288,
              -0.014379238709807396,
              -0.01783471554517746,
              -0.004451712127774954,
              -0.0074891867116093636,
              -0.007384686730802059,
              -0.011982698924839497,
              -0.009405025281012058,
              -0.025261200964450836,
              -0.023254796862602234,
              -0.003335300600156188,
              -0.00876409001648426,
              0.018475649878382683,
              -0.01139053050428629,
              0.009614026173949242,
              -0.005716165527701378,
              -0.003869993845000863,
              -0.017165912315249443,
              -0.013362103141844273,
              -0.0016040793852880597,
              -0.02634800411760807,
              -0.006130683235824108,
              -0.04840452969074249,
              -0.020328788086771965,
              0.023686731234192848,
              0.024397332221269608,
              0.029148612171411514,
              -0.013313336297869682,
              0.005172763951122761,
              -0.00545143149793148,
              -0.010059894062578678,
              -0.008018654771149158,
              0.016538910567760468,
              -0.005611665081232786,
              -0.016455311328172684,
              0.02609720453619957,
              -0.034136757254600525,
              -0.026111137121915817,
              -0.01950671896338463,
              0.02161065861582756,
              -0.011550764553248882,
              0.01779291406273842,
              -0.01468577329069376,
              -0.022237660363316536,
              -0.0067054349929094315,
              0.029287945479154587,
              -0.004845329560339451,
              0.02746267430484295,
              -0.0020969724282622337,
              -0.02605540305376053,
              0.020872190594673157,
              -0.03722996637225151,
              0.0004397720331326127,
              -0.03472195938229561,
              -0.012888368219137192,
              -0.003988427575677633,
              -0.0021770892199128866,
              0.012331033125519753,
              -0.014072705060243607,
              -0.003828193759545684,
              -0.0120941661298275,
              -0.01315310224890709,
              -0.01251913420855999,
              -0.0035025011748075485,
              0.046119458973407745,
              0.013055568560957909,
              0.004500478971749544,
              -0.014797240495681763,
              0.009202991612255573,
              -0.020523855462670326,
              -0.006771618500351906,
              -0.015340641140937805,
              0.02553986944258213,
              0.00311759184114635,
              -0.014866907149553299,
              0.03867903724312782,
              -0.01282566785812378,
              -0.011926965788006783,
              -0.015438174828886986,
              0.035335030406713486,
              -0.0234359297901392,
              0.045617856085300446,
              0.0035460428334772587,
              0.0027309407014399767,
              -0.017500314861536026,
              0.008276422508060932,
              -0.005280747544020414,
              -0.010895895771682262,
              -0.03148941695690155,
              0.018614983186125755,
              0.026361938565969467,
              -0.012303167022764683,
              -0.003533851122483611,
              -0.022530261427164078,
              -0.01612091064453125,
              -0.020091921091079712,
              -0.024271931499242783,
              0.04508838802576065,
              -0.0038978606462478638,
              0.003025283105671406,
              0.007168719545006752,
              0.009363225661218166,
              -0.0028371827211230993,
              -0.0002782320079859346,
              -0.037257835268974304,
              -0.020426321774721146,
              0.0015509583754464984,
              0.0397937074303627,
              0.0193952526897192,
              0.024620266631245613,
              0.04701119288802147,
              -0.011947865597903728,
              0.018127316609025,
              0.020217321813106537,
              -0.0008625627378933132,
              -0.0031837753485888243,
              -0.030346881598234177,
              -0.008234621956944466,
              -0.015995509922504425,
              0.014191138558089733,
              0.007067702244967222,
              -0.0076424540020525455,
              0.006956235505640507,
              0.01985505409538746,
              0.015006240457296371,
              0.011077029630541801,
              -0.003838643664494157,
              -0.0021370307076722383,
              0.0010876738233491778,
              -0.019172318279743195,
              -0.01256790105253458,
              0.01869858428835869,
              -0.018308449536561966,
              0.00748222041875124,
              -0.03380235657095909,
              -0.004695545881986618,
              0.0031559085473418236,
              -0.018865784630179405,
              0.011940899305045605,
              -0.00720355287194252,
              0.019074784591794014,
              -0.019269851967692375,
              0.010282827541232109,
              -0.016998711973428726,
              0.013821904547512531,
              -0.013229736126959324,
              -0.010888929478824139,
              -0.023463796824216843,
              -0.001105090486817062,
              0.01677577942609787,
              0.019966520369052887,
              -0.011620431207120419,
              0.0018583633936941624,
              -0.030514081940054893,
              0.015326708555221558,
              -0.008137088268995285,
              -0.029148612171411514,
              -0.003768976777791977,
              0.001748638111166656,
              0.007879321463406086,
              -0.016817579045891762,
              -0.04946346580982208,
              -0.034638360142707825,
              -0.013362103141844273,
              -0.009634925983846188,
              -0.001558795920573175,
              -0.016399577260017395,
              0.015298841521143913,
              -0.011042196303606033,
              -0.04135424643754959,
              -0.004496995359659195,
              -0.0026560488622635603,
              0.03023541532456875,
              0.022335194051265717,
              0.005200630519539118,
              0.012985901907086372,
              0.030904216691851616,
              0.013389969244599342,
              -0.011007362976670265,
              0.00028258617385290563,
              -0.019019052386283875,
              0.009920559823513031,
              0.011188496835529804,
              -0.023867864161729813,
              -0.0428311824798584,
              0.0299846138805151,
              0.027393007650971413,
              -0.017667515203356743,
              -0.029343679547309875,
              0.015856176614761353,
              -0.018782183527946472,
              -0.001386370393447578,
              0.00997629389166832,
              -0.011425363831222057,
              -0.021122990176081657,
              -0.009035791270434856,
              -0.012170800007879734,
              0.014295638538897038,
              0.01468577329069376,
              -0.005298164207488298,
              -0.017848648130893707,
              0.013013768941164017,
              -0.020119788125157356,
              0.005218047648668289,
              0.010881963185966015,
              -0.010847128927707672,
              0.024007197469472885,
              0.01471363939344883,
              0.022711394354701042,
              0.014267771504819393,
              0.03304995596408844,
              0.026334071531891823,
              -0.030709149315953255,
              0.018378116190433502,
              -0.013452669605612755,
              -0.003040958195924759,
              -0.007224452681839466,
              0.005360864568501711,
              0.004469128791242838,
              0.0304862167686224,
              -0.022711394354701042,
              -0.00927265826612711,
              0.004099894315004349,
              -0.009767292998731136,
              0.017653580754995346,
              0.008729256689548492,
              -0.00025994444149546325,
              0.004911513067781925,
              -0.011522897519171238,
              0.007928088307380676,
              0.034638360142707825,
              0.006123716477304697,
              -0.02176392637193203,
              -0.0007519666105508804,
              -0.0016493628500029445,
              0.0037445933558046818,
              -0.02690533921122551,
              0.009453792124986649,
              0.029510879889130592,
              -0.0418279804289341,
              -0.00914029125124216,
              0.027518408372998238,
              0.011334797367453575,
              0.011049163527786732,
              -0.029148612171411514,
              0.011090963147580624,
              0.013167035765945911,
              0.021025456488132477,
              -0.013410869985818863,
              -0.010206194594502449,
              -0.007503120228648186,
              0.015326708555221558,
              -0.03006821498274803,
              -0.017165912315249443,
              -0.019924720749258995,
              -0.011857299134135246,
              0.0158701092004776,
              0.021039390936493874,
              0.007565820589661598,
              -0.007872354239225388,
              0.010540595278143883,
              -0.029343679547309875,
              0.010241027921438217,
              0.009753359481692314,
              -0.004260128363966942,
              -0.00318725872784853,
              0.014769373461604118,
              -0.0038665104657411575,
              0.010450027883052826,
              0.0176257137209177,
              0.020133720710873604,
              -0.0065626176074147224,
              0.01000415999442339,
              -0.030597683042287827,
              -0.015897976234555244,
              -0.007955954410135746,
              0.004124277736991644,
              -0.00038055519689805806,
              -0.02800607495009899,
              0.006447667255997658,
              0.004775662906467915,
              0.00568133220076561,
              -0.0022397893480956554,
              -0.015354574657976627,
              -0.005329514387995005,
              0.002267656149342656,
              -0.011676165275275707,
              0.00816495530307293,
              0.016566777601838112,
              -0.023310529068112373,
              0.037007033824920654,
              0.0067054349929094315,
              0.04951919987797737,
              0.0018670717254281044,
              -0.01606517657637596,
              -0.03491702675819397,
              -0.026125071570277214,
              -0.017556047067046165,
              -0.026877472177147865,
              -0.03304995596408844,
              -0.025484135374426842,
              0.02790854312479496,
              0.019562453031539917,
              -0.027128273621201515,
              -0.03260408714413643,
              -0.018196983262896538,
              -0.019659986719489098,
              0.00997629389166832,
              0.01646924391388893,
              0.01779291406273842,
              0.010575428605079651,
              0.01617664471268654,
              -0.02841014415025711,
              0.03714636713266373,
              0.010087761096656322,
              -0.0006901372689753771,
              -0.013550203293561935,
              0.012581833638250828,
              0.011397497728466988,
              -0.01612091064453125,
              0.004803529474884272,
              0.02473173290491104,
              -0.04751279577612877,
              -0.0007872354472056031,
              0.03784303739666939,
              -0.006451150868088007,
              0.02403506450355053,
              0.03918064013123512,
              -0.019715720787644386,
              -0.03480556234717369,
              -0.01430957205593586,
              0.018322382122278214,
              0.033022090792655945,
              0.01692904531955719,
              0.003636609762907028,
              -0.0019855054561048746,
              -0.01342480257153511,
              0.0052772643975913525,
              0.013717403635382652,
              -0.026724206283688545,
              0.0027675158344209194,
              0.0054792980663478374,
              0.0037724601570516825,
              -0.02397933229804039,
              -0.017221646383404732,
              0.011676165275275707,
              0.012059332802891731,
              -0.014978373423218727,
              0.020523855462670326,
              -0.004420361947268248,
              -0.02967808023095131,
              0.0049393801018595695,
              0.02679387293756008,
              0.0015971126267686486,
              -0.020900055766105652,
              0.0110839968547225,
              -0.027936408296227455,
              0.0032029335852712393,
              0.02161065861582756,
              0.012958034873008728,
              -0.0052842311561107635,
              -0.002953874645754695,
              0.005691782105714083,
              -0.02573493681848049,
              -0.00224153115414083,
              -0.007691220846027136,
              0.0058694323524832726,
              -0.018461717292666435,
              0.04625879228115082,
              0.02917647920548916,
              -0.05105186998844147,
              0.017458513379096985,
              -0.013780103996396065,
              -0.008833757601678371,
              -0.019032984972000122,
              0.0032691170927137136,
              0.005033430177718401,
              -0.021136924624443054,
              0.02101152390241623,
              0.0037480767350643873,
              -0.03505636006593704,
              -0.0019611220341175795,
              -0.0012052366510033607,
              0.008680489845573902,
              -0.01047789491713047,
              0.02574886940419674,
              0.2039845585823059,
              -0.03405315801501274,
              0.01833631657063961,
              0.0029573580250144005,
              0.013780103996396065,
              0.0176257137209177,
              0.0037550434935837984,
              0.017709314823150635,
              -0.00217186426743865,
              -0.001844429993070662,
              0.00048462007544003427,
              0.008060455322265625,
              -0.013222768902778625,
              0.0024365982972085476,
              0.0224187932908535,
              0.0010824487544596195,
              -0.012066299095749855,
              -0.046175193041563034,
              -0.00687611848115921,
              -0.0056778485886752605,
              -0.005225013941526413,
              0.014699706807732582,
              -0.003213383723050356,
              -0.036505430936813354,
              0.014351372607052326,
              -0.012916235253214836,
              -0.006691501475870609,
              -0.026919273659586906,
              0.006419800687581301,
              0.03472195938229561,
              -0.007377719972282648,
              0.01858711615204811,
              -0.008241589181125164,
              0.015479975380003452,
              -0.009690659120678902,
              -0.020384522154927254,
              -0.02272532880306244,
              0.02222372591495514,
              0.011056129820644855,
              -0.009425925090909004,
              0.035084228962659836,
              0.010401261039078236,
              -0.008025621995329857,
              -0.012331033125519753,
              -0.00031154145835898817,
              -0.00023947982117533684,
              0.000322644627885893,
              -0.013814937323331833,
              -0.002095230622217059,
              0.03023541532456875,
              -0.010512728244066238,
              0.02654307149350643,
              0.039124906063079834,
              0.042691849172115326,
              0.0030461831483989954,
              -0.014267771504819393,
              0.002694365568459034,
              0.00595651613548398,
              0.005886849481612444,
              0.01949278637766838,
              -0.025762803852558136,
              0.0186428502202034,
              0.002546323463320732,
              0.03112715110182762,
              0.03179595246911049,
              0.015856176614761353,
              -0.019882921129465103,
              0.012393733486533165,
              0.015730775892734528,
              -0.004723412916064262,
              -0.015382441692054272,
              -0.016817579045891762,
              -0.005925165954977274,
              -0.004653745796531439,
              -0.03502849489450455,
              -0.02126232348382473,
              0.051414139568805695,
              0.01818304881453514,
              0.009565259329974651,
              0.034081026911735535,
              -0.018754318356513977,
              -0.005834599025547504,
              0.011369630694389343,
              -0.014894773252308369,
              -0.026779938489198685,
              -0.043054115027189255,
              0.017305247485637665,
              -0.01395427156239748,
              -0.017946181818842888,
              -0.016483178362250328,
              0.0023791231215000153,
              -0.021081190556287766,
              -0.008959157392382622,
              0.008617790415883064,
              -0.0004689450142905116,
              -0.010735662654042244,
              -0.019172318279743195,
              0.03940357267856598,
              -0.0014569080667570233,
              -0.021596726030111313,
              -0.03452689200639725,
              0.04703906178474426,
              0.03842823579907417,
              0.02644553780555725,
              -0.0008155376417562366,
              0.004946346860378981,
              -0.01019922737032175,
              -0.008157989010214806,
              0.022502394393086433,
              -0.0013027702225372195,
              0.001785213127732277,
              -0.012386767193675041,
              0.0019053885480389,
              0.007997754961252213,
              -0.0018931968370452523,
              -0.011327831074595451,
              0.005221530795097351,
              -0.014393172226846218,
              0.012637567706406116,
              -0.005221530795097351,
              0.0005899911629967391,
              -0.03388595953583717,
              -0.039821576327085495,
              0.017946181818842888,
              -0.0014830331783741713,
              -0.004685095977038145,
              -0.016413511708378792,
              -0.02619473822414875,
              -0.01430957205593586,
              -0.007405586540699005,
              0.011327831074595451,
              -0.03265982121229172,
              0.03530716150999069,
              -0.021582791581749916,
              -0.02906501293182373,
              0.00651036761701107,
              0.01950671896338463,
              -0.017542114481329918,
              -0.01779291406273842,
              0.009941460564732552,
              0.009460759349167347,
              -5.8073856052942574e-05,
              0.006141133140772581,
              -0.006984102074056864,
              0.01627417653799057,
              -0.0468439944088459,
              0.0076215541921556,
              -0.014434972777962685,
              -0.0014351372374221683,
              -0.03625463321805,
              -0.01478330697864294,
              -0.007649420760571957,
              0.0015466042095795274,
              -0.019311651587486267,
              0.023463796824216843,
              -0.002380864694714546,
              -0.018670717254281044,
              -0.015159507282078266,
              -0.013264569453895092,
              0.0021109057124704123,
              -0.028786344453692436,
              -0.015591442584991455,
              -0.0022641727700829506,
              -0.025219401344656944,
              -0.009077590890228748,
              -0.009579192847013474,
              -0.17745541036128998,
              0.010847128927707672,
              0.02076072245836258,
              -0.032074619084596634,
              -0.005552448332309723,
              0.02711433917284012,
              0.032269686460494995,
              -0.009850893169641495,
              -0.020482055842876434,
              0.003523401217535138,
              -0.0010632904013618827,
              -0.017012646421790123,
              -0.02856341004371643,
              -0.0007136498461477458,
              0.004350695293396711,
              0.01306950207799673,
              0.0018235299503430724,
              0.02045418880879879,
              0.024898933246731758,
              0.02825687639415264,
              0.03143368661403656,
              -0.044614654034376144,
              0.013536269776523113,
              -0.007321986369788647,
              0.02064925618469715,
              -0.009767292998731136,
              -0.010171361267566681,
              0.03137795254588127,
              -0.017151979729533195,
              -0.014671839773654938,
              0.012128999456763268,
              0.02725367434322834,
              0.012101132422685623,
              -0.004552728962153196,
              0.00025515485322102904,
              0.020886123180389404,
              0.01123029738664627,
              -0.01334816962480545,
              -0.0011651781387627125,
              -0.008415755815804005,
              0.05252880975604057,
              -0.00992752704769373,
              -0.0076145874336361885,
              -0.014121471904218197,
              -0.023644929751753807,
              -0.011097930371761322,
              0.007670320570468903,
              -0.0022746229078620672,
              0.016009442508220673,
              0.018364183604717255,
              -0.01748638041317463,
              -0.020384522154927254,
              0.020635321736335754,
              0.006398900877684355,
              0.007005002349615097,
              0.02679387293756008,
              0.011662231758236885,
              0.00821372214704752,
              -0.026166871190071106,
              0.015535708516836166,
              0.004977697040885687,
              -0.033022090792655945,
              0.026013603433966637,
              0.002319906372576952,
              -0.014978373423218727,
              -0.0016868087695911527,
              0.0018479133723303676,
              0.004333278629928827,
              -0.021986858919262886,
              0.011202430352568626,
              -0.03647756576538086,
              -0.023463796824216843,
              0.0003302644472569227,
              0.0047408295795321465,
              0.022599928081035614,
              -0.00012757742661051452,
              -0.019367385655641556,
              -0.0012609701370820403,
              0.001722512999549508,
              -0.01123029738664627,
              -0.033022090792655945,
              0.0317402184009552,
              -0.0096976263448596,
              0.026975005865097046,
              0.0032081587705761194,
              -0.012087199836969376,
              0.005521098151803017,
              0.018322382122278214,
              -0.019715720787644386,
              -0.026431605219841003,
              0.011432331055402756,
              0.0060226996429264545,
              0.017542114481329918,
              0.0019785386975854635,
              0.007412553299218416,
              0.01779291406273842,
              0.010220128111541271,
              -0.015284908004105091,
              0.022181926295161247,
              -0.012853534892201424,
              -0.02105332352221012,
              -0.017347047105431557,
              0.006381483748555183,
              0.009307491593062878,
              0.012783868238329887,
              0.02533086948096752,
              -0.019618187099695206,
              0.006694984622299671,
              0.029037145897746086,
              0.011481097899377346,
              -0.006043599918484688,
              -0.010833196341991425,
              0.00023664960463065654,
              0.02629227191209793,
              0.017820781096816063,
              0.009133324958384037,
              -0.03193528577685356,
              -0.0027918992564082146,
              0.005256364122033119,
              -0.02599967084825039,
              0.043499983847141266,
              -0.010603295639157295,
              0.003612226340919733,
              0.014978373423218727,
              -0.0007920250645838678,
              -0.01804371550679207,
              -0.11425364017486572,
              -0.008206755854189396,
              -0.002555032027885318,
              0.004354178439825773,
              -0.014490705914795399,
              -0.004120794590562582,
              0.02172212488949299,
              0.029483012855052948,
              -0.026027537882328033,
              0.010624195449054241,
              -0.020579589530825615,
              -0.02711433917284012,
              0.013090401887893677,
              0.004803529474884272,
              -0.005319064483046532,
              0.008185855112969875,
              0.0033701341599226,
              -0.0047582462430000305,
              -0.009251758456230164,
              0.026682404801249504,
              -0.0026473405305296183,
              -0.01959032006561756,
              0.014393172226846218,
              -0.024048998951911926,
              0.015842242166399956,
              0.00677858479321003,
              -0.029705947265028954,
              0.0015788250602781773,
              0.021889325231313705,
              0.011028262786567211,
              0.007398620247840881,
              -0.03455476090312004,
              -0.00881285686045885,
              -0.003410192672163248,
              -0.0019837636500597,
              -0.003326592268422246,
              -0.012909268029034138,
              -0.022558128461241722,
              0.005782349035143852,
              -0.03405315801501274,
              0.0029051078017801046,
              0.013292435556650162,
              -0.004169561434537172,
              -0.010157427750527859,
              -0.010136527009308338,
              -0.00012942795001436025,
              -0.014434972777962685,
              0.030151814222335815,
              0.023770330473780632,
              -0.00803258828818798,
              -0.012198666110634804,
              -0.02045418880879879,
              -0.02499646693468094,
              -0.005367831327021122,
              0.020941857248544693,
              0.033133555203676224,
              0.0186428502202034,
              0.013598970137536526,
              -0.004517895635217428,
              -0.012965002097189426,
              -0.000530774355866015,
              -0.005092646926641464,
              -0.01698477938771248,
              0.005144897382706404,
              0.018628917634487152,
              -0.001567504252307117,
              -0.05473028123378754,
              -0.011167597025632858,
              -0.008673523552715778,
              -0.005350414663553238,
              -0.014156305231153965,
              0.01793224923312664,
              0.013194902800023556,
              0.02155492454767227,
              -0.01858711615204811,
              -0.01012259442359209,
              -0.014281705021858215,
              -0.0021091641392558813,
              0.0022066975943744183,
              -0.018726451322436333,
              -0.01090286299586296,
              -0.0274487417191267,
              0.007698187604546547,
              -0.002624698681756854,
              -0.010220128111541271,
              -0.012804768048226833,
              -0.0012182991486042738,
              -0.02499646693468094,
              -0.011745831929147243,
              -0.029761681333184242,
              0.009662793017923832,
              0.008645656518638134,
              -0.029761681333184242,
              -0.008234621956944466,
              -0.004180011339485645,
              -0.0024644650984555483,
              -0.0031994504388421774,
              -0.01390550471842289,
              0.020217321813106537,
              0.005987866315990686,
              -0.015075907111167908,
              0.0043750787153840065,
              -0.04391798377037048,
              0.016162710264325142,
              0.0032952423207461834,
              -0.01698477938771248,
              0.011655264534056187,
              0.018823985010385513,
              0.03692343458533287,
              -0.017040513455867767,
              0.002717007417231798,
              0.008854657411575317,
              -0.007363786455243826,
              0.021485257893800735,
              -0.01738884672522545,
              -0.018977250903844833,
              -0.006447667255997658,
              -0.010993429459631443,
              0.005925165954977274,
              -0.004507445730268955,
              0.029204346239566803,
              0.005803248845040798,
              -0.01499230694025755,
              -0.016538910567760468,
              0.009572225622832775,
              -0.00025798508431762457,
              -0.007774821016937494,
              0.013828870840370655,
              -0.010234060697257519,
              0.026320137083530426,
              -0.026612738147377968,
              -0.011780665256083012,
              0.020077988505363464,
              -0.02403506450355053,
              -0.0008969607879407704,
              0.030040347948670387,
              -0.0010589362354949117,
              0.016664311289787292,
              0.02423013187944889,
              0.008596889674663544,
              0.03622676432132721,
              0.004639812745153904,
              -0.014755439944565296,
              -0.03600383177399635,
              0.0020307889208197594,
              -0.012540034018456936,
              -0.00260728201828897,
              0.022251592949032784,
              -0.0027222323697060347,
              0.003500759368762374,
              0.015479975380003452,
              -0.006398900877684355,
              0.008778023533523083,
              0.02091399021446705,
              0.019074784591794014,
              -0.029009278863668442,
              -0.015953710302710533,
              -0.00849238969385624,
              -0.0035286261700093746,
              0.02278106100857258,
              0.002276364481076598,
              -0.0034415426198393106,
              0.0367841012775898,
              -0.015048041008412838,
              0.022279459983110428,
              0.003981460817158222,
              0.0076424540020525455,
              -0.008750157430768013,
              0.012428566813468933,
              0.007039835676550865,
              -0.02912074513733387,
              -0.02827080897986889,
              -0.010568462312221527,
              0.009251758456230164,
              -0.011773698963224888,
              0.02565133571624756,
              0.029037145897746086,
              0.005252880975604057,
              -0.0007088602287694812,
              0.0114601980894804,
              -0.005998316220939159,
              0.01478330697864294,
              0.012484300881624222,
              -0.02459239959716797,
              -0.016845446079969406,
              -0.014448905363678932,
              0.024453066289424896,
              0.002779707545414567,
              -0.004451712127774954,
              0.012170800007879734,
              -0.005378281231969595,
              0.02195899188518524,
              0.012184732593595982,
              0.001861846656538546,
              0.009760326705873013,
              0.03555796295404434,
              0.00401629414409399,
              0.01490870676934719,
              0.007349853403866291,
              -0.019269851967692375,
              0.006796001922339201,
              -0.021749991923570633,
              0.006127200089395046,
              0.015828309580683708,
              -0.012463400140404701,
              -0.010094727389514446,
              -0.023157263174653053,
              0.021136924624443054,
              -0.013229736126959324,
              -0.00457362923771143,
              0.003953594248741865,
              0.03770370036363602,
              -0.0011442780960351229,
              0.015298841521143913,
              -0.01219169981777668,
              0.01080532930791378,
              -0.034276094287633896,
              0.02765774168074131,
              -0.007210519630461931,
              -0.036505430936813354,
              -0.026570938527584076,
              0.033022090792655945,
              0.021931126713752747,
              -0.01518737431615591,
              0.044558919966220856,
              -0.012212599627673626,
              0.018559250980615616,
              0.010819262824952602,
              0.024717800319194794,
              -0.011906065978109837,
              0.02638980560004711,
              0.02278106100857258,
              0.017597848549485207,
              -0.008680489845573902,
              0.01989685371518135,
              -0.01874038390815258,
              -0.021136924624443054,
              -0.012310133315622807,
              -0.006607901304960251,
              0.03279915452003479,
              -0.0008817211491987109,
              0.10160214453935623,
              0.021248390898108482,
              -0.0030949499923735857,
              0.012115065939724445,
              -0.015702908858656883,
              0.011871231719851494,
              0.013362103141844273,
              0.018921518698334694,
              -0.011474130675196648,
              -0.0010545820696279407,
              0.006339683663100004,
              -0.015591442584991455,
              -0.013689536601305008,
              -0.02423013187944889,
              0.0035355929285287857,
              -0.015605375170707703,
              0.003765493631362915,
              0.008617790415883064,
              -0.008610823191702366,
              -0.0030200581531971693,
              0.024299798533320427,
              -0.00043737722444348037,
              0.011223330162465572,
              0.01602337695658207,
              -0.013118268921971321,
              -0.009669759310781956,
              0.007018935866653919,
              -0.016455311328172684,
              -0.008673523552715778,
              -0.03112715110182762,
              0.004406428430229425,
              -0.016566777601838112,
              -0.04166077822446823,
              -0.013571103103458881,
              0.02815934270620346,
              0.01097949594259262,
              -0.001757346442900598,
              0.013341202400624752,
              0.014121471904218197,
              5.785614484921098e-05,
              0.009293558076024055,
              0.036449696868658066,
              -0.0029138161335140467,
              -0.00964885950088501,
              -0.013647736981511116,
              -0.007997754961252213,
              0.0014725831570103765,
              -0.013731337152421474,
              -0.02312939614057541
            ],
            "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
            "indexType": "cogsearchvs",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            },
            "question": "How are sentence pairs represented in the input to BERT?"
          },
          "output": [
            {
              "page_content": "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.",
              "metadata": {
                "id": "Bert_pdf-9",
                "source": "Bert.pdf"
              }
            },
            {
              "page_content": "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
              "metadata": {
                "id": "Bert_pdf-2",
                "source": "Bert.pdf"
              }
            },
            {
              "page_content": "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
              "metadata": {
                "id": "Bert_pdf-1",
                "source": "Bert.pdf"
              }
            }
          ],
          "start_time": 1695246512.116644,
          "end_time": 1695246512.696523,
          "error": null,
          "children": null,
          "node_name": "search_question_from_vectordb"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "[2023-09-20T16:48:32+0000] searchType: hybridrerank\n[2023-09-20T16:48:32+0000] Performing CogSearch\n[2023-09-20T16:48:32+0000] SearchService: dataaioaicg\n[2023-09-20T16:48:32+0000] indexName: 8fe8ee44933240fa8bc1d72858e4d1eb\n",
        "stderr": ""
      },
      "system_metrics": {
        "duration": 0.596826
      },
      "result": "[Document(page_content='A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.', metadata={'id': 'Bert_pdf-9', 'source': 'Bert.pdf'}), Document(page_content='2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', metadata={'id': 'Bert_pdf-2', 'source': 'Bert.pdf'}), Document(page_content='BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', metadata={'id': 'Bert_pdf-1', 'source': 'Bert.pdf'})]"
    },
    {
      "node": "extract_prompttemplate",
      "flow_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7",
      "run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_extract_prompttemplate_0",
      "status": "Completed",
      "inputs": {
        "overrides": {
          "chainType": "stuff",
          "deploymentType": "gpt3516k",
          "embeddingModelType": "azureopenai",
          "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
          "searchType": "hybridrerank",
          "semantic_captions": false,
          "semantic_ranker": true,
          "temperature": 0,
          "tokenLength": 1000,
          "top": 3
        }
      },
      "output": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
      "metrics": null,
      "error": null,
      "parent_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_0",
      "start_time": "2023-09-20T16:48:32.135412Z",
      "end_time": "2023-09-20T16:48:32.137681Z",
      "index": 0,
      "api_calls": [
        {
          "name": "extractPrommptTemplate",
          "type": "Tool",
          "inputs": {
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            }
          },
          "output": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
          "start_time": 1695246512.135412,
          "end_time": 1695246512.137681,
          "error": null,
          "children": null,
          "node_name": "extract_prompttemplate"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "",
        "stderr": ""
      },
      "system_metrics": {
        "duration": 0.002269
      },
      "result": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        "
    },
    {
      "node": "execute_langchain",
      "flow_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7",
      "run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_execute_langchain_0",
      "status": "Completed",
      "inputs": {
        "llm": "AzureChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.3, model_kwargs={}, openai_api_key='1361532858cb428c8da412a73105de78', openai_api_base='https://dataaiapim.azure-api.net', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=1000, tiktoken_model_name=None, deployment_name='chat16k', model_version='', openai_api_type='azure', openai_api_version='2023-07-01-preview')",
        "overrides": {
          "chainType": "stuff",
          "deploymentType": "gpt3516k",
          "embeddingModelType": "azureopenai",
          "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
          "searchType": "hybridrerank",
          "semantic_captions": false,
          "semantic_ranker": true,
          "temperature": 0,
          "tokenLength": 1000,
          "top": 3
        },
        "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
        "question": "How are sentence pairs represented in the input to BERT?",
        "retrievedDocs": "[Document(page_content='A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.', metadata={'id': 'Bert_pdf-9', 'source': 'Bert.pdf'}), Document(page_content='2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', metadata={'id': 'Bert_pdf-2', 'source': 'Bert.pdf'}), Document(page_content='BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', metadata={'id': 'Bert_pdf-1', 'source': 'Bert.pdf'})]"
      },
      "output": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B.",
      "metrics": null,
      "error": null,
      "parent_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_0",
      "start_time": "2023-09-20T16:48:32.715200Z",
      "end_time": "2023-09-20T16:48:34.147496Z",
      "index": 0,
      "api_calls": [
        {
          "name": "executeQa",
          "type": "Tool",
          "inputs": {
            "llm": {
              "model": "gpt-3.5-turbo",
              "request_timeout": null,
              "max_tokens": 1000,
              "stream": false,
              "n": 1,
              "temperature": 0.3,
              "engine": "chat16k",
              "_type": "azure-openai-chat"
            },
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            },
            "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
            "question": "How are sentence pairs represented in the input to BERT?",
            "retrievedDocs": [
              {
                "page_content": "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.",
                "metadata": {
                  "id": "Bert_pdf-9",
                  "source": "Bert.pdf"
                }
              },
              {
                "page_content": "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "metadata": {
                  "id": "Bert_pdf-2",
                  "source": "Bert.pdf"
                }
              },
              {
                "page_content": "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
                "metadata": {
                  "id": "Bert_pdf-1",
                  "source": "Bert.pdf"
                }
              }
            ]
          },
          "output": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B.",
          "start_time": 1695246512.7152,
          "end_time": 1695246514.143959,
          "error": null,
          "children": [
            {
              "name": "openai.api_resources.chat_completion.ChatCompletion.create",
              "type": "LLM",
              "inputs": {
                "messages": [
                  {
                    "role": "user",
                    "content": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        Content: A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.\nSource: Bert.pdf\n\nContent: 2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.\nSource: Bert.pdf\n\nContent: BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\nSource: Bert.pdf\n        Question: How are sentence pairs represented in the input to BERT?\n        "
                  }
                ],
                "model": "gpt-3.5-turbo",
                "request_timeout": null,
                "max_tokens": 1000,
                "stream": false,
                "n": 1,
                "temperature": 0.3,
                "engine": "chat16k",
                "api_base": "https://dataaiapim.azure-api.net",
                "organization": "",
                "api_type": "azure",
                "api_version": "2023-07-01-preview"
              },
              "output": {
                "id": "chatcmpl-80ufJMRFP3LCpOqkQz9aICocNSAMZ",
                "object": "chat.completion",
                "created": 1695228513,
                "model": "gpt-35-turbo-16k",
                "prompt_filter_results": [
                  {
                    "prompt_index": 0,
                    "content_filter_results": {
                      "hate": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "self_harm": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "sexual": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "violence": {
                        "filtered": false,
                        "severity": "safe"
                      }
                    }
                  }
                ],
                "choices": [
                  {
                    "index": 0,
                    "finish_reason": "stop",
                    "message": {
                      "role": "assistant",
                      "content": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B."
                    },
                    "content_filter_results": {
                      "hate": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "self_harm": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "sexual": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "violence": {
                        "filtered": false,
                        "severity": "safe"
                      }
                    }
                  }
                ],
                "usage": {
                  "completion_tokens": 52,
                  "prompt_tokens": 6056,
                  "total_tokens": 6108
                }
              },
              "start_time": 1695246512.724745,
              "end_time": 1695246514.14296,
              "error": null,
              "children": null,
              "node_name": null
            }
          ],
          "node_name": "execute_langchain"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "",
        "stderr": ""
      },
      "system_metrics": {
        "completion_tokens": 52,
        "prompt_tokens": 6056,
        "total_tokens": 6108,
        "duration": 1.432296
      },
      "result": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B."
    },
    {
      "node": "followup_questions",
      "flow_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7",
      "run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_followup_questions_0",
      "status": "Completed",
      "inputs": {
        "conn": "entaoai",
        "embeddedQuestion": [
          -0.011118830181658268,
          -0.01631597802042961,
          0.02559560351073742,
          -0.002060397295281291,
          -0.005500198341906071,
          0.031210750341415405,
          0.015424241311848164,
          -0.011599531397223473,
          -0.01773718185722828,
          -0.027727408334612846,
          0.028535543009638786,
          0.04366718605160713,
          -0.035139963030815125,
          0.005427048075944185,
          -0.0009474692051298916,
          0.008422723039984703,
          0.011244230903685093,
          0.009614026173949242,
          0.027797074988484383,
          -0.016956912353634834,
          -0.0016798420110717416,
          -0.006102816667407751,
          -0.016552845016121864,
          -0.0035739096347242594,
          -0.010742628946900368,
          -0.00849238969385624,
          0.029538746923208237,
          -0.020328788086771965,
          -0.003713243408128619,
          -0.009676726534962654,
          0.0021788307931274176,
          0.02357526309788227,
          -0.017918314784765244,
          -0.009948426857590675,
          -0.009258724749088287,
          0.006694984622299671,
          -0.0029294912237674,
          -0.016915112733840942,
          0.02438339963555336,
          -0.023212995380163193,
          0.0071164690889418125,
          0.021694257855415344,
          0.003977977205067873,
          -0.015270974487066269,
          -0.005953032989054918,
          0.01672004535794258,
          0.010652062483131886,
          -0.0065695843659341335,
          -0.012059332802891731,
          0.0093214251101017,
          0.00438204500824213,
          0.015856176614761353,
          -0.03945930674672127,
          -0.020997589454054832,
          -0.02251632697880268,
          -0.0034084508661180735,
          -0.002352998126298189,
          0.032966356724500656,
          -0.0272118728607893,
          -0.013543237000703812,
          0.0005050846957601607,
          -0.008130121976137161,
          0.0009370191837660968,
          0.008053488098084927,
          -0.022404860705137253,
          -0.024466998875141144,
          -0.008548122830688953,
          0.008018654771149158,
          0.019437052309513092,
          0.007398620247840881,
          0.01959032006561756,
          0.010038994252681732,
          0.0009866568725556135,
          -0.001406399649567902,
          0.0428033173084259,
          -0.01388460397720337,
          0.002535873558372259,
          -0.014755439944565296,
          0.001035423600114882,
          0.03592022880911827,
          0.012735101394355297,
          -0.002624698681756854,
          -0.0010606779251247644,
          0.026487339287996292,
          0.015897976234555244,
          -0.012742067687213421,
          0.010436095297336578,
          0.025275135412812233,
          0.002154447603970766,
          0.0040302276611328125,
          0.027379073202610016,
          0.03536289557814598,
          0.012944101355969906,
          0.0067054349929094315,
          -0.0038839271292090416,
          0.012247432954609394,
          -0.01118153054267168,
          0.038818370550870895,
          0.02634800411760807,
          -0.009523458778858185,
          -0.010157427750527859,
          0.008513289503753185,
          -0.04497692361474037,
          -0.005552448332309723,
          -0.024606334045529366,
          0.014852973632514477,
          0.010749596171081066,
          -0.01130693033337593,
          0.003262150567024946,
          0.001316703506745398,
          -0.019074784591794014,
          0.030207548290491104,
          0.006221250165253878,
          -0.024216199293732643,
          -0.0005220660241320729,
          -0.002276364481076598,
          0.015479975380003452,
          -0.039319973438978195,
          -0.009669759310781956,
          0.012602734379470348,
          0.017165912315249443,
          -0.004075510893017054,
          0.010721729137003422,
          -0.034582626074552536,
          0.03288275748491287,
          -0.003422384150326252,
          -0.010819262824952602,
          -0.03380235657095909,
          -0.006520817521959543,
          -0.04288691654801369,
          0.027420874685049057,
          0.022641727700829506,
          0.015939775854349136,
          -0.022753195837140083,
          0.005636048503220081,
          0.01854531653225422,
          -0.02760200761258602,
          -0.02050992287695408,
          -0.01959032006561756,
          -0.004643295891582966,
          0.02257206104695797,
          0.003828193759545684,
          -0.014253838919103146,
          -0.023993264883756638,
          -0.010192261077463627,
          0.018601050600409508,
          0.018782183527946472,
          0.020412389189004898,
          0.0024139564484357834,
          -0.010408228263258934,
          0.026334071531891823,
          -0.001065902877599001,
          -0.029761681333184242,
          0.013807971030473709,
          0.007726054172962904,
          0.0011695323046296835,
          0.00037707184674218297,
          0.009753359481692314,
          -0.009732459671795368,
          -0.005601215176284313,
          -0.020412389189004898,
          0.017012646421790123,
          0.017096245661377907,
          -0.009307491593062878,
          0.024118665605783463,
          0.03349582478404045,
          0.0020900056697428226,
          -0.014866907149553299,
          0.007328953128308058,
          -0.029204346239566803,
          -0.026222603395581245,
          0.018252715468406677,
          -0.01959032006561756,
          -0.003438059240579605,
          0.0040128109976649284,
          0.00180088821798563,
          0.03391382470726967,
          -0.02806180901825428,
          -0.02987314760684967,
          -0.015619308687746525,
          0.0193952526897192,
          0.0065521677024662495,
          0.020119788125157356,
          0.023547397926449776,
          -0.03098781779408455,
          -0.02559560351073742,
          -0.003594809677451849,
          -0.01974358595907688,
          0.01012259442359209,
          -0.034192491322755814,
          0.02382606454193592,
          0.007102536037564278,
          0.000681428937241435,
          -0.01843385025858879,
          -0.6148517727851868,
          -0.03820530325174332,
          -0.009300525300204754,
          -0.028535543009638786,
          0.01698477938771248,
          0.005514131393283606,
          -0.011627398431301117,
          0.03349582478404045,
          -0.024104731157422066,
          0.033579424023628235,
          -0.0008451460162177682,
          -0.023296596482396126,
          -0.0038212270010262728,
          0.007733020931482315,
          -0.010498794727027416,
          -0.012992868199944496,
          0.006503400858491659,
          0.004162594676017761,
          0.00922389142215252,
          0.022335194051265717,
          -0.03500062972307205,
          0.00739165348932147,
          -0.026069337502121925,
          -0.004152144771069288,
          -0.025372669100761414,
          0.0031245583668351173,
          0.006506884470582008,
          -0.021095123142004013,
          -0.009816059842705727,
          0.017151979729533195,
          -0.02045418880879879,
          -0.001589275081641972,
          0.021889325231313705,
          -0.030207548290491104,
          0.025010401383042336,
          0.021471325308084488,
          0.0021439974661916494,
          0.009084558114409447,
          -0.0007767854258418083,
          0.027880676090717316,
          -0.01596764288842678,
          -0.002070847200229764,
          -0.0024035065434873104,
          -0.0031228167936205864,
          0.017360979691147804,
          -0.000540788983926177,
          0.0186428502202034,
          -0.010498794727027416,
          0.006841285154223442,
          -0.004591045901179314,
          -0.008708356879651546,
          -0.003521659644320607,
          -0.007572787348181009,
          -0.002574190264567733,
          0.03550222888588905,
          0.011418397538363934,
          0.0342482253909111,
          -0.0024975568521767855,
          0.0014272996922954917,
          -0.011919998563826084,
          0.008631723001599312,
          0.017305247485637665,
          -0.012303167022764683,
          -0.01918625272810459,
          0.018823985010385513,
          0.009565259329974651,
          -0.006287433672696352,
          0.003291758941486478,
          0.00914725847542286,
          -0.04558999091386795,
          0.0003217737830709666,
          0.015229174867272377,
          0.0004941992810927331,
          -0.015284908004105091,
          0.00554199842736125,
          0.012338000349700451,
          0.0033718757331371307,
          0.011188496835529804,
          -0.002103938953951001,
          0.019757520407438278,
          -0.017444580793380737,
          -0.012247432954609394,
          -0.028842078521847725,
          0.002976516494527459,
          -0.0024052481167018414,
          0.0001037927286233753,
          -0.01738884672522545,
          0.017193779349327087,
          -0.01478330697864294,
          -0.0036505432799458504,
          0.0033718757331371307,
          0.03199101984500885,
          -0.028842078521847725,
          -0.042050912976264954,
          0.004918479826301336,
          0.011376597918570042,
          0.013125235214829445,
          -0.0074613201431930065,
          0.03112715110182762,
          -0.03722996637225151,
          0.014246871694922447,
          -0.020565655082464218,
          0.012351933866739273,
          -0.005886849481612444,
          0.02675207331776619,
          0.03600383177399635,
          -0.007475253660231829,
          0.0011259905295446515,
          -0.025707069784402847,
          0.0035303679760545492,
          -0.005911232437938452,
          -0.010401261039078236,
          -0.0035390763077884912,
          -0.0003302644472569227,
          -0.014212038367986679,
          -0.04040677472949028,
          0.011843365617096424,
          0.01065902877599001,
          0.01631597802042961,
          -0.003516434459015727,
          0.02599967084825039,
          0.01843385025858879,
          0.015521774999797344,
          -0.02886994555592537,
          0.01868464983999729,
          0.012045399285852909,
          0.0024522733874619007,
          0.006210800260305405,
          0.020844323560595512,
          0.02886994555592537,
          -0.011495031416416168,
          0.02172212488949299,
          0.010213160887360573,
          -0.010213160887360573,
          -0.002133547328412533,
          0.01571684330701828,
          0.0012836117530241609,
          -0.007726054172962904,
          0.01097949594259262,
          -0.0035094679333269596,
          -0.02130412496626377,
          -0.008144055493175983,
          -0.00489758001640439,
          -0.012553967535495758,
          -0.028591277077794075,
          -0.015647174790501595,
          -0.021931126713752747,
          -0.0011024780105799437,
          0.0001773239200701937,
          -0.02266959473490715,
          0.011599531397223473,
          -0.018475649878382683,
          -0.02528906799852848,
          -0.017347047105431557,
          -0.023394130170345306,
          -0.014616106636822224,
          -0.011655264534056187,
          -0.032213952392339706,
          -0.008617790415883064,
          -0.029092878103256226,
          0.006754201836884022,
          0.02534480206668377,
          -0.027588075026869774,
          -0.017765048891305923,
          -0.009676726534962654,
          -0.013877637684345245,
          0.0418279804289341,
          0.030792750418186188,
          0.0045875622890889645,
          -0.016190577298402786,
          0.013034668751060963,
          -0.03792663663625717,
          0.0009213441517204046,
          0.02201472595334053,
          0.0011190238874405622,
          0.01848958246409893,
          -0.02605540305376053,
          0.009906627237796783,
          -0.00020399325876496732,
          -0.032576221972703934,
          -0.009857860393822193,
          0.040573976933956146,
          -0.002218889305368066,
          0.006458117626607418,
          0.012233499437570572,
          0.0017199005233123899,
          0.003911793697625399,
          -0.0036261598579585552,
          -0.021025456488132477,
          0.015842242166399956,
          0.011237263679504395,
          0.019534585997462273,
          -0.017904382199048996,
          0.01627417653799057,
          -0.004322828259319067,
          0.02237699367105961,
          0.007948988117277622,
          0.027615942060947418,
          -0.014532506465911865,
          0.018754318356513977,
          0.025163667276501656,
          0.009850893169641495,
          -0.0022711395286023617,
          -0.0011756281601265073,
          -0.006064499728381634,
          -0.00982302613556385,
          -0.0011277322191745043,
          -0.03553009778261185,
          0.023338396102190018,
          0.0015709876315668225,
          0.02220979332923889,
          -0.02906501293182373,
          -0.033217158168554306,
          -0.01804371550679207,
          -0.008206755854189396,
          0.0012374575017020106,
          -0.009084558114409447,
          0.03265982121229172,
          -0.0279642753303051,
          0.00955132581293583,
          -0.006064499728381634,
          -0.012909268029034138,
          0.038706906139850616,
          0.012254400178790092,
          0.006893535144627094,
          0.006043599918484688,
          -0.0029660663567483425,
          -0.00960009265691042,
          0.015521774999797344,
          -0.020342722535133362,
          0.019228052347898483,
          0.02076072245836258,
          0.029956746846437454,
          -0.0038665104657411575,
          -0.015424241311848164,
          -0.010533628053963184,
          0.02630620449781418,
          0.009356258437037468,
          0.03279915452003479,
          0.003523401217535138,
          -0.012156866490840912,
          0.0030531499069184065,
          -0.004110344685614109,
          -0.0040406775660812855,
          0.053977880626916885,
          -0.01005292683839798,
          0.01995258778333664,
          0.030820617452263832,
          -0.01141143124550581,
          -0.0074404203332960606,
          0.017263446003198624,
          -0.011947865597903728,
          -0.017360979691147804,
          0.007551887072622776,
          0.017207713797688484,
          0.0009274400072172284,
          0.024801399558782578,
          0.0003629207785706967,
          -0.011195464059710503,
          0.010011127218604088,
          0.004267094656825066,
          -0.01249823346734047,
          0.02024518884718418,
          0.01787651516497135,
          0.023561330512166023,
          -0.014643972739577293,
          -0.030151814222335815,
          -0.028298676013946533,
          -0.012017532251775265,
          0.00023577875981573015,
          -0.017514247447252274,
          -0.004256644751876593,
          -0.010247994214296341,
          0.006573067978024483,
          0.003876960603520274,
          0.004364628344774246,
          -0.02363099716603756,
          0.01733311451971531,
          0.005022980272769928,
          0.02181965857744217,
          -0.006931852083653212,
          -0.020886123180389404,
          0.028131475672125816,
          -0.0040580942295491695,
          -0.011146697215735912,
          -0.015438174828886986,
          -0.019910788163542747,
          -0.010617229156196117,
          -0.028075743466615677,
          0.017597848549485207,
          0.008882524445652962,
          0.0077887545339763165,
          -0.020273054018616676,
          -0.003695826744660735,
          -0.002946907887235284,
          0.0015431208303198218,
          0.008861623704433441,
          -0.01118153054267168,
          0.013076468370854855,
          0.01125816348940134,
          -0.012045399285852909,
          0.005364347714930773,
          -0.016761844977736473,
          0.014448905363678932,
          0.049909334629774094,
          -0.006653184536844492,
          -0.02172212488949299,
          -0.01985505409538746,
          -0.018768250942230225,
          -0.018670717254281044,
          0.02332446351647377,
          0.010387328453361988,
          -0.019116584211587906,
          0.01153683103621006,
          0.024759599938988686,
          0.02372853085398674,
          -0.009425925090909004,
          0.0016868087695911527,
          0.011919998563826084,
          -0.0067054349929094315,
          -0.017402781173586845,
          -0.03753650188446045,
          -0.029455145820975304,
          0.0005564640159718692,
          0.025804603472352028,
          0.06325750797986984,
          0.0035094679333269596,
          0.0036993101239204407,
          -0.019464919343590736,
          0.0037480767350643873,
          -0.018015848472714424,
          -0.01592584326863289,
          0.0011303446954116225,
          -0.015633242204785347,
          -0.021485257893800735,
          -0.00836002267897129,
          0.0035652013029903173,
          0.0056499820202589035,
          0.02126232348382473,
          0.0009013149538077414,
          0.0013184451963752508,
          -0.05768415704369545,
          0.02559560351073742,
          -0.000957048439886421,
          0.003974494058638811,
          -0.014337439090013504,
          0.020816456526517868,
          0.03461049497127533,
          0.005099613685160875,
          -0.01752818003296852,
          0.01899118535220623,
          0.02509400062263012,
          0.018712516874074936,
          -0.04010024294257164,
          0.0060157328844070435,
          0.006917918566614389,
          -0.008798924274742603,
          0.00041408237302675843,
          -0.013139168731868267,
          0.022599928081035614,
          0.01708231307566166,
          0.006802968215197325,
          0.03380235657095909,
          0.0037202101666480303,
          -0.003915277309715748,
          0.012066299095749855,
          0.008645656518638134,
          -0.006245633587241173,
          -0.0037027932703495026,
          0.00836002267897129,
          -0.0035808763932436705,
          0.018503516912460327,
          0.02282286249101162,
          0.0013524078531190753,
          0.007238386198878288,
          -0.014379238709807396,
          -0.01783471554517746,
          -0.004451712127774954,
          -0.0074891867116093636,
          -0.007384686730802059,
          -0.011982698924839497,
          -0.009405025281012058,
          -0.025261200964450836,
          -0.023254796862602234,
          -0.003335300600156188,
          -0.00876409001648426,
          0.018475649878382683,
          -0.01139053050428629,
          0.009614026173949242,
          -0.005716165527701378,
          -0.003869993845000863,
          -0.017165912315249443,
          -0.013362103141844273,
          -0.0016040793852880597,
          -0.02634800411760807,
          -0.006130683235824108,
          -0.04840452969074249,
          -0.020328788086771965,
          0.023686731234192848,
          0.024397332221269608,
          0.029148612171411514,
          -0.013313336297869682,
          0.005172763951122761,
          -0.00545143149793148,
          -0.010059894062578678,
          -0.008018654771149158,
          0.016538910567760468,
          -0.005611665081232786,
          -0.016455311328172684,
          0.02609720453619957,
          -0.034136757254600525,
          -0.026111137121915817,
          -0.01950671896338463,
          0.02161065861582756,
          -0.011550764553248882,
          0.01779291406273842,
          -0.01468577329069376,
          -0.022237660363316536,
          -0.0067054349929094315,
          0.029287945479154587,
          -0.004845329560339451,
          0.02746267430484295,
          -0.0020969724282622337,
          -0.02605540305376053,
          0.020872190594673157,
          -0.03722996637225151,
          0.0004397720331326127,
          -0.03472195938229561,
          -0.012888368219137192,
          -0.003988427575677633,
          -0.0021770892199128866,
          0.012331033125519753,
          -0.014072705060243607,
          -0.003828193759545684,
          -0.0120941661298275,
          -0.01315310224890709,
          -0.01251913420855999,
          -0.0035025011748075485,
          0.046119458973407745,
          0.013055568560957909,
          0.004500478971749544,
          -0.014797240495681763,
          0.009202991612255573,
          -0.020523855462670326,
          -0.006771618500351906,
          -0.015340641140937805,
          0.02553986944258213,
          0.00311759184114635,
          -0.014866907149553299,
          0.03867903724312782,
          -0.01282566785812378,
          -0.011926965788006783,
          -0.015438174828886986,
          0.035335030406713486,
          -0.0234359297901392,
          0.045617856085300446,
          0.0035460428334772587,
          0.0027309407014399767,
          -0.017500314861536026,
          0.008276422508060932,
          -0.005280747544020414,
          -0.010895895771682262,
          -0.03148941695690155,
          0.018614983186125755,
          0.026361938565969467,
          -0.012303167022764683,
          -0.003533851122483611,
          -0.022530261427164078,
          -0.01612091064453125,
          -0.020091921091079712,
          -0.024271931499242783,
          0.04508838802576065,
          -0.0038978606462478638,
          0.003025283105671406,
          0.007168719545006752,
          0.009363225661218166,
          -0.0028371827211230993,
          -0.0002782320079859346,
          -0.037257835268974304,
          -0.020426321774721146,
          0.0015509583754464984,
          0.0397937074303627,
          0.0193952526897192,
          0.024620266631245613,
          0.04701119288802147,
          -0.011947865597903728,
          0.018127316609025,
          0.020217321813106537,
          -0.0008625627378933132,
          -0.0031837753485888243,
          -0.030346881598234177,
          -0.008234621956944466,
          -0.015995509922504425,
          0.014191138558089733,
          0.007067702244967222,
          -0.0076424540020525455,
          0.006956235505640507,
          0.01985505409538746,
          0.015006240457296371,
          0.011077029630541801,
          -0.003838643664494157,
          -0.0021370307076722383,
          0.0010876738233491778,
          -0.019172318279743195,
          -0.01256790105253458,
          0.01869858428835869,
          -0.018308449536561966,
          0.00748222041875124,
          -0.03380235657095909,
          -0.004695545881986618,
          0.0031559085473418236,
          -0.018865784630179405,
          0.011940899305045605,
          -0.00720355287194252,
          0.019074784591794014,
          -0.019269851967692375,
          0.010282827541232109,
          -0.016998711973428726,
          0.013821904547512531,
          -0.013229736126959324,
          -0.010888929478824139,
          -0.023463796824216843,
          -0.001105090486817062,
          0.01677577942609787,
          0.019966520369052887,
          -0.011620431207120419,
          0.0018583633936941624,
          -0.030514081940054893,
          0.015326708555221558,
          -0.008137088268995285,
          -0.029148612171411514,
          -0.003768976777791977,
          0.001748638111166656,
          0.007879321463406086,
          -0.016817579045891762,
          -0.04946346580982208,
          -0.034638360142707825,
          -0.013362103141844273,
          -0.009634925983846188,
          -0.001558795920573175,
          -0.016399577260017395,
          0.015298841521143913,
          -0.011042196303606033,
          -0.04135424643754959,
          -0.004496995359659195,
          -0.0026560488622635603,
          0.03023541532456875,
          0.022335194051265717,
          0.005200630519539118,
          0.012985901907086372,
          0.030904216691851616,
          0.013389969244599342,
          -0.011007362976670265,
          0.00028258617385290563,
          -0.019019052386283875,
          0.009920559823513031,
          0.011188496835529804,
          -0.023867864161729813,
          -0.0428311824798584,
          0.0299846138805151,
          0.027393007650971413,
          -0.017667515203356743,
          -0.029343679547309875,
          0.015856176614761353,
          -0.018782183527946472,
          -0.001386370393447578,
          0.00997629389166832,
          -0.011425363831222057,
          -0.021122990176081657,
          -0.009035791270434856,
          -0.012170800007879734,
          0.014295638538897038,
          0.01468577329069376,
          -0.005298164207488298,
          -0.017848648130893707,
          0.013013768941164017,
          -0.020119788125157356,
          0.005218047648668289,
          0.010881963185966015,
          -0.010847128927707672,
          0.024007197469472885,
          0.01471363939344883,
          0.022711394354701042,
          0.014267771504819393,
          0.03304995596408844,
          0.026334071531891823,
          -0.030709149315953255,
          0.018378116190433502,
          -0.013452669605612755,
          -0.003040958195924759,
          -0.007224452681839466,
          0.005360864568501711,
          0.004469128791242838,
          0.0304862167686224,
          -0.022711394354701042,
          -0.00927265826612711,
          0.004099894315004349,
          -0.009767292998731136,
          0.017653580754995346,
          0.008729256689548492,
          -0.00025994444149546325,
          0.004911513067781925,
          -0.011522897519171238,
          0.007928088307380676,
          0.034638360142707825,
          0.006123716477304697,
          -0.02176392637193203,
          -0.0007519666105508804,
          -0.0016493628500029445,
          0.0037445933558046818,
          -0.02690533921122551,
          0.009453792124986649,
          0.029510879889130592,
          -0.0418279804289341,
          -0.00914029125124216,
          0.027518408372998238,
          0.011334797367453575,
          0.011049163527786732,
          -0.029148612171411514,
          0.011090963147580624,
          0.013167035765945911,
          0.021025456488132477,
          -0.013410869985818863,
          -0.010206194594502449,
          -0.007503120228648186,
          0.015326708555221558,
          -0.03006821498274803,
          -0.017165912315249443,
          -0.019924720749258995,
          -0.011857299134135246,
          0.0158701092004776,
          0.021039390936493874,
          0.007565820589661598,
          -0.007872354239225388,
          0.010540595278143883,
          -0.029343679547309875,
          0.010241027921438217,
          0.009753359481692314,
          -0.004260128363966942,
          -0.00318725872784853,
          0.014769373461604118,
          -0.0038665104657411575,
          0.010450027883052826,
          0.0176257137209177,
          0.020133720710873604,
          -0.0065626176074147224,
          0.01000415999442339,
          -0.030597683042287827,
          -0.015897976234555244,
          -0.007955954410135746,
          0.004124277736991644,
          -0.00038055519689805806,
          -0.02800607495009899,
          0.006447667255997658,
          0.004775662906467915,
          0.00568133220076561,
          -0.0022397893480956554,
          -0.015354574657976627,
          -0.005329514387995005,
          0.002267656149342656,
          -0.011676165275275707,
          0.00816495530307293,
          0.016566777601838112,
          -0.023310529068112373,
          0.037007033824920654,
          0.0067054349929094315,
          0.04951919987797737,
          0.0018670717254281044,
          -0.01606517657637596,
          -0.03491702675819397,
          -0.026125071570277214,
          -0.017556047067046165,
          -0.026877472177147865,
          -0.03304995596408844,
          -0.025484135374426842,
          0.02790854312479496,
          0.019562453031539917,
          -0.027128273621201515,
          -0.03260408714413643,
          -0.018196983262896538,
          -0.019659986719489098,
          0.00997629389166832,
          0.01646924391388893,
          0.01779291406273842,
          0.010575428605079651,
          0.01617664471268654,
          -0.02841014415025711,
          0.03714636713266373,
          0.010087761096656322,
          -0.0006901372689753771,
          -0.013550203293561935,
          0.012581833638250828,
          0.011397497728466988,
          -0.01612091064453125,
          0.004803529474884272,
          0.02473173290491104,
          -0.04751279577612877,
          -0.0007872354472056031,
          0.03784303739666939,
          -0.006451150868088007,
          0.02403506450355053,
          0.03918064013123512,
          -0.019715720787644386,
          -0.03480556234717369,
          -0.01430957205593586,
          0.018322382122278214,
          0.033022090792655945,
          0.01692904531955719,
          0.003636609762907028,
          -0.0019855054561048746,
          -0.01342480257153511,
          0.0052772643975913525,
          0.013717403635382652,
          -0.026724206283688545,
          0.0027675158344209194,
          0.0054792980663478374,
          0.0037724601570516825,
          -0.02397933229804039,
          -0.017221646383404732,
          0.011676165275275707,
          0.012059332802891731,
          -0.014978373423218727,
          0.020523855462670326,
          -0.004420361947268248,
          -0.02967808023095131,
          0.0049393801018595695,
          0.02679387293756008,
          0.0015971126267686486,
          -0.020900055766105652,
          0.0110839968547225,
          -0.027936408296227455,
          0.0032029335852712393,
          0.02161065861582756,
          0.012958034873008728,
          -0.0052842311561107635,
          -0.002953874645754695,
          0.005691782105714083,
          -0.02573493681848049,
          -0.00224153115414083,
          -0.007691220846027136,
          0.0058694323524832726,
          -0.018461717292666435,
          0.04625879228115082,
          0.02917647920548916,
          -0.05105186998844147,
          0.017458513379096985,
          -0.013780103996396065,
          -0.008833757601678371,
          -0.019032984972000122,
          0.0032691170927137136,
          0.005033430177718401,
          -0.021136924624443054,
          0.02101152390241623,
          0.0037480767350643873,
          -0.03505636006593704,
          -0.0019611220341175795,
          -0.0012052366510033607,
          0.008680489845573902,
          -0.01047789491713047,
          0.02574886940419674,
          0.2039845585823059,
          -0.03405315801501274,
          0.01833631657063961,
          0.0029573580250144005,
          0.013780103996396065,
          0.0176257137209177,
          0.0037550434935837984,
          0.017709314823150635,
          -0.00217186426743865,
          -0.001844429993070662,
          0.00048462007544003427,
          0.008060455322265625,
          -0.013222768902778625,
          0.0024365982972085476,
          0.0224187932908535,
          0.0010824487544596195,
          -0.012066299095749855,
          -0.046175193041563034,
          -0.00687611848115921,
          -0.0056778485886752605,
          -0.005225013941526413,
          0.014699706807732582,
          -0.003213383723050356,
          -0.036505430936813354,
          0.014351372607052326,
          -0.012916235253214836,
          -0.006691501475870609,
          -0.026919273659586906,
          0.006419800687581301,
          0.03472195938229561,
          -0.007377719972282648,
          0.01858711615204811,
          -0.008241589181125164,
          0.015479975380003452,
          -0.009690659120678902,
          -0.020384522154927254,
          -0.02272532880306244,
          0.02222372591495514,
          0.011056129820644855,
          -0.009425925090909004,
          0.035084228962659836,
          0.010401261039078236,
          -0.008025621995329857,
          -0.012331033125519753,
          -0.00031154145835898817,
          -0.00023947982117533684,
          0.000322644627885893,
          -0.013814937323331833,
          -0.002095230622217059,
          0.03023541532456875,
          -0.010512728244066238,
          0.02654307149350643,
          0.039124906063079834,
          0.042691849172115326,
          0.0030461831483989954,
          -0.014267771504819393,
          0.002694365568459034,
          0.00595651613548398,
          0.005886849481612444,
          0.01949278637766838,
          -0.025762803852558136,
          0.0186428502202034,
          0.002546323463320732,
          0.03112715110182762,
          0.03179595246911049,
          0.015856176614761353,
          -0.019882921129465103,
          0.012393733486533165,
          0.015730775892734528,
          -0.004723412916064262,
          -0.015382441692054272,
          -0.016817579045891762,
          -0.005925165954977274,
          -0.004653745796531439,
          -0.03502849489450455,
          -0.02126232348382473,
          0.051414139568805695,
          0.01818304881453514,
          0.009565259329974651,
          0.034081026911735535,
          -0.018754318356513977,
          -0.005834599025547504,
          0.011369630694389343,
          -0.014894773252308369,
          -0.026779938489198685,
          -0.043054115027189255,
          0.017305247485637665,
          -0.01395427156239748,
          -0.017946181818842888,
          -0.016483178362250328,
          0.0023791231215000153,
          -0.021081190556287766,
          -0.008959157392382622,
          0.008617790415883064,
          -0.0004689450142905116,
          -0.010735662654042244,
          -0.019172318279743195,
          0.03940357267856598,
          -0.0014569080667570233,
          -0.021596726030111313,
          -0.03452689200639725,
          0.04703906178474426,
          0.03842823579907417,
          0.02644553780555725,
          -0.0008155376417562366,
          0.004946346860378981,
          -0.01019922737032175,
          -0.008157989010214806,
          0.022502394393086433,
          -0.0013027702225372195,
          0.001785213127732277,
          -0.012386767193675041,
          0.0019053885480389,
          0.007997754961252213,
          -0.0018931968370452523,
          -0.011327831074595451,
          0.005221530795097351,
          -0.014393172226846218,
          0.012637567706406116,
          -0.005221530795097351,
          0.0005899911629967391,
          -0.03388595953583717,
          -0.039821576327085495,
          0.017946181818842888,
          -0.0014830331783741713,
          -0.004685095977038145,
          -0.016413511708378792,
          -0.02619473822414875,
          -0.01430957205593586,
          -0.007405586540699005,
          0.011327831074595451,
          -0.03265982121229172,
          0.03530716150999069,
          -0.021582791581749916,
          -0.02906501293182373,
          0.00651036761701107,
          0.01950671896338463,
          -0.017542114481329918,
          -0.01779291406273842,
          0.009941460564732552,
          0.009460759349167347,
          -5.8073856052942574e-05,
          0.006141133140772581,
          -0.006984102074056864,
          0.01627417653799057,
          -0.0468439944088459,
          0.0076215541921556,
          -0.014434972777962685,
          -0.0014351372374221683,
          -0.03625463321805,
          -0.01478330697864294,
          -0.007649420760571957,
          0.0015466042095795274,
          -0.019311651587486267,
          0.023463796824216843,
          -0.002380864694714546,
          -0.018670717254281044,
          -0.015159507282078266,
          -0.013264569453895092,
          0.0021109057124704123,
          -0.028786344453692436,
          -0.015591442584991455,
          -0.0022641727700829506,
          -0.025219401344656944,
          -0.009077590890228748,
          -0.009579192847013474,
          -0.17745541036128998,
          0.010847128927707672,
          0.02076072245836258,
          -0.032074619084596634,
          -0.005552448332309723,
          0.02711433917284012,
          0.032269686460494995,
          -0.009850893169641495,
          -0.020482055842876434,
          0.003523401217535138,
          -0.0010632904013618827,
          -0.017012646421790123,
          -0.02856341004371643,
          -0.0007136498461477458,
          0.004350695293396711,
          0.01306950207799673,
          0.0018235299503430724,
          0.02045418880879879,
          0.024898933246731758,
          0.02825687639415264,
          0.03143368661403656,
          -0.044614654034376144,
          0.013536269776523113,
          -0.007321986369788647,
          0.02064925618469715,
          -0.009767292998731136,
          -0.010171361267566681,
          0.03137795254588127,
          -0.017151979729533195,
          -0.014671839773654938,
          0.012128999456763268,
          0.02725367434322834,
          0.012101132422685623,
          -0.004552728962153196,
          0.00025515485322102904,
          0.020886123180389404,
          0.01123029738664627,
          -0.01334816962480545,
          -0.0011651781387627125,
          -0.008415755815804005,
          0.05252880975604057,
          -0.00992752704769373,
          -0.0076145874336361885,
          -0.014121471904218197,
          -0.023644929751753807,
          -0.011097930371761322,
          0.007670320570468903,
          -0.0022746229078620672,
          0.016009442508220673,
          0.018364183604717255,
          -0.01748638041317463,
          -0.020384522154927254,
          0.020635321736335754,
          0.006398900877684355,
          0.007005002349615097,
          0.02679387293756008,
          0.011662231758236885,
          0.00821372214704752,
          -0.026166871190071106,
          0.015535708516836166,
          0.004977697040885687,
          -0.033022090792655945,
          0.026013603433966637,
          0.002319906372576952,
          -0.014978373423218727,
          -0.0016868087695911527,
          0.0018479133723303676,
          0.004333278629928827,
          -0.021986858919262886,
          0.011202430352568626,
          -0.03647756576538086,
          -0.023463796824216843,
          0.0003302644472569227,
          0.0047408295795321465,
          0.022599928081035614,
          -0.00012757742661051452,
          -0.019367385655641556,
          -0.0012609701370820403,
          0.001722512999549508,
          -0.01123029738664627,
          -0.033022090792655945,
          0.0317402184009552,
          -0.0096976263448596,
          0.026975005865097046,
          0.0032081587705761194,
          -0.012087199836969376,
          0.005521098151803017,
          0.018322382122278214,
          -0.019715720787644386,
          -0.026431605219841003,
          0.011432331055402756,
          0.0060226996429264545,
          0.017542114481329918,
          0.0019785386975854635,
          0.007412553299218416,
          0.01779291406273842,
          0.010220128111541271,
          -0.015284908004105091,
          0.022181926295161247,
          -0.012853534892201424,
          -0.02105332352221012,
          -0.017347047105431557,
          0.006381483748555183,
          0.009307491593062878,
          0.012783868238329887,
          0.02533086948096752,
          -0.019618187099695206,
          0.006694984622299671,
          0.029037145897746086,
          0.011481097899377346,
          -0.006043599918484688,
          -0.010833196341991425,
          0.00023664960463065654,
          0.02629227191209793,
          0.017820781096816063,
          0.009133324958384037,
          -0.03193528577685356,
          -0.0027918992564082146,
          0.005256364122033119,
          -0.02599967084825039,
          0.043499983847141266,
          -0.010603295639157295,
          0.003612226340919733,
          0.014978373423218727,
          -0.0007920250645838678,
          -0.01804371550679207,
          -0.11425364017486572,
          -0.008206755854189396,
          -0.002555032027885318,
          0.004354178439825773,
          -0.014490705914795399,
          -0.004120794590562582,
          0.02172212488949299,
          0.029483012855052948,
          -0.026027537882328033,
          0.010624195449054241,
          -0.020579589530825615,
          -0.02711433917284012,
          0.013090401887893677,
          0.004803529474884272,
          -0.005319064483046532,
          0.008185855112969875,
          0.0033701341599226,
          -0.0047582462430000305,
          -0.009251758456230164,
          0.026682404801249504,
          -0.0026473405305296183,
          -0.01959032006561756,
          0.014393172226846218,
          -0.024048998951911926,
          0.015842242166399956,
          0.00677858479321003,
          -0.029705947265028954,
          0.0015788250602781773,
          0.021889325231313705,
          0.011028262786567211,
          0.007398620247840881,
          -0.03455476090312004,
          -0.00881285686045885,
          -0.003410192672163248,
          -0.0019837636500597,
          -0.003326592268422246,
          -0.012909268029034138,
          -0.022558128461241722,
          0.005782349035143852,
          -0.03405315801501274,
          0.0029051078017801046,
          0.013292435556650162,
          -0.004169561434537172,
          -0.010157427750527859,
          -0.010136527009308338,
          -0.00012942795001436025,
          -0.014434972777962685,
          0.030151814222335815,
          0.023770330473780632,
          -0.00803258828818798,
          -0.012198666110634804,
          -0.02045418880879879,
          -0.02499646693468094,
          -0.005367831327021122,
          0.020941857248544693,
          0.033133555203676224,
          0.0186428502202034,
          0.013598970137536526,
          -0.004517895635217428,
          -0.012965002097189426,
          -0.000530774355866015,
          -0.005092646926641464,
          -0.01698477938771248,
          0.005144897382706404,
          0.018628917634487152,
          -0.001567504252307117,
          -0.05473028123378754,
          -0.011167597025632858,
          -0.008673523552715778,
          -0.005350414663553238,
          -0.014156305231153965,
          0.01793224923312664,
          0.013194902800023556,
          0.02155492454767227,
          -0.01858711615204811,
          -0.01012259442359209,
          -0.014281705021858215,
          -0.0021091641392558813,
          0.0022066975943744183,
          -0.018726451322436333,
          -0.01090286299586296,
          -0.0274487417191267,
          0.007698187604546547,
          -0.002624698681756854,
          -0.010220128111541271,
          -0.012804768048226833,
          -0.0012182991486042738,
          -0.02499646693468094,
          -0.011745831929147243,
          -0.029761681333184242,
          0.009662793017923832,
          0.008645656518638134,
          -0.029761681333184242,
          -0.008234621956944466,
          -0.004180011339485645,
          -0.0024644650984555483,
          -0.0031994504388421774,
          -0.01390550471842289,
          0.020217321813106537,
          0.005987866315990686,
          -0.015075907111167908,
          0.0043750787153840065,
          -0.04391798377037048,
          0.016162710264325142,
          0.0032952423207461834,
          -0.01698477938771248,
          0.011655264534056187,
          0.018823985010385513,
          0.03692343458533287,
          -0.017040513455867767,
          0.002717007417231798,
          0.008854657411575317,
          -0.007363786455243826,
          0.021485257893800735,
          -0.01738884672522545,
          -0.018977250903844833,
          -0.006447667255997658,
          -0.010993429459631443,
          0.005925165954977274,
          -0.004507445730268955,
          0.029204346239566803,
          0.005803248845040798,
          -0.01499230694025755,
          -0.016538910567760468,
          0.009572225622832775,
          -0.00025798508431762457,
          -0.007774821016937494,
          0.013828870840370655,
          -0.010234060697257519,
          0.026320137083530426,
          -0.026612738147377968,
          -0.011780665256083012,
          0.020077988505363464,
          -0.02403506450355053,
          -0.0008969607879407704,
          0.030040347948670387,
          -0.0010589362354949117,
          0.016664311289787292,
          0.02423013187944889,
          0.008596889674663544,
          0.03622676432132721,
          0.004639812745153904,
          -0.014755439944565296,
          -0.03600383177399635,
          0.0020307889208197594,
          -0.012540034018456936,
          -0.00260728201828897,
          0.022251592949032784,
          -0.0027222323697060347,
          0.003500759368762374,
          0.015479975380003452,
          -0.006398900877684355,
          0.008778023533523083,
          0.02091399021446705,
          0.019074784591794014,
          -0.029009278863668442,
          -0.015953710302710533,
          -0.00849238969385624,
          -0.0035286261700093746,
          0.02278106100857258,
          0.002276364481076598,
          -0.0034415426198393106,
          0.0367841012775898,
          -0.015048041008412838,
          0.022279459983110428,
          0.003981460817158222,
          0.0076424540020525455,
          -0.008750157430768013,
          0.012428566813468933,
          0.007039835676550865,
          -0.02912074513733387,
          -0.02827080897986889,
          -0.010568462312221527,
          0.009251758456230164,
          -0.011773698963224888,
          0.02565133571624756,
          0.029037145897746086,
          0.005252880975604057,
          -0.0007088602287694812,
          0.0114601980894804,
          -0.005998316220939159,
          0.01478330697864294,
          0.012484300881624222,
          -0.02459239959716797,
          -0.016845446079969406,
          -0.014448905363678932,
          0.024453066289424896,
          0.002779707545414567,
          -0.004451712127774954,
          0.012170800007879734,
          -0.005378281231969595,
          0.02195899188518524,
          0.012184732593595982,
          0.001861846656538546,
          0.009760326705873013,
          0.03555796295404434,
          0.00401629414409399,
          0.01490870676934719,
          0.007349853403866291,
          -0.019269851967692375,
          0.006796001922339201,
          -0.021749991923570633,
          0.006127200089395046,
          0.015828309580683708,
          -0.012463400140404701,
          -0.010094727389514446,
          -0.023157263174653053,
          0.021136924624443054,
          -0.013229736126959324,
          -0.00457362923771143,
          0.003953594248741865,
          0.03770370036363602,
          -0.0011442780960351229,
          0.015298841521143913,
          -0.01219169981777668,
          0.01080532930791378,
          -0.034276094287633896,
          0.02765774168074131,
          -0.007210519630461931,
          -0.036505430936813354,
          -0.026570938527584076,
          0.033022090792655945,
          0.021931126713752747,
          -0.01518737431615591,
          0.044558919966220856,
          -0.012212599627673626,
          0.018559250980615616,
          0.010819262824952602,
          0.024717800319194794,
          -0.011906065978109837,
          0.02638980560004711,
          0.02278106100857258,
          0.017597848549485207,
          -0.008680489845573902,
          0.01989685371518135,
          -0.01874038390815258,
          -0.021136924624443054,
          -0.012310133315622807,
          -0.006607901304960251,
          0.03279915452003479,
          -0.0008817211491987109,
          0.10160214453935623,
          0.021248390898108482,
          -0.0030949499923735857,
          0.012115065939724445,
          -0.015702908858656883,
          0.011871231719851494,
          0.013362103141844273,
          0.018921518698334694,
          -0.011474130675196648,
          -0.0010545820696279407,
          0.006339683663100004,
          -0.015591442584991455,
          -0.013689536601305008,
          -0.02423013187944889,
          0.0035355929285287857,
          -0.015605375170707703,
          0.003765493631362915,
          0.008617790415883064,
          -0.008610823191702366,
          -0.0030200581531971693,
          0.024299798533320427,
          -0.00043737722444348037,
          0.011223330162465572,
          0.01602337695658207,
          -0.013118268921971321,
          -0.009669759310781956,
          0.007018935866653919,
          -0.016455311328172684,
          -0.008673523552715778,
          -0.03112715110182762,
          0.004406428430229425,
          -0.016566777601838112,
          -0.04166077822446823,
          -0.013571103103458881,
          0.02815934270620346,
          0.01097949594259262,
          -0.001757346442900598,
          0.013341202400624752,
          0.014121471904218197,
          5.785614484921098e-05,
          0.009293558076024055,
          0.036449696868658066,
          -0.0029138161335140467,
          -0.00964885950088501,
          -0.013647736981511116,
          -0.007997754961252213,
          0.0014725831570103765,
          -0.013731337152421474,
          -0.02312939614057541
        ],
        "existingAnswer": false,
        "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
        "indexType": "cogsearchvs",
        "jsonAnswer": null,
        "llm": "AzureChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.3, model_kwargs={}, openai_api_key='1361532858cb428c8da412a73105de78', openai_api_base='https://dataaiapim.azure-api.net', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=1000, tiktoken_model_name=None, deployment_name='chat16k', model_version='', openai_api_type='azure', openai_api_version='2023-07-01-preview')",
        "modifiedAnswer": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B.",
        "overrides": {
          "chainType": "stuff",
          "deploymentType": "gpt3516k",
          "embeddingModelType": "azureopenai",
          "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
          "searchType": "hybridrerank",
          "semantic_captions": false,
          "semantic_ranker": true,
          "temperature": 0,
          "tokenLength": 1000,
          "top": 3
        },
        "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
        "question": "How are sentence pairs represented in the input to BERT?",
        "retrievedDocs": "[Document(page_content='A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.', metadata={'id': 'Bert_pdf-9', 'source': 'Bert.pdf'}), Document(page_content='2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', metadata={'id': 'Bert_pdf-2', 'source': 'Bert.pdf'}), Document(page_content='BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', metadata={'id': 'Bert_pdf-1', 'source': 'Bert.pdf'})]"
      },
      "output": {
        "values": [
          {
            "recordId": 0,
            "data": {
              "data_points": [
                "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.",
                "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008)."
              ],
              "answer": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B.",
              "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).']<br>        Question: How are sentence pairs represented in the input to BERT?<br>        ",
              "sources": "\nBert.pdf",
              "nextQuestions": "<What are the two unsupervised tasks used in pre-training BERT?>\n<What is the difference between the feature-based and fine-tuning approaches for applying pre-trained language representations to downstream tasks?>\n<What is the architecture of BERT and how does it differ from other models like OpenAI GPT?>",
              "error": ""
            }
          }
        ]
      },
      "metrics": null,
      "error": null,
      "parent_run_id": "2670599d-496c-4774-b25c-36cdd8be92c7_0",
      "start_time": "2023-09-20T16:48:34.150503Z",
      "end_time": "2023-09-20T16:48:36.372012Z",
      "index": 0,
      "api_calls": [
        {
          "name": "generateFollowupQuestions",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "embeddedQuestion": [
              -0.011118830181658268,
              -0.01631597802042961,
              0.02559560351073742,
              -0.002060397295281291,
              -0.005500198341906071,
              0.031210750341415405,
              0.015424241311848164,
              -0.011599531397223473,
              -0.01773718185722828,
              -0.027727408334612846,
              0.028535543009638786,
              0.04366718605160713,
              -0.035139963030815125,
              0.005427048075944185,
              -0.0009474692051298916,
              0.008422723039984703,
              0.011244230903685093,
              0.009614026173949242,
              0.027797074988484383,
              -0.016956912353634834,
              -0.0016798420110717416,
              -0.006102816667407751,
              -0.016552845016121864,
              -0.0035739096347242594,
              -0.010742628946900368,
              -0.00849238969385624,
              0.029538746923208237,
              -0.020328788086771965,
              -0.003713243408128619,
              -0.009676726534962654,
              0.0021788307931274176,
              0.02357526309788227,
              -0.017918314784765244,
              -0.009948426857590675,
              -0.009258724749088287,
              0.006694984622299671,
              -0.0029294912237674,
              -0.016915112733840942,
              0.02438339963555336,
              -0.023212995380163193,
              0.0071164690889418125,
              0.021694257855415344,
              0.003977977205067873,
              -0.015270974487066269,
              -0.005953032989054918,
              0.01672004535794258,
              0.010652062483131886,
              -0.0065695843659341335,
              -0.012059332802891731,
              0.0093214251101017,
              0.00438204500824213,
              0.015856176614761353,
              -0.03945930674672127,
              -0.020997589454054832,
              -0.02251632697880268,
              -0.0034084508661180735,
              -0.002352998126298189,
              0.032966356724500656,
              -0.0272118728607893,
              -0.013543237000703812,
              0.0005050846957601607,
              -0.008130121976137161,
              0.0009370191837660968,
              0.008053488098084927,
              -0.022404860705137253,
              -0.024466998875141144,
              -0.008548122830688953,
              0.008018654771149158,
              0.019437052309513092,
              0.007398620247840881,
              0.01959032006561756,
              0.010038994252681732,
              0.0009866568725556135,
              -0.001406399649567902,
              0.0428033173084259,
              -0.01388460397720337,
              0.002535873558372259,
              -0.014755439944565296,
              0.001035423600114882,
              0.03592022880911827,
              0.012735101394355297,
              -0.002624698681756854,
              -0.0010606779251247644,
              0.026487339287996292,
              0.015897976234555244,
              -0.012742067687213421,
              0.010436095297336578,
              0.025275135412812233,
              0.002154447603970766,
              0.0040302276611328125,
              0.027379073202610016,
              0.03536289557814598,
              0.012944101355969906,
              0.0067054349929094315,
              -0.0038839271292090416,
              0.012247432954609394,
              -0.01118153054267168,
              0.038818370550870895,
              0.02634800411760807,
              -0.009523458778858185,
              -0.010157427750527859,
              0.008513289503753185,
              -0.04497692361474037,
              -0.005552448332309723,
              -0.024606334045529366,
              0.014852973632514477,
              0.010749596171081066,
              -0.01130693033337593,
              0.003262150567024946,
              0.001316703506745398,
              -0.019074784591794014,
              0.030207548290491104,
              0.006221250165253878,
              -0.024216199293732643,
              -0.0005220660241320729,
              -0.002276364481076598,
              0.015479975380003452,
              -0.039319973438978195,
              -0.009669759310781956,
              0.012602734379470348,
              0.017165912315249443,
              -0.004075510893017054,
              0.010721729137003422,
              -0.034582626074552536,
              0.03288275748491287,
              -0.003422384150326252,
              -0.010819262824952602,
              -0.03380235657095909,
              -0.006520817521959543,
              -0.04288691654801369,
              0.027420874685049057,
              0.022641727700829506,
              0.015939775854349136,
              -0.022753195837140083,
              0.005636048503220081,
              0.01854531653225422,
              -0.02760200761258602,
              -0.02050992287695408,
              -0.01959032006561756,
              -0.004643295891582966,
              0.02257206104695797,
              0.003828193759545684,
              -0.014253838919103146,
              -0.023993264883756638,
              -0.010192261077463627,
              0.018601050600409508,
              0.018782183527946472,
              0.020412389189004898,
              0.0024139564484357834,
              -0.010408228263258934,
              0.026334071531891823,
              -0.001065902877599001,
              -0.029761681333184242,
              0.013807971030473709,
              0.007726054172962904,
              0.0011695323046296835,
              0.00037707184674218297,
              0.009753359481692314,
              -0.009732459671795368,
              -0.005601215176284313,
              -0.020412389189004898,
              0.017012646421790123,
              0.017096245661377907,
              -0.009307491593062878,
              0.024118665605783463,
              0.03349582478404045,
              0.0020900056697428226,
              -0.014866907149553299,
              0.007328953128308058,
              -0.029204346239566803,
              -0.026222603395581245,
              0.018252715468406677,
              -0.01959032006561756,
              -0.003438059240579605,
              0.0040128109976649284,
              0.00180088821798563,
              0.03391382470726967,
              -0.02806180901825428,
              -0.02987314760684967,
              -0.015619308687746525,
              0.0193952526897192,
              0.0065521677024662495,
              0.020119788125157356,
              0.023547397926449776,
              -0.03098781779408455,
              -0.02559560351073742,
              -0.003594809677451849,
              -0.01974358595907688,
              0.01012259442359209,
              -0.034192491322755814,
              0.02382606454193592,
              0.007102536037564278,
              0.000681428937241435,
              -0.01843385025858879,
              -0.6148517727851868,
              -0.03820530325174332,
              -0.009300525300204754,
              -0.028535543009638786,
              0.01698477938771248,
              0.005514131393283606,
              -0.011627398431301117,
              0.03349582478404045,
              -0.024104731157422066,
              0.033579424023628235,
              -0.0008451460162177682,
              -0.023296596482396126,
              -0.0038212270010262728,
              0.007733020931482315,
              -0.010498794727027416,
              -0.012992868199944496,
              0.006503400858491659,
              0.004162594676017761,
              0.00922389142215252,
              0.022335194051265717,
              -0.03500062972307205,
              0.00739165348932147,
              -0.026069337502121925,
              -0.004152144771069288,
              -0.025372669100761414,
              0.0031245583668351173,
              0.006506884470582008,
              -0.021095123142004013,
              -0.009816059842705727,
              0.017151979729533195,
              -0.02045418880879879,
              -0.001589275081641972,
              0.021889325231313705,
              -0.030207548290491104,
              0.025010401383042336,
              0.021471325308084488,
              0.0021439974661916494,
              0.009084558114409447,
              -0.0007767854258418083,
              0.027880676090717316,
              -0.01596764288842678,
              -0.002070847200229764,
              -0.0024035065434873104,
              -0.0031228167936205864,
              0.017360979691147804,
              -0.000540788983926177,
              0.0186428502202034,
              -0.010498794727027416,
              0.006841285154223442,
              -0.004591045901179314,
              -0.008708356879651546,
              -0.003521659644320607,
              -0.007572787348181009,
              -0.002574190264567733,
              0.03550222888588905,
              0.011418397538363934,
              0.0342482253909111,
              -0.0024975568521767855,
              0.0014272996922954917,
              -0.011919998563826084,
              0.008631723001599312,
              0.017305247485637665,
              -0.012303167022764683,
              -0.01918625272810459,
              0.018823985010385513,
              0.009565259329974651,
              -0.006287433672696352,
              0.003291758941486478,
              0.00914725847542286,
              -0.04558999091386795,
              0.0003217737830709666,
              0.015229174867272377,
              0.0004941992810927331,
              -0.015284908004105091,
              0.00554199842736125,
              0.012338000349700451,
              0.0033718757331371307,
              0.011188496835529804,
              -0.002103938953951001,
              0.019757520407438278,
              -0.017444580793380737,
              -0.012247432954609394,
              -0.028842078521847725,
              0.002976516494527459,
              -0.0024052481167018414,
              0.0001037927286233753,
              -0.01738884672522545,
              0.017193779349327087,
              -0.01478330697864294,
              -0.0036505432799458504,
              0.0033718757331371307,
              0.03199101984500885,
              -0.028842078521847725,
              -0.042050912976264954,
              0.004918479826301336,
              0.011376597918570042,
              0.013125235214829445,
              -0.0074613201431930065,
              0.03112715110182762,
              -0.03722996637225151,
              0.014246871694922447,
              -0.020565655082464218,
              0.012351933866739273,
              -0.005886849481612444,
              0.02675207331776619,
              0.03600383177399635,
              -0.007475253660231829,
              0.0011259905295446515,
              -0.025707069784402847,
              0.0035303679760545492,
              -0.005911232437938452,
              -0.010401261039078236,
              -0.0035390763077884912,
              -0.0003302644472569227,
              -0.014212038367986679,
              -0.04040677472949028,
              0.011843365617096424,
              0.01065902877599001,
              0.01631597802042961,
              -0.003516434459015727,
              0.02599967084825039,
              0.01843385025858879,
              0.015521774999797344,
              -0.02886994555592537,
              0.01868464983999729,
              0.012045399285852909,
              0.0024522733874619007,
              0.006210800260305405,
              0.020844323560595512,
              0.02886994555592537,
              -0.011495031416416168,
              0.02172212488949299,
              0.010213160887360573,
              -0.010213160887360573,
              -0.002133547328412533,
              0.01571684330701828,
              0.0012836117530241609,
              -0.007726054172962904,
              0.01097949594259262,
              -0.0035094679333269596,
              -0.02130412496626377,
              -0.008144055493175983,
              -0.00489758001640439,
              -0.012553967535495758,
              -0.028591277077794075,
              -0.015647174790501595,
              -0.021931126713752747,
              -0.0011024780105799437,
              0.0001773239200701937,
              -0.02266959473490715,
              0.011599531397223473,
              -0.018475649878382683,
              -0.02528906799852848,
              -0.017347047105431557,
              -0.023394130170345306,
              -0.014616106636822224,
              -0.011655264534056187,
              -0.032213952392339706,
              -0.008617790415883064,
              -0.029092878103256226,
              0.006754201836884022,
              0.02534480206668377,
              -0.027588075026869774,
              -0.017765048891305923,
              -0.009676726534962654,
              -0.013877637684345245,
              0.0418279804289341,
              0.030792750418186188,
              0.0045875622890889645,
              -0.016190577298402786,
              0.013034668751060963,
              -0.03792663663625717,
              0.0009213441517204046,
              0.02201472595334053,
              0.0011190238874405622,
              0.01848958246409893,
              -0.02605540305376053,
              0.009906627237796783,
              -0.00020399325876496732,
              -0.032576221972703934,
              -0.009857860393822193,
              0.040573976933956146,
              -0.002218889305368066,
              0.006458117626607418,
              0.012233499437570572,
              0.0017199005233123899,
              0.003911793697625399,
              -0.0036261598579585552,
              -0.021025456488132477,
              0.015842242166399956,
              0.011237263679504395,
              0.019534585997462273,
              -0.017904382199048996,
              0.01627417653799057,
              -0.004322828259319067,
              0.02237699367105961,
              0.007948988117277622,
              0.027615942060947418,
              -0.014532506465911865,
              0.018754318356513977,
              0.025163667276501656,
              0.009850893169641495,
              -0.0022711395286023617,
              -0.0011756281601265073,
              -0.006064499728381634,
              -0.00982302613556385,
              -0.0011277322191745043,
              -0.03553009778261185,
              0.023338396102190018,
              0.0015709876315668225,
              0.02220979332923889,
              -0.02906501293182373,
              -0.033217158168554306,
              -0.01804371550679207,
              -0.008206755854189396,
              0.0012374575017020106,
              -0.009084558114409447,
              0.03265982121229172,
              -0.0279642753303051,
              0.00955132581293583,
              -0.006064499728381634,
              -0.012909268029034138,
              0.038706906139850616,
              0.012254400178790092,
              0.006893535144627094,
              0.006043599918484688,
              -0.0029660663567483425,
              -0.00960009265691042,
              0.015521774999797344,
              -0.020342722535133362,
              0.019228052347898483,
              0.02076072245836258,
              0.029956746846437454,
              -0.0038665104657411575,
              -0.015424241311848164,
              -0.010533628053963184,
              0.02630620449781418,
              0.009356258437037468,
              0.03279915452003479,
              0.003523401217535138,
              -0.012156866490840912,
              0.0030531499069184065,
              -0.004110344685614109,
              -0.0040406775660812855,
              0.053977880626916885,
              -0.01005292683839798,
              0.01995258778333664,
              0.030820617452263832,
              -0.01141143124550581,
              -0.0074404203332960606,
              0.017263446003198624,
              -0.011947865597903728,
              -0.017360979691147804,
              0.007551887072622776,
              0.017207713797688484,
              0.0009274400072172284,
              0.024801399558782578,
              0.0003629207785706967,
              -0.011195464059710503,
              0.010011127218604088,
              0.004267094656825066,
              -0.01249823346734047,
              0.02024518884718418,
              0.01787651516497135,
              0.023561330512166023,
              -0.014643972739577293,
              -0.030151814222335815,
              -0.028298676013946533,
              -0.012017532251775265,
              0.00023577875981573015,
              -0.017514247447252274,
              -0.004256644751876593,
              -0.010247994214296341,
              0.006573067978024483,
              0.003876960603520274,
              0.004364628344774246,
              -0.02363099716603756,
              0.01733311451971531,
              0.005022980272769928,
              0.02181965857744217,
              -0.006931852083653212,
              -0.020886123180389404,
              0.028131475672125816,
              -0.0040580942295491695,
              -0.011146697215735912,
              -0.015438174828886986,
              -0.019910788163542747,
              -0.010617229156196117,
              -0.028075743466615677,
              0.017597848549485207,
              0.008882524445652962,
              0.0077887545339763165,
              -0.020273054018616676,
              -0.003695826744660735,
              -0.002946907887235284,
              0.0015431208303198218,
              0.008861623704433441,
              -0.01118153054267168,
              0.013076468370854855,
              0.01125816348940134,
              -0.012045399285852909,
              0.005364347714930773,
              -0.016761844977736473,
              0.014448905363678932,
              0.049909334629774094,
              -0.006653184536844492,
              -0.02172212488949299,
              -0.01985505409538746,
              -0.018768250942230225,
              -0.018670717254281044,
              0.02332446351647377,
              0.010387328453361988,
              -0.019116584211587906,
              0.01153683103621006,
              0.024759599938988686,
              0.02372853085398674,
              -0.009425925090909004,
              0.0016868087695911527,
              0.011919998563826084,
              -0.0067054349929094315,
              -0.017402781173586845,
              -0.03753650188446045,
              -0.029455145820975304,
              0.0005564640159718692,
              0.025804603472352028,
              0.06325750797986984,
              0.0035094679333269596,
              0.0036993101239204407,
              -0.019464919343590736,
              0.0037480767350643873,
              -0.018015848472714424,
              -0.01592584326863289,
              0.0011303446954116225,
              -0.015633242204785347,
              -0.021485257893800735,
              -0.00836002267897129,
              0.0035652013029903173,
              0.0056499820202589035,
              0.02126232348382473,
              0.0009013149538077414,
              0.0013184451963752508,
              -0.05768415704369545,
              0.02559560351073742,
              -0.000957048439886421,
              0.003974494058638811,
              -0.014337439090013504,
              0.020816456526517868,
              0.03461049497127533,
              0.005099613685160875,
              -0.01752818003296852,
              0.01899118535220623,
              0.02509400062263012,
              0.018712516874074936,
              -0.04010024294257164,
              0.0060157328844070435,
              0.006917918566614389,
              -0.008798924274742603,
              0.00041408237302675843,
              -0.013139168731868267,
              0.022599928081035614,
              0.01708231307566166,
              0.006802968215197325,
              0.03380235657095909,
              0.0037202101666480303,
              -0.003915277309715748,
              0.012066299095749855,
              0.008645656518638134,
              -0.006245633587241173,
              -0.0037027932703495026,
              0.00836002267897129,
              -0.0035808763932436705,
              0.018503516912460327,
              0.02282286249101162,
              0.0013524078531190753,
              0.007238386198878288,
              -0.014379238709807396,
              -0.01783471554517746,
              -0.004451712127774954,
              -0.0074891867116093636,
              -0.007384686730802059,
              -0.011982698924839497,
              -0.009405025281012058,
              -0.025261200964450836,
              -0.023254796862602234,
              -0.003335300600156188,
              -0.00876409001648426,
              0.018475649878382683,
              -0.01139053050428629,
              0.009614026173949242,
              -0.005716165527701378,
              -0.003869993845000863,
              -0.017165912315249443,
              -0.013362103141844273,
              -0.0016040793852880597,
              -0.02634800411760807,
              -0.006130683235824108,
              -0.04840452969074249,
              -0.020328788086771965,
              0.023686731234192848,
              0.024397332221269608,
              0.029148612171411514,
              -0.013313336297869682,
              0.005172763951122761,
              -0.00545143149793148,
              -0.010059894062578678,
              -0.008018654771149158,
              0.016538910567760468,
              -0.005611665081232786,
              -0.016455311328172684,
              0.02609720453619957,
              -0.034136757254600525,
              -0.026111137121915817,
              -0.01950671896338463,
              0.02161065861582756,
              -0.011550764553248882,
              0.01779291406273842,
              -0.01468577329069376,
              -0.022237660363316536,
              -0.0067054349929094315,
              0.029287945479154587,
              -0.004845329560339451,
              0.02746267430484295,
              -0.0020969724282622337,
              -0.02605540305376053,
              0.020872190594673157,
              -0.03722996637225151,
              0.0004397720331326127,
              -0.03472195938229561,
              -0.012888368219137192,
              -0.003988427575677633,
              -0.0021770892199128866,
              0.012331033125519753,
              -0.014072705060243607,
              -0.003828193759545684,
              -0.0120941661298275,
              -0.01315310224890709,
              -0.01251913420855999,
              -0.0035025011748075485,
              0.046119458973407745,
              0.013055568560957909,
              0.004500478971749544,
              -0.014797240495681763,
              0.009202991612255573,
              -0.020523855462670326,
              -0.006771618500351906,
              -0.015340641140937805,
              0.02553986944258213,
              0.00311759184114635,
              -0.014866907149553299,
              0.03867903724312782,
              -0.01282566785812378,
              -0.011926965788006783,
              -0.015438174828886986,
              0.035335030406713486,
              -0.0234359297901392,
              0.045617856085300446,
              0.0035460428334772587,
              0.0027309407014399767,
              -0.017500314861536026,
              0.008276422508060932,
              -0.005280747544020414,
              -0.010895895771682262,
              -0.03148941695690155,
              0.018614983186125755,
              0.026361938565969467,
              -0.012303167022764683,
              -0.003533851122483611,
              -0.022530261427164078,
              -0.01612091064453125,
              -0.020091921091079712,
              -0.024271931499242783,
              0.04508838802576065,
              -0.0038978606462478638,
              0.003025283105671406,
              0.007168719545006752,
              0.009363225661218166,
              -0.0028371827211230993,
              -0.0002782320079859346,
              -0.037257835268974304,
              -0.020426321774721146,
              0.0015509583754464984,
              0.0397937074303627,
              0.0193952526897192,
              0.024620266631245613,
              0.04701119288802147,
              -0.011947865597903728,
              0.018127316609025,
              0.020217321813106537,
              -0.0008625627378933132,
              -0.0031837753485888243,
              -0.030346881598234177,
              -0.008234621956944466,
              -0.015995509922504425,
              0.014191138558089733,
              0.007067702244967222,
              -0.0076424540020525455,
              0.006956235505640507,
              0.01985505409538746,
              0.015006240457296371,
              0.011077029630541801,
              -0.003838643664494157,
              -0.0021370307076722383,
              0.0010876738233491778,
              -0.019172318279743195,
              -0.01256790105253458,
              0.01869858428835869,
              -0.018308449536561966,
              0.00748222041875124,
              -0.03380235657095909,
              -0.004695545881986618,
              0.0031559085473418236,
              -0.018865784630179405,
              0.011940899305045605,
              -0.00720355287194252,
              0.019074784591794014,
              -0.019269851967692375,
              0.010282827541232109,
              -0.016998711973428726,
              0.013821904547512531,
              -0.013229736126959324,
              -0.010888929478824139,
              -0.023463796824216843,
              -0.001105090486817062,
              0.01677577942609787,
              0.019966520369052887,
              -0.011620431207120419,
              0.0018583633936941624,
              -0.030514081940054893,
              0.015326708555221558,
              -0.008137088268995285,
              -0.029148612171411514,
              -0.003768976777791977,
              0.001748638111166656,
              0.007879321463406086,
              -0.016817579045891762,
              -0.04946346580982208,
              -0.034638360142707825,
              -0.013362103141844273,
              -0.009634925983846188,
              -0.001558795920573175,
              -0.016399577260017395,
              0.015298841521143913,
              -0.011042196303606033,
              -0.04135424643754959,
              -0.004496995359659195,
              -0.0026560488622635603,
              0.03023541532456875,
              0.022335194051265717,
              0.005200630519539118,
              0.012985901907086372,
              0.030904216691851616,
              0.013389969244599342,
              -0.011007362976670265,
              0.00028258617385290563,
              -0.019019052386283875,
              0.009920559823513031,
              0.011188496835529804,
              -0.023867864161729813,
              -0.0428311824798584,
              0.0299846138805151,
              0.027393007650971413,
              -0.017667515203356743,
              -0.029343679547309875,
              0.015856176614761353,
              -0.018782183527946472,
              -0.001386370393447578,
              0.00997629389166832,
              -0.011425363831222057,
              -0.021122990176081657,
              -0.009035791270434856,
              -0.012170800007879734,
              0.014295638538897038,
              0.01468577329069376,
              -0.005298164207488298,
              -0.017848648130893707,
              0.013013768941164017,
              -0.020119788125157356,
              0.005218047648668289,
              0.010881963185966015,
              -0.010847128927707672,
              0.024007197469472885,
              0.01471363939344883,
              0.022711394354701042,
              0.014267771504819393,
              0.03304995596408844,
              0.026334071531891823,
              -0.030709149315953255,
              0.018378116190433502,
              -0.013452669605612755,
              -0.003040958195924759,
              -0.007224452681839466,
              0.005360864568501711,
              0.004469128791242838,
              0.0304862167686224,
              -0.022711394354701042,
              -0.00927265826612711,
              0.004099894315004349,
              -0.009767292998731136,
              0.017653580754995346,
              0.008729256689548492,
              -0.00025994444149546325,
              0.004911513067781925,
              -0.011522897519171238,
              0.007928088307380676,
              0.034638360142707825,
              0.006123716477304697,
              -0.02176392637193203,
              -0.0007519666105508804,
              -0.0016493628500029445,
              0.0037445933558046818,
              -0.02690533921122551,
              0.009453792124986649,
              0.029510879889130592,
              -0.0418279804289341,
              -0.00914029125124216,
              0.027518408372998238,
              0.011334797367453575,
              0.011049163527786732,
              -0.029148612171411514,
              0.011090963147580624,
              0.013167035765945911,
              0.021025456488132477,
              -0.013410869985818863,
              -0.010206194594502449,
              -0.007503120228648186,
              0.015326708555221558,
              -0.03006821498274803,
              -0.017165912315249443,
              -0.019924720749258995,
              -0.011857299134135246,
              0.0158701092004776,
              0.021039390936493874,
              0.007565820589661598,
              -0.007872354239225388,
              0.010540595278143883,
              -0.029343679547309875,
              0.010241027921438217,
              0.009753359481692314,
              -0.004260128363966942,
              -0.00318725872784853,
              0.014769373461604118,
              -0.0038665104657411575,
              0.010450027883052826,
              0.0176257137209177,
              0.020133720710873604,
              -0.0065626176074147224,
              0.01000415999442339,
              -0.030597683042287827,
              -0.015897976234555244,
              -0.007955954410135746,
              0.004124277736991644,
              -0.00038055519689805806,
              -0.02800607495009899,
              0.006447667255997658,
              0.004775662906467915,
              0.00568133220076561,
              -0.0022397893480956554,
              -0.015354574657976627,
              -0.005329514387995005,
              0.002267656149342656,
              -0.011676165275275707,
              0.00816495530307293,
              0.016566777601838112,
              -0.023310529068112373,
              0.037007033824920654,
              0.0067054349929094315,
              0.04951919987797737,
              0.0018670717254281044,
              -0.01606517657637596,
              -0.03491702675819397,
              -0.026125071570277214,
              -0.017556047067046165,
              -0.026877472177147865,
              -0.03304995596408844,
              -0.025484135374426842,
              0.02790854312479496,
              0.019562453031539917,
              -0.027128273621201515,
              -0.03260408714413643,
              -0.018196983262896538,
              -0.019659986719489098,
              0.00997629389166832,
              0.01646924391388893,
              0.01779291406273842,
              0.010575428605079651,
              0.01617664471268654,
              -0.02841014415025711,
              0.03714636713266373,
              0.010087761096656322,
              -0.0006901372689753771,
              -0.013550203293561935,
              0.012581833638250828,
              0.011397497728466988,
              -0.01612091064453125,
              0.004803529474884272,
              0.02473173290491104,
              -0.04751279577612877,
              -0.0007872354472056031,
              0.03784303739666939,
              -0.006451150868088007,
              0.02403506450355053,
              0.03918064013123512,
              -0.019715720787644386,
              -0.03480556234717369,
              -0.01430957205593586,
              0.018322382122278214,
              0.033022090792655945,
              0.01692904531955719,
              0.003636609762907028,
              -0.0019855054561048746,
              -0.01342480257153511,
              0.0052772643975913525,
              0.013717403635382652,
              -0.026724206283688545,
              0.0027675158344209194,
              0.0054792980663478374,
              0.0037724601570516825,
              -0.02397933229804039,
              -0.017221646383404732,
              0.011676165275275707,
              0.012059332802891731,
              -0.014978373423218727,
              0.020523855462670326,
              -0.004420361947268248,
              -0.02967808023095131,
              0.0049393801018595695,
              0.02679387293756008,
              0.0015971126267686486,
              -0.020900055766105652,
              0.0110839968547225,
              -0.027936408296227455,
              0.0032029335852712393,
              0.02161065861582756,
              0.012958034873008728,
              -0.0052842311561107635,
              -0.002953874645754695,
              0.005691782105714083,
              -0.02573493681848049,
              -0.00224153115414083,
              -0.007691220846027136,
              0.0058694323524832726,
              -0.018461717292666435,
              0.04625879228115082,
              0.02917647920548916,
              -0.05105186998844147,
              0.017458513379096985,
              -0.013780103996396065,
              -0.008833757601678371,
              -0.019032984972000122,
              0.0032691170927137136,
              0.005033430177718401,
              -0.021136924624443054,
              0.02101152390241623,
              0.0037480767350643873,
              -0.03505636006593704,
              -0.0019611220341175795,
              -0.0012052366510033607,
              0.008680489845573902,
              -0.01047789491713047,
              0.02574886940419674,
              0.2039845585823059,
              -0.03405315801501274,
              0.01833631657063961,
              0.0029573580250144005,
              0.013780103996396065,
              0.0176257137209177,
              0.0037550434935837984,
              0.017709314823150635,
              -0.00217186426743865,
              -0.001844429993070662,
              0.00048462007544003427,
              0.008060455322265625,
              -0.013222768902778625,
              0.0024365982972085476,
              0.0224187932908535,
              0.0010824487544596195,
              -0.012066299095749855,
              -0.046175193041563034,
              -0.00687611848115921,
              -0.0056778485886752605,
              -0.005225013941526413,
              0.014699706807732582,
              -0.003213383723050356,
              -0.036505430936813354,
              0.014351372607052326,
              -0.012916235253214836,
              -0.006691501475870609,
              -0.026919273659586906,
              0.006419800687581301,
              0.03472195938229561,
              -0.007377719972282648,
              0.01858711615204811,
              -0.008241589181125164,
              0.015479975380003452,
              -0.009690659120678902,
              -0.020384522154927254,
              -0.02272532880306244,
              0.02222372591495514,
              0.011056129820644855,
              -0.009425925090909004,
              0.035084228962659836,
              0.010401261039078236,
              -0.008025621995329857,
              -0.012331033125519753,
              -0.00031154145835898817,
              -0.00023947982117533684,
              0.000322644627885893,
              -0.013814937323331833,
              -0.002095230622217059,
              0.03023541532456875,
              -0.010512728244066238,
              0.02654307149350643,
              0.039124906063079834,
              0.042691849172115326,
              0.0030461831483989954,
              -0.014267771504819393,
              0.002694365568459034,
              0.00595651613548398,
              0.005886849481612444,
              0.01949278637766838,
              -0.025762803852558136,
              0.0186428502202034,
              0.002546323463320732,
              0.03112715110182762,
              0.03179595246911049,
              0.015856176614761353,
              -0.019882921129465103,
              0.012393733486533165,
              0.015730775892734528,
              -0.004723412916064262,
              -0.015382441692054272,
              -0.016817579045891762,
              -0.005925165954977274,
              -0.004653745796531439,
              -0.03502849489450455,
              -0.02126232348382473,
              0.051414139568805695,
              0.01818304881453514,
              0.009565259329974651,
              0.034081026911735535,
              -0.018754318356513977,
              -0.005834599025547504,
              0.011369630694389343,
              -0.014894773252308369,
              -0.026779938489198685,
              -0.043054115027189255,
              0.017305247485637665,
              -0.01395427156239748,
              -0.017946181818842888,
              -0.016483178362250328,
              0.0023791231215000153,
              -0.021081190556287766,
              -0.008959157392382622,
              0.008617790415883064,
              -0.0004689450142905116,
              -0.010735662654042244,
              -0.019172318279743195,
              0.03940357267856598,
              -0.0014569080667570233,
              -0.021596726030111313,
              -0.03452689200639725,
              0.04703906178474426,
              0.03842823579907417,
              0.02644553780555725,
              -0.0008155376417562366,
              0.004946346860378981,
              -0.01019922737032175,
              -0.008157989010214806,
              0.022502394393086433,
              -0.0013027702225372195,
              0.001785213127732277,
              -0.012386767193675041,
              0.0019053885480389,
              0.007997754961252213,
              -0.0018931968370452523,
              -0.011327831074595451,
              0.005221530795097351,
              -0.014393172226846218,
              0.012637567706406116,
              -0.005221530795097351,
              0.0005899911629967391,
              -0.03388595953583717,
              -0.039821576327085495,
              0.017946181818842888,
              -0.0014830331783741713,
              -0.004685095977038145,
              -0.016413511708378792,
              -0.02619473822414875,
              -0.01430957205593586,
              -0.007405586540699005,
              0.011327831074595451,
              -0.03265982121229172,
              0.03530716150999069,
              -0.021582791581749916,
              -0.02906501293182373,
              0.00651036761701107,
              0.01950671896338463,
              -0.017542114481329918,
              -0.01779291406273842,
              0.009941460564732552,
              0.009460759349167347,
              -5.8073856052942574e-05,
              0.006141133140772581,
              -0.006984102074056864,
              0.01627417653799057,
              -0.0468439944088459,
              0.0076215541921556,
              -0.014434972777962685,
              -0.0014351372374221683,
              -0.03625463321805,
              -0.01478330697864294,
              -0.007649420760571957,
              0.0015466042095795274,
              -0.019311651587486267,
              0.023463796824216843,
              -0.002380864694714546,
              -0.018670717254281044,
              -0.015159507282078266,
              -0.013264569453895092,
              0.0021109057124704123,
              -0.028786344453692436,
              -0.015591442584991455,
              -0.0022641727700829506,
              -0.025219401344656944,
              -0.009077590890228748,
              -0.009579192847013474,
              -0.17745541036128998,
              0.010847128927707672,
              0.02076072245836258,
              -0.032074619084596634,
              -0.005552448332309723,
              0.02711433917284012,
              0.032269686460494995,
              -0.009850893169641495,
              -0.020482055842876434,
              0.003523401217535138,
              -0.0010632904013618827,
              -0.017012646421790123,
              -0.02856341004371643,
              -0.0007136498461477458,
              0.004350695293396711,
              0.01306950207799673,
              0.0018235299503430724,
              0.02045418880879879,
              0.024898933246731758,
              0.02825687639415264,
              0.03143368661403656,
              -0.044614654034376144,
              0.013536269776523113,
              -0.007321986369788647,
              0.02064925618469715,
              -0.009767292998731136,
              -0.010171361267566681,
              0.03137795254588127,
              -0.017151979729533195,
              -0.014671839773654938,
              0.012128999456763268,
              0.02725367434322834,
              0.012101132422685623,
              -0.004552728962153196,
              0.00025515485322102904,
              0.020886123180389404,
              0.01123029738664627,
              -0.01334816962480545,
              -0.0011651781387627125,
              -0.008415755815804005,
              0.05252880975604057,
              -0.00992752704769373,
              -0.0076145874336361885,
              -0.014121471904218197,
              -0.023644929751753807,
              -0.011097930371761322,
              0.007670320570468903,
              -0.0022746229078620672,
              0.016009442508220673,
              0.018364183604717255,
              -0.01748638041317463,
              -0.020384522154927254,
              0.020635321736335754,
              0.006398900877684355,
              0.007005002349615097,
              0.02679387293756008,
              0.011662231758236885,
              0.00821372214704752,
              -0.026166871190071106,
              0.015535708516836166,
              0.004977697040885687,
              -0.033022090792655945,
              0.026013603433966637,
              0.002319906372576952,
              -0.014978373423218727,
              -0.0016868087695911527,
              0.0018479133723303676,
              0.004333278629928827,
              -0.021986858919262886,
              0.011202430352568626,
              -0.03647756576538086,
              -0.023463796824216843,
              0.0003302644472569227,
              0.0047408295795321465,
              0.022599928081035614,
              -0.00012757742661051452,
              -0.019367385655641556,
              -0.0012609701370820403,
              0.001722512999549508,
              -0.01123029738664627,
              -0.033022090792655945,
              0.0317402184009552,
              -0.0096976263448596,
              0.026975005865097046,
              0.0032081587705761194,
              -0.012087199836969376,
              0.005521098151803017,
              0.018322382122278214,
              -0.019715720787644386,
              -0.026431605219841003,
              0.011432331055402756,
              0.0060226996429264545,
              0.017542114481329918,
              0.0019785386975854635,
              0.007412553299218416,
              0.01779291406273842,
              0.010220128111541271,
              -0.015284908004105091,
              0.022181926295161247,
              -0.012853534892201424,
              -0.02105332352221012,
              -0.017347047105431557,
              0.006381483748555183,
              0.009307491593062878,
              0.012783868238329887,
              0.02533086948096752,
              -0.019618187099695206,
              0.006694984622299671,
              0.029037145897746086,
              0.011481097899377346,
              -0.006043599918484688,
              -0.010833196341991425,
              0.00023664960463065654,
              0.02629227191209793,
              0.017820781096816063,
              0.009133324958384037,
              -0.03193528577685356,
              -0.0027918992564082146,
              0.005256364122033119,
              -0.02599967084825039,
              0.043499983847141266,
              -0.010603295639157295,
              0.003612226340919733,
              0.014978373423218727,
              -0.0007920250645838678,
              -0.01804371550679207,
              -0.11425364017486572,
              -0.008206755854189396,
              -0.002555032027885318,
              0.004354178439825773,
              -0.014490705914795399,
              -0.004120794590562582,
              0.02172212488949299,
              0.029483012855052948,
              -0.026027537882328033,
              0.010624195449054241,
              -0.020579589530825615,
              -0.02711433917284012,
              0.013090401887893677,
              0.004803529474884272,
              -0.005319064483046532,
              0.008185855112969875,
              0.0033701341599226,
              -0.0047582462430000305,
              -0.009251758456230164,
              0.026682404801249504,
              -0.0026473405305296183,
              -0.01959032006561756,
              0.014393172226846218,
              -0.024048998951911926,
              0.015842242166399956,
              0.00677858479321003,
              -0.029705947265028954,
              0.0015788250602781773,
              0.021889325231313705,
              0.011028262786567211,
              0.007398620247840881,
              -0.03455476090312004,
              -0.00881285686045885,
              -0.003410192672163248,
              -0.0019837636500597,
              -0.003326592268422246,
              -0.012909268029034138,
              -0.022558128461241722,
              0.005782349035143852,
              -0.03405315801501274,
              0.0029051078017801046,
              0.013292435556650162,
              -0.004169561434537172,
              -0.010157427750527859,
              -0.010136527009308338,
              -0.00012942795001436025,
              -0.014434972777962685,
              0.030151814222335815,
              0.023770330473780632,
              -0.00803258828818798,
              -0.012198666110634804,
              -0.02045418880879879,
              -0.02499646693468094,
              -0.005367831327021122,
              0.020941857248544693,
              0.033133555203676224,
              0.0186428502202034,
              0.013598970137536526,
              -0.004517895635217428,
              -0.012965002097189426,
              -0.000530774355866015,
              -0.005092646926641464,
              -0.01698477938771248,
              0.005144897382706404,
              0.018628917634487152,
              -0.001567504252307117,
              -0.05473028123378754,
              -0.011167597025632858,
              -0.008673523552715778,
              -0.005350414663553238,
              -0.014156305231153965,
              0.01793224923312664,
              0.013194902800023556,
              0.02155492454767227,
              -0.01858711615204811,
              -0.01012259442359209,
              -0.014281705021858215,
              -0.0021091641392558813,
              0.0022066975943744183,
              -0.018726451322436333,
              -0.01090286299586296,
              -0.0274487417191267,
              0.007698187604546547,
              -0.002624698681756854,
              -0.010220128111541271,
              -0.012804768048226833,
              -0.0012182991486042738,
              -0.02499646693468094,
              -0.011745831929147243,
              -0.029761681333184242,
              0.009662793017923832,
              0.008645656518638134,
              -0.029761681333184242,
              -0.008234621956944466,
              -0.004180011339485645,
              -0.0024644650984555483,
              -0.0031994504388421774,
              -0.01390550471842289,
              0.020217321813106537,
              0.005987866315990686,
              -0.015075907111167908,
              0.0043750787153840065,
              -0.04391798377037048,
              0.016162710264325142,
              0.0032952423207461834,
              -0.01698477938771248,
              0.011655264534056187,
              0.018823985010385513,
              0.03692343458533287,
              -0.017040513455867767,
              0.002717007417231798,
              0.008854657411575317,
              -0.007363786455243826,
              0.021485257893800735,
              -0.01738884672522545,
              -0.018977250903844833,
              -0.006447667255997658,
              -0.010993429459631443,
              0.005925165954977274,
              -0.004507445730268955,
              0.029204346239566803,
              0.005803248845040798,
              -0.01499230694025755,
              -0.016538910567760468,
              0.009572225622832775,
              -0.00025798508431762457,
              -0.007774821016937494,
              0.013828870840370655,
              -0.010234060697257519,
              0.026320137083530426,
              -0.026612738147377968,
              -0.011780665256083012,
              0.020077988505363464,
              -0.02403506450355053,
              -0.0008969607879407704,
              0.030040347948670387,
              -0.0010589362354949117,
              0.016664311289787292,
              0.02423013187944889,
              0.008596889674663544,
              0.03622676432132721,
              0.004639812745153904,
              -0.014755439944565296,
              -0.03600383177399635,
              0.0020307889208197594,
              -0.012540034018456936,
              -0.00260728201828897,
              0.022251592949032784,
              -0.0027222323697060347,
              0.003500759368762374,
              0.015479975380003452,
              -0.006398900877684355,
              0.008778023533523083,
              0.02091399021446705,
              0.019074784591794014,
              -0.029009278863668442,
              -0.015953710302710533,
              -0.00849238969385624,
              -0.0035286261700093746,
              0.02278106100857258,
              0.002276364481076598,
              -0.0034415426198393106,
              0.0367841012775898,
              -0.015048041008412838,
              0.022279459983110428,
              0.003981460817158222,
              0.0076424540020525455,
              -0.008750157430768013,
              0.012428566813468933,
              0.007039835676550865,
              -0.02912074513733387,
              -0.02827080897986889,
              -0.010568462312221527,
              0.009251758456230164,
              -0.011773698963224888,
              0.02565133571624756,
              0.029037145897746086,
              0.005252880975604057,
              -0.0007088602287694812,
              0.0114601980894804,
              -0.005998316220939159,
              0.01478330697864294,
              0.012484300881624222,
              -0.02459239959716797,
              -0.016845446079969406,
              -0.014448905363678932,
              0.024453066289424896,
              0.002779707545414567,
              -0.004451712127774954,
              0.012170800007879734,
              -0.005378281231969595,
              0.02195899188518524,
              0.012184732593595982,
              0.001861846656538546,
              0.009760326705873013,
              0.03555796295404434,
              0.00401629414409399,
              0.01490870676934719,
              0.007349853403866291,
              -0.019269851967692375,
              0.006796001922339201,
              -0.021749991923570633,
              0.006127200089395046,
              0.015828309580683708,
              -0.012463400140404701,
              -0.010094727389514446,
              -0.023157263174653053,
              0.021136924624443054,
              -0.013229736126959324,
              -0.00457362923771143,
              0.003953594248741865,
              0.03770370036363602,
              -0.0011442780960351229,
              0.015298841521143913,
              -0.01219169981777668,
              0.01080532930791378,
              -0.034276094287633896,
              0.02765774168074131,
              -0.007210519630461931,
              -0.036505430936813354,
              -0.026570938527584076,
              0.033022090792655945,
              0.021931126713752747,
              -0.01518737431615591,
              0.044558919966220856,
              -0.012212599627673626,
              0.018559250980615616,
              0.010819262824952602,
              0.024717800319194794,
              -0.011906065978109837,
              0.02638980560004711,
              0.02278106100857258,
              0.017597848549485207,
              -0.008680489845573902,
              0.01989685371518135,
              -0.01874038390815258,
              -0.021136924624443054,
              -0.012310133315622807,
              -0.006607901304960251,
              0.03279915452003479,
              -0.0008817211491987109,
              0.10160214453935623,
              0.021248390898108482,
              -0.0030949499923735857,
              0.012115065939724445,
              -0.015702908858656883,
              0.011871231719851494,
              0.013362103141844273,
              0.018921518698334694,
              -0.011474130675196648,
              -0.0010545820696279407,
              0.006339683663100004,
              -0.015591442584991455,
              -0.013689536601305008,
              -0.02423013187944889,
              0.0035355929285287857,
              -0.015605375170707703,
              0.003765493631362915,
              0.008617790415883064,
              -0.008610823191702366,
              -0.0030200581531971693,
              0.024299798533320427,
              -0.00043737722444348037,
              0.011223330162465572,
              0.01602337695658207,
              -0.013118268921971321,
              -0.009669759310781956,
              0.007018935866653919,
              -0.016455311328172684,
              -0.008673523552715778,
              -0.03112715110182762,
              0.004406428430229425,
              -0.016566777601838112,
              -0.04166077822446823,
              -0.013571103103458881,
              0.02815934270620346,
              0.01097949594259262,
              -0.001757346442900598,
              0.013341202400624752,
              0.014121471904218197,
              5.785614484921098e-05,
              0.009293558076024055,
              0.036449696868658066,
              -0.0029138161335140467,
              -0.00964885950088501,
              -0.013647736981511116,
              -0.007997754961252213,
              0.0014725831570103765,
              -0.013731337152421474,
              -0.02312939614057541
            ],
            "existingAnswer": false,
            "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
            "indexType": "cogsearchvs",
            "jsonAnswer": null,
            "llm": {
              "model": "gpt-3.5-turbo",
              "request_timeout": null,
              "max_tokens": 1000,
              "stream": false,
              "n": 1,
              "temperature": 0.3,
              "engine": "chat16k",
              "_type": "azure-openai-chat"
            },
            "modifiedAnswer": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B.",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            },
            "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
            "question": "How are sentence pairs represented in the input to BERT?",
            "retrievedDocs": [
              {
                "page_content": "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.",
                "metadata": {
                  "id": "Bert_pdf-9",
                  "source": "Bert.pdf"
                }
              },
              {
                "page_content": "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "metadata": {
                  "id": "Bert_pdf-2",
                  "source": "Bert.pdf"
                }
              },
              {
                "page_content": "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
                "metadata": {
                  "id": "Bert_pdf-1",
                  "source": "Bert.pdf"
                }
              }
            ]
          },
          "output": {
            "values": [
              {
                "recordId": 0,
                "data": {
                  "data_points": [
                    "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.",
                    "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                    "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008)."
                  ],
                  "answer": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B.",
                  "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).']<br>        Question: How are sentence pairs represented in the input to BERT?<br>        ",
                  "sources": "\nBert.pdf",
                  "nextQuestions": "<What are the two unsupervised tasks used in pre-training BERT?>\n<What is the difference between the feature-based and fine-tuning approaches for applying pre-trained language representations to downstream tasks?>\n<What is the architecture of BERT and how does it differ from other models like OpenAI GPT?>",
                  "error": ""
                }
              }
            ]
          },
          "start_time": 1695246514.150503,
          "end_time": 1695246516.348566,
          "error": null,
          "children": [
            {
              "name": "openai.api_resources.chat_completion.ChatCompletion.create",
              "type": "LLM",
              "inputs": {
                "messages": [
                  {
                    "role": "user",
                    "content": "\n    Generate three very brief questions that the user would likely ask next.\n    Use double angle brackets to reference the questions, e.g. <What is Azure?>.\n    Try not to repeat questions that have already been asked.  Don't include the context in the answer.\n\n    Return the questions in the following format:\n    <>\n    <>\n    <>\n    \n    ALWAYS return a \"NEXT QUESTIONS\" part in your answer.\n\n    ['A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).']\n    "
                  }
                ],
                "model": "gpt-3.5-turbo",
                "request_timeout": null,
                "max_tokens": 1000,
                "stream": false,
                "n": 1,
                "temperature": 0.3,
                "engine": "chat16k",
                "api_base": "https://dataaiapim.azure-api.net",
                "organization": "",
                "api_type": "azure",
                "api_version": "2023-07-01-preview"
              },
              "output": {
                "id": "chatcmpl-80ufKV6vIWPT5HZwqEvN9eYTrshLx",
                "object": "chat.completion",
                "created": 1695228514,
                "model": "gpt-35-turbo-16k",
                "prompt_filter_results": [
                  {
                    "prompt_index": 0,
                    "content_filter_results": {
                      "hate": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "self_harm": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "sexual": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "violence": {
                        "filtered": false,
                        "severity": "safe"
                      }
                    }
                  }
                ],
                "choices": [
                  {
                    "index": 0,
                    "finish_reason": "stop",
                    "message": {
                      "role": "assistant",
                      "content": "<What are the two unsupervised tasks used in pre-training BERT?>\n<What is the difference between the feature-based and fine-tuning approaches for applying pre-trained language representations to downstream tasks?>\n<What is the architecture of BERT and how does it differ from other models like OpenAI GPT?>"
                    },
                    "content_filter_results": {
                      "hate": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "self_harm": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "sexual": {
                        "filtered": false,
                        "severity": "safe"
                      },
                      "violence": {
                        "filtered": false,
                        "severity": "safe"
                      }
                    }
                  }
                ],
                "usage": {
                  "completion_tokens": 62,
                  "prompt_tokens": 6402,
                  "total_tokens": 6464
                }
              },
              "start_time": 1695246514.174635,
              "end_time": 1695246515.807909,
              "error": null,
              "children": null,
              "node_name": null
            }
          ],
          "node_name": "followup_questions"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "[2023-09-20T16:48:35+0000] Next questions: <What are the two unsupervised tasks used in pre-training BERT?>\n<What is the difference between the feature-based and fine-tuning approaches for applying pre-trained language representations to downstream tasks?>\n<What is the architecture of BERT and how does it differ from other models like OpenAI GPT?>\n[2023-09-20T16:48:35+0000] Total docs: 1\n[2023-09-20T16:48:36+0000] \tIndexed 1 sections, 1 succeeded\n",
        "stderr": ""
      },
      "system_metrics": {
        "completion_tokens": 62,
        "prompt_tokens": 6402,
        "total_tokens": 6464,
        "duration": 2.221509
      },
      "result": {
        "values": [
          {
            "recordId": 0,
            "data": {
              "data_points": [
                "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.",
                "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008)."
              ],
              "answer": "Sentence pairs are represented in the input to BERT by packing them together into a single token sequence. The sentences are separated by a special token ([SEP]), and a learned embedding is added to each token to indicate whether it belongs to sentence A or sentence B.",
              "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).']<br>        Question: How are sentence pairs represented in the input to BERT?<br>        ",
              "sources": "\nBert.pdf",
              "nextQuestions": "<What are the two unsupervised tasks used in pre-training BERT?>\n<What is the difference between the feature-based and fine-tuning approaches for applying pre-trained language representations to downstream tasks?>\n<What is the architecture of BERT and how does it differ from other models like OpenAI GPT?>",
              "error": ""
            }
          }
        ]
      }
    }
  ]
}