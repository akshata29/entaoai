$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: blue
endpoint_name: sqlask
model: azureml:sqlask:5
type: managed
  # You can also specify model files path inline
  # path: examples/flows/chat/basic-chat
environment: 
  #name: entaoai
  image: akshata13/entaoai:latest
  # inference config is used to build a serving container for online deployments
  inference_config:
    liveness_route:
      path: /health
      port: 8080
    readiness_route:
      path: /health
      port: 8080
    scoring_route:
      path: /score
      port: 8080
instance_type: Standard_F4s_v2
instance_count: 1
app_insights_enabled: true
data_collector:
    collections:
      requests:
        enabled: 'true'
      response:
        enabled: 'true'
      model_inputs:
        enabled: 'true' 
      model_outputs:
        enabled: 'true'
request_settings:
  max_concurrent_requests_per_instance: 10
  request_timeout_ms: 300000
scale_settings:
  type: default
readiness_probe:
    failure_threshold: 30
    initial_delay: 10
    period: 10
    success_threshold: 1
    timeout: 2
liveness_probe:
    failure_threshold: 30
    initial_delay: 10
    period: 10
    success_threshold: 1
    timeout: 2
environment_variables:
  # "compute" mode is the default mode, if you want to deploy to serving mode, you need to set this env variable to "serving"
  PROMPTFLOW_RUN_MODE: serving
  AZURE_CLIENT_ID: e17108b7-9cb4-4b54-b57c-e2be41c99910
  PROMPTFLOW_RESPONSE_INCLUDED_FIELDS: '["output", "answer", "context"]'
  RUN_MODE: serving

  # for pulling connections from workspace
  PRT_CONFIG_OVERRIDE: deployment.subscription_id=e2171f6d-2650-45e6-af7e-6d6e44ca92b1,deployment.resource_group=dataai,deployment.workspace_name=dataaiamlwks,deployment.endpoint_name=sqlask,deployment.deployment_name=blue,deployment.mt_service_endpoint=https://southcentralus.api.azureml.ms,deployment.runtime_name=entaoai,storage.storage_account=dataaiamlwksstor
